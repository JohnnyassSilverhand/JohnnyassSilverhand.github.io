[ { "title": "BART源码精读", "url": "/posts/barta/", "categories": "Research, SourceCodeReading", "tags": "BART, Music Generation, AI", "date": "2023-03-28 08:43:30 +0800", "snippet": "BART Architecture源码仓库地址源码中各class的介绍与模型结构： BartLearnedPositionalEmbedding该class定义了BART模型中的位置嵌入层。与传统的位置嵌入不同，BART采用了可学习的位置嵌入，以提高模型的泛化能力。该层的输入是token嵌入和位置编码，输出是嵌入向量。 BartAttention该class定义了BART模型中的自注意力机制。该层的输入是query、key和value，输出是加权的value向量。BART采用了多头注意力机制，可以捕捉不同的特征。 BartEncoderLayer/BartDecoderLayer该class定义了BART模型中的编码器层和解码器层。这两个层的结构类似，都包含了多头注意力、前向传播和残差连接等模块。编码器层用于对输入文本进行编码，解码器层用于生成目标文本。 BartEncoder/BartDecoder该class定义了BART模型中的编码器和解码器。编码器由多个编码器层组成，用于对输入文本进行编码。解码器由多个解码器层组成，用于生成目标文本。编码器和解码器之间还包含了一个连接层，用于将编码器的输出传递给解码器。 BartModel该class定义了完整的BART模型，包括编码器、解码器和连接层等。该模型可以用于生成式任务，如文本摘要、机器翻译等。 BartForConditionalGeneration该class继承自BartModel，用于有条件的生成任务，如文本摘要、机器翻译等。该模型包含了一个线性层，用于将解码器的输出映射为目标文本。BartAttentionMulti-headed attention from ‘Attention Is All You Need’ paperBartModel def __init__(self, config: BartConfig): super().__init__(config) \"\"\" padding_idx表示在词汇表中的填充符号的索引，一般为0。 在BART模型中，输入文本会被分成一系列的token，如果某个样本的token数量不足时， 就需要在其后添加填充符号，以保证所有样本的token数量一致。 vocab_size表示词汇表的大小，即词汇表中不同单词的数量。 它们都是模型的输入参数。 \"\"\" padding_idx, vocab_size = config.pad_token_id, config.vocab_size # config.d_model：超参数，它指定了模型中隐藏层的大小，也称为嵌入维度 # nn.Embedding类将输入序列中的每个单词映射到一个嵌入向量 # 这个映射是通过将单词的整数索引作为输入，查找embed_tokens.weight（嵌入向量W）矩阵，返回相应的嵌入向量来实现的 # embed_tokens.weight shape like (vocab_size, config.d_model) self.shared = nn.Embedding(vocab_size, config.d_model, padding_idx) self.encoder = BartEncoder(config, self.shared) self.decoder = BartDecoder(config, self.shared) # Initialize weights and apply final processing self.post_init()参数输入参数 padding_idx：表示在词汇表中的填充符号的索引，一般为0。在BART模型中，输入文本会被分成一系列的token，如果某个样本的token数量不足时，就需要在其后添加填充符号，以保证所有样本的token数量一致。 vocab_size：表示词汇表的大小，即词汇表中不同单词的数量。超参数 config.d_model：超参数，它指定了模型中隐藏层的大小，也称为嵌入维度。embed_tokens.weight shape like (vocab_size, config.d_model)forward输入序列首先被转化为单词索引序列，然后通过self.shared层进行嵌入映射，通过将单词的整数索引作为输入，查找embed_tokens.weight（嵌入向量W）矩阵，返回相应的嵌入向量。这些嵌入向量embedded_tokens将作为模型的输入，依次被传递到编码器和解码器中进行处理，生成对应的输出。return Seq2SeqModelOutput( last_hidden_state=decoder_outputs.last_hidden_state, past_key_values=decoder_outputs.past_key_values, decoder_hidden_states=decoder_outputs.hidden_states, decoder_attentions=decoder_outputs.attentions, cross_attentions=decoder_outputs.cross_attentions, encoder_last_hidden_state=encoder_outputs.last_hidden_state, encoder_hidden_states=encoder_outputs.hidden_states, encoder_attentions=encoder_outputs.attentions, )shared在BART模型中，有些层需要在编码器和解码器之间共享，例如嵌入层（也就是上面提到的embedded_tokens，存有嵌入向量权重矩阵）。因此，BART模型使用了一个称为”shared”的参数，来将这些层在编码器和解码器之间共享。这个shared参数是一个可学习的参数，可以在模型的训练过程中进行更新。这样，BART模型就可以在编码器和解码器之间共享这些层，从而减少了模型中的参数数量，加快了模型的训练速度，并提高了模型的泛化能力。在实现过程中，BARTModel类中的shared属性是一个nn.Embedding类，nn.Embedding类将一个整数张量中的每个整数索引映射到一个固定大小的嵌入向量，它通过查找嵌入矩阵中相应的行来实现这个映射。在模型的前向传播过程中，输入序列的每个单词索引都通过self.shared进行映射，得到一个固定大小的嵌入向量，作为模型的输入。BartEncoderSelf-attention mask在Encoder中，自注意力掩码被设置为遮盖当前位置之后的所有位置，这是因为编码器需要前序信息而不包括后序信息。自注意力掩码信息被储存在attention_mask张量中，大小为(batch_size, sequence_length)，用于遮盖输入序列中的一些位置以表示哪些位置可以被模型忽略，1为not maksed，0为masked。head_mask：指定需要遮盖的注意力头，减少计算量# expand attention_maskif attention_mask is not None: # [bsz, seq_len] -&gt; [bsz, 1, tgt_seq_len, src_seq_len] attention_mask = _expand_mask(attention_mask, inputs_embeds.dtype) # 与自注意力得分矩阵的大小匹配 encoder_states = () if output_hidden_states else None all_attentions = () if output_attentions else None # check if head_mask has a correct number of layers specified if desired if head_mask is not None: if head_mask.size()[0] != (len(self.layers)): raise ValueError( f\"The head_mask should be specified for {len(self.layers)} layers, but it is for\" f\" {head_mask.size()[0]}.\" )注意力分数矩阵是一个形状为 [batch_size, num_heads, sequence_length, sequence_length] 的 4D 张量，其中 batch_size 是批次大小，num_heads 是注意力头的数量，sequence_length 是序列的长度。注意力分数矩阵中的每个元素都代表了一个位置对于另一个位置的注意力权重。超参数 dropout：对于每个Transformer模块的输入向量，以config.dropout的概率将其中一部分设置为0，以达到正则化的效果。 layerdrop：对于每个Transformer模块，以config.encoder_layerdrop的概率不更新其中一部分模块，以达到正则化的效果。学习嵌入信息 embed_tokens(nn.Embedding)：学习的嵌入向量矩阵 embed_positions(BartLearnedPositionalEmbedding)：学习的位置嵌入矩阵BartLearnedPositionalEmbeddingBartLearnedPositionalEmbedding类继承了nn.Embedding类，用于学习位置嵌入（positional embeddings）。该类的输入参数是num_embeddings和embedding_dim，分别指定了可能的位置数量和嵌入的维度。BartLearnedPositionalEmbedding类的构造函数使用了一个特殊的hack，即如果padding_idx被指定了，则需要通过偏移(offset)将嵌入id加2，同时调整num_embeddings，以便在padding_idx位置插入特殊嵌入。例子：假设我们有一个BART模型，它有6个位置嵌入和2个特殊的嵌入（BOS和EOS）。在这种情况下，如果我们将padding_idx设置为1，则所有嵌入id都需要偏移2个位置，以便将填充嵌入插入到1的位置。因此，新的嵌入id将是： 0：BOS(begin-of-sequence)嵌入 1：填充嵌入(padding_idx) 2：EOS(end-of-sequence)嵌入 3：第一个位置嵌入 4：第二个位置嵌入 5：第三个位置嵌入在BartLearnedPositionalEmbedding类的forward方法中，输入张量的形状被期望是[bsz x seqlen]。该方法首先计算出输入序列中每个令牌的位置，然后将这些位置传递给nn.Embedding的forward方法。为了计算令牌的位置，使用了torch.arange()函数生成从past_key_values_length到past_key_values_length+seq_len的序列，这些序列在第一维上被扩展为bsz，以匹配输入张量的形状，即[bsz, seq_len]。最后，将位置id与偏移值相加，以获得在BART模型中使用的实际嵌入id。在实现中，使用了PyTorch的super()方法调用父类nn.Embedding的forward方法，以获取位置序列的嵌入向量。class BartLearnedPositionalEmbedding(nn.Embedding): \"\"\" This module learns positional embeddings up to a fixed maximum size. \"\"\" def __init__(self, num_embeddings: int, embedding_dim: int): # Bart is set up so that if padding_idx is specified then offset the embedding ids by 2 # and adjust num_embeddings appropriately. Other models don't have this hack self.offset = 2 super().__init__(num_embeddings + self.offset, embedding_dim) def forward(self, input_ids: torch.Tensor, past_key_values_length: int = 0): \"\"\"`input_ids' shape is expected to be [bsz x seqlen].\"\"\" bsz, seq_len = input_ids.shape[:2] positions = torch.arange( past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device ).expand(bsz, -1) return super().forward(positions + self.offset)BartEncoderLayer BartEncoderLayer self-attention dropout, +=residual（随机失活，残差连接） self-attention layer norm fc1, activation dropout, fc2 dropout, +=residual final norm Architecture BartEncoder 将词嵌入向量与位置嵌入向量相加，得到hidden_states norm, dropout BartEncoderLayer for config.encoder_layers（会以layerdrop的概率决定是否跳过layer）, 每一层的状态、自注意力矩阵会被分别累加并返回 BartDecoder与Encoder的区别在于自注意力掩码与交叉注意层。Self-attention mask在Decoder中，自注意力掩码需要遮盖当前位置之后的所有位置，以及遮盖所有编码器的输出位置，以防止解码器在生成输出时访问来自未来的信息和编码器输出中不应访问的信息。_prepare_decoder_attention_mask()会生成一个causal_mask，即遮盖当前位置的后序信息，再加上原先的attention_mask（忽略pad token），得到combined_attention_mask作为decoder的自注意力掩码，用于计算decoder的自注意力得分。past_key_value是在decoder中使用的一种机制（自注意力蒸馏），用于存储解码器过程中生成的键值对，以便在生成下一个token时重复使用，减少计算量。BartDecoderLayer此处的交叉注意力层，是指利用Encoder的hidden_states来投影得到Key/Value矩阵，利用Decoder的hidden_states来投影得到Query矩阵。引入encoder的信息来计算交叉注意力得分。 BartDecoderLayer self-attention dropout, +=residual（随机失活，残差连接） self-attention layer norm encoder_attn（交叉注意力） dropout, +=residual encoder_attn_layer_norm fc1, activation dropout, fc2 dropout, +=residual final norm Architecture BartDecoder 将词嵌入向量与位置嵌入向量相加，得到hidden_states norm, dropout BartDecoderLayer for config.decoder_layers（会以layerdrop的概率决定是否跳过layer）, 每一层的状态、自注意力矩阵、交叉注意力矩阵会被分别累加并返回，还会返回上一状态与上一解码键值对的信息，避免冗余计算 下一步工作接下来要继续读的： bartAttention的实现精读 预训练model的方式 用于不同下游任务的微调 conditional generation，生成文本摘要，在模型最后具有一个language modeling head，是一个线性层 sequence classification，序列分类，在模型最后具有一个分类头（classification head） question answering，问答，在模型最后有一个具有一个线性层 用于causal language model的bart decoder " }, { "title": "Paper Summary of BART", "url": "/posts/bart/", "categories": "Research, PaperReading", "tags": "BART, Music Generation, AI", "date": "2023-03-14 08:43:30 +0800", "snippet": "Paper Summary of BART本次阅读了论文 BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension语言模型预训练+下游任务fine-tune 用任意噪声函数破坏文本：随机打乱句子顺序；将文本替换为单个掩码令牌etc 学习模型来重建原始文本。BART是一个encoder-decoder的结构（bidirectional noising encoder(bert) + left-right decoder(GPT)），其encoder端的输入是加了噪音的序列，decoder端的输入是right-shifted的序列，decoder端的目标是原序列。模型设计的目的很明确，就是在利用encoder端的双向建模能力的同时，保留自回归的特性，以适用于生成任务。噪声方式： Token Masking: 就是BERT的方法，随机将token替换成[MASK] Token Deletion: 随机删去token Text Infilling: 随机将一段连续的token（称作span）替换成一个[MASK]，span的长度服从 λ=3的泊松分布。注意span长度为0就相当于插入一个[MASK]。 Sentence Permutation: 将一个document的句子打乱 Document Rotation: 从document序列中随机选择一个token，然后使得该token作为document的开头以上方式进行组合。利用fairseq库中的BARTModel对文本预测填空进行尝试：import torchimport torchtext# bart = torch.hub.load('pytorch/fairseq', 'bart.base')from fairseq.models.bart import BARTModelbart = BARTModel.from_pretrained('&lt;path-of-bart.large&gt;', checkpoint_file='model.pt')# bart.cuda()bart.eval()# 定义一个文本字段# text_field = torchtext.data.Field(tokenize='spacy', batch_first=True)# 将字符串列表传递给字段的 process 方法以生成张量# tensors = text_field.process(['The cat &lt;mask&gt; on the &lt;mask&gt;.', 'The dog &lt;mask&gt; on the &lt;mask&gt;.']).cuda()# s = torch.Tensor(['The cat &lt;mask&gt; on the &lt;mask&gt;.', 'The dog &lt;mask&gt; on the &lt;mask&gt;.']).cuda()print(bart.fill_mask(['The cat &lt;mask&gt; on the &lt;mask&gt;.', 'The dog &lt;mask&gt; on the &lt;mask&gt;.'], topk=5, beam=20))# [[('The cat was on the ground.', tensor(-0.6183)), ('The cat was on the floor.', tensor(-0.6798)), ('The cat sleeps on the couch.', tensor(-0.6830))]] topk 参数指定要返回的候选预测中的最高分数的数量。例如，如果您将 topk=3 设置为 bart.fill_mask() 方法，则将返回每个掩码位置的前三个候选预测。 beam 参数是用于束搜索（beam search）的参数。它指定在查找最佳预测时要考虑的最佳候选预测的数量。例如，如果您将 beam=10 设置为 bart.fill_mask() 方法，则在查找最佳预测时将考虑最佳的前10个候选预测。增加 topk 或 beam 参数值通常会增加计算成本。2023-03-14 13:31:44 | INFO | fairseq.tasks.denoising | dictionary: 50264 types [[('The cat is still on the.', tensor(-1.6241)), ('The cat is sleeping on the.', tensor(-1.6287)), ('The cat is sitting on the.', tensor(-1.7007)), ('The cat is back on the.', tensor(-1.7144)), ('The cat is asleep on the.', tensor(-1.7318))], [('The dog is still on the.', tensor(-1.7400)), ('The dog jumped up on the.', tensor(-1.8462)), ('The dog was still on the.', tensor(-1.8523)), ('The dog is sleeping on the.', tensor(-1.8564)), ('The dog was not on the.', tensor(-1.8638))]]只显示了第一个掩码的预测结果，这可能是因为第二个掩码的预测结果分数较低，未能通过 topk 和 beam 过滤器。print(bart.fill_mask(['Snow on the &lt;mask&gt;.', 'She &lt;mask&gt; to the front with her &lt;mask&gt; on Sunday night.'], *topk*=5, *beam*=20))[[('Snow on the ground.', tensor(-2.2770)), ('Snow on the way.', tensor(-2.3293)), ('Snow on the horizon.', tensor(-2.4373)), ('Snow on the roads.', tensor(-2.4572)), ('Snow on the mountains.', tensor(-2.5085))], [('She made her way to the front with her husband on Sunday', tensor(-1.2503)), ('She made her way to the front with her family on Sunday', tensor(-1.2704)), ('She returned to the front with her husband on Sunday night.', tensor(-1.3483)), ('She made her way to the front with her children on Sunday', tensor(-1.3583)), ('She made her way to the front with her daughter on Sunday', tensor(-1.3607))]]参考：https://zhuanlan.zhihu.com/p/173858031fairseq/examples/bart at main · facebookresearch/fairseq (github.com) [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension Papers With Code](https://paperswithcode.com/paper/bart-denoising-sequence-to-sequence-pre) " }, { "title": "Computer Network: Transport Layer(2)", "url": "/posts/transport/", "categories": "Courses", "tags": "Computer Network", "date": "2023-02-11 09:40:30 +0800", "snippet": " Transport Layer Transport-Layer Services Multiplexing &amp; Demultiplexing UDP RDT TCP 1. Transport-Layer Services 1.1 Transport layer protocols 1.2 Network Layer protocols 2. Multiplexing &amp; Demultiplexing 2.1 definition 2.2 Multiplexing &amp; Demultiplexing in UDP 2.3 Multiplexing &amp; Demultiplexing in TCP 2.4 Security 3. UDP(User Datagram Protocol) 3.1 UDP segment structure 3.1.1 checksum 3.2 Applications running on UDP 4. Reliable Data Transfer 4.1 rdt1.0 4.2 rdt2.0 4.2.2 rdt2.1 4.2.2 rtd2.2 4.3 rdt3.0 4.4 Pipelined Reliable Data Transfer Protocols 4.4.1 Go Back N 4.4.2 Selective Repeat 5. TCP(Transmission Control Protocol) 5.1 Definition Features MSS, MTU, TCP segment size TCP segment structure 5.2 RTT Estimation and Timeout 5.3 RDT service in TCP 快速重传 5.4 FLow Control 5.5 Connection Management 3-way handshake close SYN flood attack - classic Dos attack 5.6 Congestion Control TCP Congestion Control 限制流量速率 感知拥塞 拥塞控制算法 Transport LayerTransport-Layer Services相比运行在hosts（IP地址）之间的网络层协议，传输层协议运行在不同hosts的processes之间，实现logical communication，且其部署在end systems中而非routers中。网络层协议有UDP与TCP，两者都提供多路复用/解复用与错误检测的服务，但TCP还提供可靠数据传输、流控制、拥塞控制的服务。Multiplexing &amp; Demultiplexing在多主机、多应用、多进程、多socket上，不同应用间的通信是通过segment在socket中传输实现的，多路复用/解复用就用于定位一个segment的目的socket与源socket是什么，解复用是发送至socket，多路复用是从socket接收并发至网络层，这通过两者实现：1）唯一标识符socket，2）segment中指定源与目的socket port number。UDP socket只关注目的地，而TCP socket由于面向连接，会建立源-目的地的一对一连接。UDPUDP header中有源端口号、目的端口号（用于多路复用/解复用），length与checksum（提供data integrity服务，将16bit data相加后取反，进位时最低位+1，接收时data+checksum全为1则说明无误）。使用UDP的应用有：DNS（实时性、无需连接、减小内存占用、UDP header字节更少更轻量），SNMP，NFS。RDT rdt1.0：假设信道完全可靠，接收方无需任何feedback。 rdt2.x：假设信道存在bit error，即packet可能受损但不丢包。 rdt2.0：接收方采用ACK, NAK, ARQ，接收方确认收到传送ACK，出错传送NAK，发送方收到NAK则重传。 rdt2.1：假设ACK/NAK packet也可能受损，发送方只要收到受损的feedback packet就重传，为排除冗余packet，为数据packet添加单bit序号字段1/0，接收方使用ACK, NAK 1/0。 rdt2.2：接收方移除NAK，采用ACK 1/0。 rdt3.0：假设信道存在bit error且lossy，即packet可能受损或可能丢包，发送方发送每个packet时启动定时器，超时则重传，接收方使用ACK 1/0。 pipelined：rdt协议的缺陷是由于其核心为停等协议，每次传输都至少要等待一个RTT，因此采用流水线设计，不需要等待前序packet被确认收到。 Go Back N：序号范围扩大，双方都需要维持缓冲区。发送方维护一个全局计时器和一个窗口(base,nextseqnum)，base为最小未确认packet的序号，每次更新base时重启计时器，超时重传当前窗口所有未确认的分组。接收方采用累计确认ACK k，不符合当前需求序号k的packet一律丢弃，即接收方保证正确、正序确认packet。 Selective Repeat：双方都维持缓冲区和窗口，且由于feedback的延迟或丢包，窗口不同步。发送方为每个分组维持一个定时器，超时重传。接收方只要确认收到当前窗口和上一窗口范围内的分组，就发送ACK k（即使冗余），防止由于ACK丢失，发送方一直重传。 TCPTCP的特征为面向连接（体现在3次握手）、全双工、点对点（多重广播）、双向传输。TCP的连接管理： 3次握手：client开辟连接，发送SYN segment（SYN=1)，server发送SYNACK segment（单独ACK不包含任何数据，不是捎带确认，该确认segment中会给予该SYN segment一个cookie，此时不开辟连接，防止SYN flood attack），client发送ACK segment，可能包含请求数据，收到后server开辟连接。 关闭连接：client发送FIN segment（FIN=1）请求关闭，server发送ACK确认后一段时间再发送FIN segment，client确认后，两端真正关闭连接。TCP提供RDT、流控制、拥塞控制服务。 RDT：发送方维持一个定时器，超时重传，并累计确认，收到冗余ACK就快速重传，无需等到超时。 流控制：在header中维持一个receive window（用最后读取、最后接收、最后发送、最后确认与buffer size来表示）。 拥塞控制： 通过cwnd控制速度； 丢包（超时/冗余ACK x3）视为拥堵，收到一个ACK则增大cwnd，pushy strategy； 拥塞控制算法：初始cwnd=MSS，慢启动（cwnd+=MSS，指数增长）、拥塞避免（cwnd+=MSS*MSS/cwnd，线性增长）、快速恢复（cwnd+=MSS，指数增长），只要丢包（timeout/duplicate ACK）ssthresh=cwnd/2（乘性减半）且重传，超时则重启慢启动，冗余ACK则cwnd=ssthresh+3MSS，进入快速恢复，直到首次收到新的ACK，视为拥塞结束，进入拥塞避免。 1. Transport-Layer ServicesConclusion：相比运行在hosts（IP地址）之间的网络层协议，传输层协议运行在不同hosts的processes之间，实现logical communication，且其部署在end systems中而非routers中。网络层协议有UDP与TCP，两者都提供多路复用/解复用与错误检测的服务，但TCP还提供可靠数据传输、流控制、拥塞控制的服务。服务：传输层协议提供了运行在不同hosts上运行的processes实现逻辑通信logical communication（指他们在逻辑上直接连接，但实际上可能经过了无数routers）。相对的，一个网络层协议在hosts之间实现逻辑通信。传输层协议能提供的服务被底层的网络层协议所限制。部署：传输层协议在end systems中实现而非routers中，路由器只负责转发datagram。行为：传输层协议将应用层message封装为传输层message，传输给网络层，网络层再封装成数据报datagram。也就是说每一层就都会将数据封装为能在该层传输、向下层传输的packet。1.1 Transport layer protocolsbetween processes running on hosts UDP(User Datagram Protocol)-datagram/segment TCP(Transmission Control Protocol)-segment 两者都有/UDP仅有的services： transport-layer multiplexing and demultiplexing 传输层的多路复用与解复用，将host-host delivery扩展到process-process delivery integrity/error checkingTCP独有： reliable data transfer congestion control1.2 Network Layer protocolsbetween hosts(IP addresses)IP(Internet Protocol)-datagramservices: best-effort delivery service no guarantees, unreliable service 2. Multiplexing &amp; DemultiplexingConclusion：在多主机、多应用、多进程、多socket上，不同应用间的通信是通过segment在socket中传输实现的，多路复用/解复用就用于定位一个segment的目的socket与源socket是什么，解复用是发送至socket，多路复用是从socket接收并发至网络层，这通过两者实现：1）唯一标识符socket，2）segment中指定源与目的socket port number。UDP socket只关注目的地，而TCP socket由于面向连接，会建立源-目的地的一对一连接。已知：多主机，多应用，一个应用对应多进程，一个进程对应多socket（多线程），segment要通过socket传输。需求：问题在于如何找到正确的socket，多路复用/解复用服务能解决这个问题。所有计算机网络都需要多路复用/解复用服务。2.1 definition demultiplexing，解复用，就是将transport-layer segment中的data传送到正确的socket中 multiplexing，多路复用，将从不同socket接收到的data chunks封装成segments，添加头信息（用于解复用），发送至network layer 要实现这项技术这需要： unique identifiers for sockets 唯一标识符socket segment中含有字段指定destination socket，而这包括source port number &amp; destination port number（每个端口号都是16-bit number，传输层一般指定1024-65535 ，0-1023是著名端口号，一般为一些广泛使用的application保留，如HTTP-80） 一般来说，应用的client side会让传输层自动生成socket端口号，而server side则指定一个特定端口号2.2 Multiplexing &amp; Demultiplexing in UDP$return\\ address = source\\ port\\ number + source\\ IP\\ address$$UDP\\ socket= (destination\\ IP\\ address,\\ destination\\ port\\ number)$UDP只关注目的地，一个UDP socket能接收source不同、发送destination不同的信息。2.3 Multiplexing &amp; Demultiplexing in TCP$TCP\\ socket=(source\\ IP\\ address,\\ source\\ port\\ number,\\ destination\\ IP\\ address,destination\\ port\\ number)$TCP关注source &amp; destination，完全双向一对一连接。原理： TCP server具有一个welcoming socket，用于等待连接请求 TCP client创建一个socket，通过其发送连接请求segment，会在其中指定其该四元组 TCP server收到连接请求后，根据该请求segment中的四元组创建一个新的socket2.4 Security攻击者可以监听/攻击一些知名应用的默认端口，造成其缓冲区溢出，可能可以在该主机上执行任何代码。因此有必要查询本机中监听端口的进程，可以用nmap。3. UDP(User Datagram Protocol)Conclusion：UDP header中有源端口号、目的端口号（用于多路复用/解复用），length与checksum（提供data integrity服务，将16bit data相加后取反，进位时最低位+1，接收时data+checksum全为1则说明无误）。使用UDP的应用有：DNS（实时性、无需连接、减小内存占用、UDP header字节更少更轻量），SNMP，NFS。UDP从应用程序进程中获取消息，为多路复用/解复用服务附加源端口号和目的端口号字段，添加另外两个小字段，并将结果段传递给网络层。why connectionless: no handshaking before communication3.1 UDP segment structureUDP header有8 bytes，指定了源和目的的端口号用于multiplexing &amp; demultiplexing，而源IP和目的IP是由network layer封装的，在IP datagram header，作为伪UDP segment header。3.1.1 checksumUDP提供error detection功能，考虑到既不能保证链路的可靠性（link-layer protocol不提供reliability）也不能保证内存中的错误检测，如果端端数据传输服务要提供错误检测，UDP必须在传输层提供端到端错误检测，但是UDP只检查错误，废除segment或warning，不会correct。类似based on UDP的应用可以在应用层提供可靠数据传输，主要是衡量在更低或更高级别实现该功能的成本与必要性。UDP header中用checksum检测是否出错： 发送方将segment中所有的16-bit字相加，循环进位（进位时最低位+1），再求反码（对无符号二进制数字来说，先反后加和先加后反是一样的），作为header中checksum字段的值，发送segment 接收时计算data中所有的16-bit字与header中checksum之和，若全为1则正确，若有一个0则说明该packet出错3.2 Applications running on UDP DNS Finer application-level control over what data is sent, and when 应用程序级控制什么时间发送什么数据（TCP有拥堵控制，会造成延迟，有些应用能够容忍data loss） No connection establishment 无需连接，三次握手很耗时；这就是为什么DNS用UDP。HTTP用TCP（需要可靠数据传输），针对使用后者造成的延迟，chrome使用QUIC protocol(Quick UDP Internet Connection，在UDP作为传输层协议的基础上，在应用层协议上实现了可靠数据传输) No connection state 记录连接状态需要缓冲区与拥堵控制参数，会占用内存 Small packet header overhead，TCP segment header-20 bytes，UDP segment header-8 bytes SNMP(network management) NFS(remote file server)4. Reliable Data Transfer本章只讨论unidirectional data transfer(only sender-&gt;receiver)的RDT protocols。bidirectional(full-duplex) 参考：[可靠数据传输原理 YieldNull](https://yieldnull.com/blog/943b65e3a64843303b8f15e1acbf79a77ace947f/#21-rdt-21-acknak) Conclusion： rdt1.0：假设信道完全可靠，接收方无需任何feedback。 rdt2.x：假设信道存在bit error，即packet可能受损但不丢包。 rdt2.0：接收方采用ACK, NAK, ARQ，接收方确认收到传送ACK，出错传送NAK，发送方收到NAK则重传。 rdt2.1：假设ACK/NAK packet也可能受损，发送方只要收到受损的feedback packet就重传，为排除冗余packet，为数据packet添加单bit序号字段1/0，接收方使用ACK, NAK 1/0。 rdt2.2：接收方移除NAK，采用ACK 1/0。 rdt3.0：假设信道存在bit error且lossy，即packet可能受损或可能丢包，发送方发送每个packet时启动定时器，超时则重传，接收方使用ACK 1/0。 pipelined：rdt协议的缺陷是由于其核心为停等协议，每次传输都至少要等待一个RTT，因此采用流水线设计，不需要等待前序packet被确认收到。 Go Back N：序号范围扩大，双方都需要维持缓冲区。发送方维护一个全局计时器和一个窗口(base,nextseqnum)，base为最小未确认packet的序号，每次更新base时重启计时器，超时重传当前窗口所有未确认的分组。接收方采用累计确认ACK k，不符合当前需求序号k的packet一律丢弃，即接收方保证正确、正序确认packet。 Selective Repeat：双方都维持缓冲区和窗口，且由于feedback的延迟或丢包，窗口不同步。发送方为每个分组维持一个定时器，超时重传。接收方只要确认收到当前窗口和上一窗口范围内的分组，就发送ACK k（即使冗余），防止由于ACK丢失，发送方一直重传。 4.1 rdt1.0完全可靠信道，假定双方发送/接收速率一致，理想状态，也最简单，一发一收就行，接收方无需任何反馈信息。4.2 rdt2.0比特差错信道，收到的packet可能受损。需要以下功能： ACK(postive acknowledg) NAK(negative acknowledge) ARQ(Automatic Repeat reQuest)，自动重传请求，存在比特差错则请求重传 实现ARQ还需要三种协议功能：检测、反馈、重传 error detection, 发送方在packet中添加额外的checksum bit字段用于检测 receiver feedback，接收方反馈是否正确接收 retransmission，发送方重传但ACK/NAK packet的命也是命但ACK/NAK packet也可能受损，解决方法是： 收到受损的feedback packet就直接重传，这样会导致冗余分组（duplicate packet），即接收方不确定该packet是新packet还是重传 针对这个问题，发送方为每个packet添加序号字段，接收方检测其序号即可确定是否重传（对于停等协议，由于发送方必须确保当前特定packet送达才能继续传送下一个，时序性是一定的，只要确定序号字段是否改变即可，可以用1/0）4.2.2 rdt2.1针对ACK/NAK packet可能受损的问题进行改进，发送方为每个packet添加单bit序号1/0，只要收到受损的feedback packet就重传，接收方通过检测序号是否匹配状态来判断是否冗余分组。4.2.2 rtd2.2移除了NAK，只使用ACK 1/0。发送方：收到的ACK序号与状态不匹配则重传，反之switch接收方：若packet受损则发送与状态相反的ACK，反之发送与packet序号一致的ACK4.3 rdt3.0比特差错、丢包信道，现在涉及到受损、丢包、超时和乱序的情况，相比rdt2.2，发送方每次发送packet都会启动定时器。 检测丢包（设置定时器，不论能否确定丢包都重发） 丢包后如何弥补（ARQ，重传机制；rdt2.2，处理冗余packet）接收方与rdt2.2是一致的，只关心收到的packet，错误则传送与状态相反标号的ACK，其余情况传送与packet相同标号的ACK，只有与当前所在状态标号相同的时候向上层传输数据。发送方每次发送packet都启动定时器，每隔一个cycle就重传，直到接收到正确的ACK，关闭定时器，切换到下一状态，等待上层再次调用，该过程中忽略所有可能延迟到达的receiver feedback。由于01序号交替，也被称为alter-nating-bit protocol比特交替协议4.4 Pipelined Reliable Data Transfer Protocolsrdt协议的缺陷是由于其核心为停等协议，每次传输都至少要等待一个RTT，效率低下，因此采用pipeline技术，发送端能任意发送多个分组到信道中，毋须按照严格时序。shortage： rdt3.0的比特交替分组编号方式（0-1）失效，需要增加编号范围 分组可能会失序到达，双方都需要建立缓冲区分配序号：参数： N：这里的N不是固定的，$N=nextseqnum-base+1$，但有上限 base：最小未被确认packet的序号 nextseqnum：下一个等待被调用发送的packet序号序号范围：一般序号会承载在packet header的一个固定字段中，序号范围由字段大小k进行确定，为[0,2^k^-1]。当序号用完之后，进行取模运算，从头开始编号。由于序号是取模分配的，如果窗口长度N和序号范围太接近，在一个窗口中存在两个相同序号，产生冲突。对选择重传协议而言，窗口长度必须小于或等于序号空间大小的一半。两种方式处理数据丢失、损坏、失序、超时： Go Back N，退回N步 Selective Repeat，选择4.4.1 Go Back N初始情况下，base=1, nextseqnum=1。发送方要响应三个事件： 上层调用 receiver feedback error，继续等 ACK k，base=K+1，移动窗口 若所有分组已确认（也没有要发送的了），关闭计时器等待上层再次调用 若还有分组未确认，重启定时器 time out，GBN协议重传所有已发送但未被确认（窗口内）的分组发送方：发送方只有一个全局timer，每次所有已发送packet已确认时停止，每次超时、base未确认，都会重启定时器。接收方：重传之后累计确认的问题：由于接收方按序接收，假设packet k超时，从它开始后面的数据都会被丢弃/缓存，无法确认，因此累计到的expectedseqnum也不会变，依然还是k-1，直到确认packet k开始。shortage： 一旦某个分组超时/丢失，就引起大量分组重传，成本较大 接收方的累计确认机制会丢弃乱序的分组，造成重传4.4.2 Selective Repeat选择重传是对退回N步协议的改进，即发送方只会重传那些它怀疑在接收方出错（丢失或受损）的分组，而接收方将失序但正确的分组缓存起来，从而避免不必要的重传。相比Go Back N，其为每个packet维护一个定时器，超时未确认则重传。由于ACK也有丢包的可能性，每次接收方收到上个窗口[rcv_base-N,rcv_base)的packet，都返回ACK，防止发送方未收到ACK一直重传。5. TCP(Transmission Control Protocol)Conclusion：TCP的特征为面向连接（体现在3次握手）、全双工、点对点（多重广播）、双向传输。TCP的连接管理： 3次握手：client开辟连接，发送SYN segment（SYN=1)，server发送SYNACK segment（单独ACK不包含任何数据，不是捎带确认，该确认segment中会给予该SYN segment一个cookie，此时不开辟连接，防止SYN flood attack），client发送ACK segment，可能包含请求数据，收到后server开辟连接。 关闭连接：client发送FIN segment（FIN=1）请求关闭，server发送ACK确认后一段时间再发送FIN segment，client确认后，两端真正关闭连接。TCP提供RDT、流控制、拥塞控制服务。 RDT：发送方维持一个定时器，超时重传，并累计确认，收到冗余ACK就快速重传，无需等到超时。 流控制：在header中维持一个receive window（用最后读取、最后接收、最后发送、最后确认与buffer size来表示）。 拥塞控制： 通过cwnd控制速度； 丢包（超时/冗余ACK x3）视为拥堵，收到一个ACK则增大cwnd，pushy strategy； 拥塞控制算法：初始cwnd=MSS，慢启动（cwnd+=MSS，指数增长）、拥塞避免（cwnd+=MSS*MSS/cwnd，线性增长）、快速恢复（cwnd+=MSS，指数增长），只要丢包（timeout/duplicate ACK）ssthresh=cwnd/2且重传，超时则重启慢启动，冗余ACK则cwnd=ssthresh+3MSS，进入快速恢复，直到首次收到新的ACK，视为拥塞结束，进入拥塞避免。 5.1 DefinitionFeatures connection-oriented: 3-way handshake full-duplex service point-to-point, multicasting bidirectional MSS, MTU, TCP segment sizeMSS（Maximum Segment Size）是指TCP协议中一个数据段（segment）能够承载的最大数据量（不包括TCP header，只是应用层所需data field），它是TCP连接双方在握手过程中协商得出的结果。MTU（Maximum Transmission Unit）是指数据链路层协议中一个数据帧（frame）能够承载的最大数据量。TCP segment size指的是一个TCP数据段的大小，它包括TCP首部和数据部分。发送方根据MTU的大小来调整MSS的大小，以确保发送的TCP数据段可以被正确地传输和接收。TCP segment structureTCP将数据看作无组织但有序的字节流，为每个字节编号与累计确认来实现有序确认。Sequence number: 为file data的每个字节编号，[0,MSS]为segment 1，[MSS,2MSS]为segment 2，以此类推。但一般会随机初始化序号，否则容易造成冲突。acknowledgement number：TCP可能收到乱序的segment，此时会将其缓存等待重排。TCP接收方会在acknowlegment number field填入其需求的最小的字节编号，比如已收到[0,55],[100,155]，其发送给发送方的segment中acknowlegment number为56。TCP采用累计确认机制（cumulative acknowledgements），每次只确认最小的缺失字节编号。捎带确认：接收端发送数据时顺带地返回一个确认（ACK）信息，而不需要独立地发送一个ACK消息。发送端只能独立发送ACK消息。减少网络上的通信次数，提高网络效率。以下情况接收端必须独立发送ACK： 接收端接收到SYN segment时，需要独立发送SYNACK作为第二次握手的响应。 接收端接收到一个受损segment时。捎带确认只能确认已经正确接收的数据。Telnet：用于远程登陆的应用层协议5.2 RTT Estimation and Timeout$EstimatedRTT=(1-\\alpha) \\cdot EstimatedRTT +\\alpha \\cdot SampleRTT $TCP通过一定策略计算SampleRTT（不会采样每个segment），并计算其指数加权移动平均(EWMA)，估计出EstimatedRTT。 $DevRTT=(1-\\beta)\\cdot DevRTT+\\beta \\cdot SampleRTT-EstimatedRTT $ DevRTT是预测值与测量值差值的EWMA。$TimeoutInterval=EstimatedRTT+4\\cdot DevRTT$出现超时后，TimeoutInterval应当加倍，以免即将被确认的后继报文段过早出现超时。但只要收到报文段并更新EstimatedRTT，就再次计算TimeoutInterval。5.3 RDT service in TCPassumption：发送方不受TCP流控制、拥塞控制的限制；数据长度小于MSS（不需要切割），且单向数据传送。TCP在发送方部署一个single timer，发送方响应以下事件： call from above，从上层接受数据，若定时器未运行，启动定时器； Timeout，重传最小序号的未应答segment，启动定时器； receiver feedback，收到ACK k，TCP采用累计确认，这可以说明k以前的字节都已经收到，故更新base。若当前仍然存在未应答segment，启动定时器。快速重传duplicate ACK：假设接收方当前需求为segment y，接收方只要收到的segment k(k&gt;y)，就发送冗余ACK y，发送方只要收到3次ACK y，就快速重传y。此外，由于TCP采用累计确认，只要收到ACK k(k&gt;y)，就会在超时前重启计时器，无需重传segment y。TCP不是GBN协议（不会重传一整个窗口），也不完全是SR协议（TCP可以有选择地确认失序报文段），可以被分类为其混合体。5.4 FLow Control用receive window来实现Flow Control。 receiver： LastByteRead:应用从缓存中读取的最后一个字节编号 LastByteRcvd: 最后一个放入缓存的字节编号 RcvBuffer: total buffer size rwnd: receive window=RcvBuffer-[LastByteRcvd-LastByteRead] sender： LastByteSent LastByteAcked $LastByteSent-LastByteAcked≤ rwnd$当rwnd为0时，sender会一直继续发送单字节数据segment，这些segment会被receiver确认，直到确认segment中包含一个非0的rwnd。5.5 Connection Management3-way handshake client TCP发送SYN segment，SYN=1，序号client_isn随机初始化 server为该连接分配TCP缓存和变量，向client TCP发送SYNACK segment，SYN=1，ACK=client_isn+1，包含初始化的server_isn client为该连接分配缓存和变量，向server发送segment，SYN=0，ACK=server_isn+1，该segment可以负载dataclose client发送close request，segment中FIN=1 server发送ACK，再发送其终止segment，FIN=1 client发送ACK，client TCP等待30s后终止，释放资源 server收到ACK，server TCP终止，释放资源SYN flood attack - classic Dos attack向服务器发送多个SYN segment，server会在第二次握手为其开辟空间，导致服务器连接资源殆尽SYN cookie：收到连接请求时生成一个初始TCP序列号，与SYN segment的源地址有关，作为cookie，不开辟空间，发送含有该序列号的SYNACK segment；当client返回第三次握手的ACK segment，验证该ACK segment序号与SYNACK segment中的cookie值匹配，此时server才生成连接。5.6 Congestion Control根据【网络层是否为运输层拥塞控制提供了显式帮助】来分类： end-to-end，无关网络层，TCP采用这种方法 网络辅助的拥塞控制，routers向发送方提供显式反馈信息。 routers-sender：choke packet receiver-sender：路由器标记packet中的某个field，收到flagged packet后，receiver通知sender，这种通知至少要经过一个RTT TCP Congestion ControlTCP拥塞控制通过三个方面实现： TCP发送方能够限制发送流量的速率 TCP发送方能够感知拥塞 TCP发送方感知到拥塞时，如何应对限制流量速率变量： LastByteSent LastByteAcked cwnd，congestion window$LastByteSend-LastAcked≤min{cwnd,rwnd}$v=cwnd/RTT，通过调整cwnd的值，可以调整连接发送数据的速率感知拥塞丢包：timeout/3 duplicate ACK(4 ACK in total)，视作拥堵，减小cwndACK segment：TCP是self-clocking的，使用确认/计时来增大cwnd带宽检测：只要收到ACK，一直增加cwnd，直到出现丢包，减小该速率，然后再次开始探测（增大cwnd），总之就是不停试探边界拥塞控制算法 慢启动slow-start（强制） initial：cwnd≤MSS，v=MSS/RTT 收到一个首次确认的ACK就翻倍，MSS*2^n^ 若出现拥塞，更新慢启动阈值ssthresh（slow-start thresh）=cwnd/2 超时，cwnd重置为1MSS，重新开始慢启动 3 冗余ACK，TCP快速重传，cwnd=ssthresh+3MSS，进入快速恢复 当cwnd≥ssthresh，结束慢启动，进入拥塞避免 拥塞避免（强制） 每个RTT，cwnd+=MSS，线性增加；例如，对每个到达的ACK，cwnd+=MSS*MSS/cwnd 拥塞，ssthresh=0.5 cwnd timeout，cwnd=1MSS 三个冗余ACK，cwnd=ssthresh+3MSS，进入快速恢复 快速恢复（推荐） 收到ACK，cwnd=ssthresh，切换拥塞避免 拥塞 超时，ssthresh=cwnd/2，cwnd=1 MSS，重启慢启动 冗余ACK，cwnd+=MSS，快速恢复的主体 AIMD(Additive-Increase, Multiplicative-Decrease, AIMD)，加性增、乘性减：TCP线性增加其cwnd，直到出现3个冗余ACK，cwnd减半，再开始线性增长，不断探测可用带宽高度理想化的TCP稳态动态性模型：$一条连接的平均吞吐量=\\frac{0.75W}{RTT}$W:当前cwnd经高带宽路径的TCP：$一条连接的平均吞吐量=\\frac{1.22MSS}{RTT \\sqrt{L}}$" }, { "title": "Computer Network: Data Plane of Network Layer", "url": "/posts/NetworkLayerDataPlane/", "categories": "Courses", "tags": "Computer Network", "date": "2023-02-10 09:36:30 +0800", "snippet": " Network Layer: Data Plane 1. Network Layer Services 2. Router 2.1 input port processing 2.2 Switching fabric 2.2.1 switching via memory 2.2.2 Switching via a bus 2.2.3 Switching via an interconnection network 2.3 Output Port Processing 2.3.1 Queuing 2.3.2 Packet Scheduling 3. Internet Protocol(IP) Network Layer: Data Plane1. Network Layer Services forwarding，data plane, hardware routing，control plane，routing algorithm, software, according to forwarding table(packet header-output matching)SDN(Software-Defined Networking)，软件定义网络services: Guaranteed delivery，确保交付 Guaranteed delivery with bounded delay，在一定时延范围内确保交付 In-order packet delivery，有序交付 Guaranteed minimal bandwidth，最小带宽确保交付 Security2. Router2.1 input port processing match: lookup, longest prefix-matching rule action: -&gt;switching fabric(queued here, then-&gt;the specific output port-&gt;output link)存储转发表的copy2.2 Switching fabric2.2.1 switching via memorypackets需要先被存储在内存中，再转发。obviously limited by the memory bandwidth，路由器的总转发量（B/2）不超过内存带宽（B）的一半，为了防止读写冲突、防止输出端口阻塞。2.2.2 Switching via a bus输入端口为packet标记其输入输出端口信息，一次只有一个数据包在一根shared bus上，每个output port都会接收到该数据包，但只有匹配的output port会保留它，此后移除掉标签信息。obviously limited by the bus speed2.2.3 Switching via an interconnection network解决了bus speed limitations，non-blocking（因为有多条总线，但单条总线上一次还是只能传输一个packet），对于N input port to N output port，共有2N buses2.3 Output Port Processing2.3.1 Queuing Input Queuing，Head-Of-the-Line, HOL, 线路前部阻塞 Output Queuing，多个packet同时到达输出端口，此时需要排队等待传输到output link，当缓存区满需要丢包，移除新到达的（drop-tail） or移除已经在排队的分组。2.3.2 Packet Schedulingon the output port FIFO/FCFS Priority Queuing，高优先级队列优先 non-preemptive，非抢占，不截断当前packet的传输 preemptive RR/WFQ(Weighted Fair Queuing) RR: multiple classes轮换，根据work-conserving queuing，只要当前类别队列为空，就立刻轮换到下一个非空类别队列 WFQ：与RR相比，为每个class分配了权重，w与每个类在任何时间内可能收到的不同数量的packet有关。 3. Internet Protocol(IP)fragment: 被MTU限制，IP datagram中的data可能需要被切割成fragments（片），但在到达transport layer前进行reassemble。切割和重组一般都在网络层的路由器中进行。如何组装：用header中的第二行，来自同一个datagram的（identifier，源和目的IP地址）一致（三元组标识）；最后一个fragment的flags=0，其余设为1；offset用于指定其在原datagram中的位置，检查是否有片段遗失。interface: between host and link; between a link and a router;在Internet中，每台主机接口、路由器接口都要拥有全球唯一的IP地址点分十进制记法Internet地址分配策略：无类别域间路由选择（Classless Interdomain Routing，CIDR），一般化子网寻址network mask 子网掩码表示方法：a.b.c.d/x(223.1.1.0/24)， x为该子网prefix长度，转发时只需要考虑前x bitsaddress aggregation/route aggregation/route summarization: ISP向外界通告“send me目的IP为所在a.b.c.d/x（包括多个子网）的所有datagram”。有点像聚合多个子网，找到更短的共同prefix。路由选择时采用最长前缀匹配，classful addressing分类编址：A-8 bits，B-16bits，C-24bitsbroacast：255.255.255.255，报文交付给同一网络中的所有主机DHCP(Dynamic Host Configuration Protocol)，动态主机配置协议，自动将一个主机接入网络，即插即用/零配置，即网络管理员不必手动配置；client-server，server给新到达网络的client分配IP地址 client发送DHCP discover message in UDP segment，用广播机制，让DHCP 发现自己 DHCP server收到DHCP discover message，发送DHCP offer message作为回应，也用广播机制，便于client找到最近的server，包含推荐的客户IP地址、子网掩码和地址租用期（该分配的IP地址是有期限的） client选择server，向其发送DHCP request message server发送DHCP ACK message进行确认，client收到后交互完成Network Address Translation，NAT，网络地址转换用于专用网络或具有专用地址的地域NAT后的主机发送datagram，经过NAT，NAT路由器重写datagram的目的IP与port，再发出，收到响应时根据NAT转换表传送给真正的目的主机NAT穿越（NAT traversal）IPv6现实原因：IPv4的32位地址快用完了 fragments：不允许在intermediate上切割/重组，只能在source和destination中进行，如果超出链路限制，路由器会直接drop并发送ICMP报错 checksum：既然transport和link layer都提供我们网络层就不提供了，否则每一跳都要计算checksum代价很大，这是ipv4的弊端 options：放在next header field中IPv4-&gt;IPv6流表matching+actionactions forwarding，Forward(路由器的interface) dropping modify-fieldfunctions： simple forwarding 负载均衡（分流） firewall（只匹配转发特定地址的流量）monolithically/monolithic 武断的euphemism 委婉说法" }, { "title": "Computer Network: Control Plane of Network Layer", "url": "/posts/NetworkLayerControlPlane/", "categories": "Courses", "tags": "Computer Network", "date": "2023-02-10 09:36:30 +0800", "snippet": " Network Layer: Control Plane RIP, Router Information Protocol OSPF(Open Shortest Path First，开放式最短路径优先) 层次化路由 Network Layer: Control Plane路由算法 集中式：全局网络知识计算，需要知道网络中每条链路的开销，具有全局状态信息的算法被称为链路状态 分散式：迭代、分布式计算最低开销路径静态/动态负载敏感/负载迟钝链路状态路由选择算法（Link state broadcast，LS算法），知道每条链路的开销 Dijkstra prim距离向量路由选择算法（Distance-Vector，DV），迭代异步分布式$d_x(y)=min_v{c(x,v)+d_v(y)}$链路开销改变与链路故障，只有更新最短链路开销时才广播，更新失效的链路增加毒性逆转，假设xyz三角，z-y-x最短，z向y发送的距离表将欺骗其z无法到达x，这样y就不会去尝试到达x，解决特定环路问题LS缺陷：全局泛洪，代价过大DV缺陷：无法收敛，无穷迭代RIP, Router Information Protocol基于DV算法周期性/请求，向邻居router节点发送距离矢量信息，更新到不同host节点的最短路径在网络层以进程方式实现，借助UDP协议传输距离矢量报文OSPF(Open Shortest Path First，开放式最短路径优先)AS, Autonomous System，自治系统Link State Protocol自治系统内部路由选择协议，向自治系统内所有其他路由器周期性（或发生变化时）广播更新路由选择信息，通过全网泛洪在IP数据包上直接传送，不需要用到传输层允许多个代价相同的路由，用不同方式计算代价，跳数/延迟层次化网络，每个link state packet可以仅在area内泛红，而不会波及到较大规模的全网 对相同AS非相同area内的通信，需要路由到backbone（骨干区域）再中转到对应目标 对不同AS，则路由到边界border路由器层次化路由规模性（代价）/管理性（可扩展性） 自治区内采用合适的路由选择协议和算法，内部网关协议 自治区间，BGPBGP, Border Gateway Protocol, 边界网关协议，基于改进的DV算法，会告知到达子网的详细AS路径，避免环路，加快收敛AS间，eBGPAS内，iBGP，毫无隐瞒，最大性能网关路由器运行eBGP and iBGP收集子网内的路由可达信息（iBGP），通过TCP segment传达给另一个自治区的网关路由器（eBGP） AS-path：前缀通告所经过的AS及代价（AS内部代价和，注意AS视作一个点 ） next hop：链路节点，热土豆路由，谁最近选谁策略：为路径打分，代价/安全性；接收/过滤，通告/隐瞒SDNmatch-action北向接口：操作网络应用南向接口：转发流表" }, { "title": "Computer Network: Application Layer", "url": "/posts/Application/", "categories": "Courses", "tags": "Computer Network", "date": "2023-02-10 09:36:30 +0800", "snippet": " Overview The * Layer Internet Model Applications 1. Principles of network applications 1.1 Network application architectures client-server architecture(client, server, data center, advantages and shortages, examples) P2P architecture 1.2 Processes Communicating object interface what application developer controls 1.3 Transport Services Available to Applications 4 dimensions 1.4 Transport Services Provided by the Internet TCP Secure Sockets Layer(SSL) UDP 1.5 Application-Layer Protocols what a protocol determines public/proprietary 2. The Web and HTTP 2.1 HTTP(Hyper Text Transfer Protocol) architecture: client-server transport protocol: TCP 2.2 Non-Persistent and Persistent Connections Non-Persistent connections round-trip time(RTT) shortcomings persistent connections 2.3 HTTP Message Format Request Response Message 2.4 User-Server Interaction: Cookies 2.5 Web Caching advantages Content Distribution Networks(CDNs) Conditional GET 3. Electronic Mail 3.1 Internet mail system 3.2 Comparisons between HTTP &amp; SMTP 3.3 Mail Message Formats 3.4 Mail Access Protocols Post Office Protocol—Version 3 (POP3) Internet Mail Access Protocol (IMAP) 4. DNS(Domain Name System) 4.1 Services provided by DNS definition 4.2 How DNS Works 3 classes of DNS servers cache 4.3 DNS Records and Messages 参数 安全性 4.4 Distribution P2P File Distribution DASH CDN (Content Distribution Network) cluster selection strategies 4.5 Netflix, YouTube, and kankan Netflix YouTube 4.6 client-server application using UDP/TCP OverviewThe * Layer Internet Model layer application protocol Application 两个应用间的双向可靠字节流传输，例如http/bit-torrent HTTP/SMTP/SSH/FTP Transport TCP/IP保证数据可靠传输；保证正确数据传输顺序正确；UDP TCP/UDP/RTP Network IP is unreliable and makes no guarantees；没有可靠数据传输的保证 IP Link 通过host和router之间或router之间的单个link来传送数据 Ethernet/wifi/DSL/3G Physical     数据传输：7层模型：Applications1. Principles of network applications1.1 Network application architecturesSummary: architectures for applications , not networkConclusion：应用程序的主流体系架构有client-server与P2P两种。 在client-server中，存在有client、server、多台dedicated servers组成的data center几种host，客户端之间不能直接通信，服务端是固定的、等待连接的；采用data center集成服务器的好处是有利于响应客户端的所有要求，减少单个服务器的拥堵/过载（集成服务，一对一-&gt;多对多），坏处是提高了带宽代价（servers间的反复互连）。eg. FTP/Web/E-MAIL 在P2P中，客户端之间进行直连通信，其优点为可扩展性、（对服务器，因为几乎不依赖服务器）低带宽代价，其缺点为安全性低（直连而不通过服务器显然隐蔽性低）、低性能（高度去中心化结构，散点结构）。eg. Skype/迅雷/BitTorrent。优缺点的评价维度如下：可扩展、集成服务、带宽代价、安全性、性能   client-server P2P scalability   high 集成服务 high low low bandwith cost No Yes security high low performance high low 我的理解是这样，P2P和中心化网络、分布式网络的区别应该在P2P是host-host，中心化是存在一个data center/中心服务器，分布式是存在多个分布的data center。改天问下chatgpt。。client-server architecture(client, server, data center, advantages and shortages, examples)Hosts: client, server, data center(multiple servers) server: fixed address 固定地址 always-on 等待连接 client: not directly communicate with each other 客户端之间不能直接相互通信 data center: including multiple servers 多个服务器组成 advantage: solve the problem of incapability of keeping up with all requests 响应需求↑ shortage: must pay recurring interconnection and bandwidth costs(with the designs with data centers) 带宽代价↑ examples: Web, FTP, Telnet, E-MAILP2P architecture peer-to-peer: directly, without passing through a dedicated server 客户端直连 advantage self-scalability 可扩展 cost effective, not require significant server infrastructure and server bandwidth 服务器带宽/代价↓ shortage: security, performance and reliability due to their highly decentralized structure 安全性、性能↓ example: BitTorrent, Xunlei, Skype1.2 Processes CommunicatingSummary: processes running on different hosts(potentially on different operating systems) communicate through application layerConclusion: application间的通信本质上是进程间通信，在不同OS、host上运行的进程通过application layer通信。一般将通信双方的进程标记为client/server，一个Web process可以既是client（browser process，download operation）也是server（server process，upload operation），该标记是人为的、非绝对的。要进行通信，首先要确定双方进程的地址，通过 [IP: port number]的形式定位，IP定位host，端口号定位该host上运行的process；在通信过程中，利用每个host上传输层与应用层/进程与网络之间的接口Socket。对于应用层开发者来说，其能控制的是Socket中所有应用层的部分，以及传输层协议及其部分参数（最大缓冲容量、最大段容量），该application将会建立在开发者选择的传输层协议所提供的服务之上。object label: client(download) process /&amp; server(upload) process Identify Processes: use IP addresses(32 bits, identify the host) and port number(identify the process) to address processes(example: [IP: port number],[localhost:8080])examples: a Web server is identified by port number 80. A mail server process (using the SMTP protocol) is identified by port number 25. interfaceinterface between process(application) and network, between application layer and transport layer within a host: socket (API)what application developer controls on the application layer side: everything of the socket （socket中的应用层部分 on the transport layer side which transport protocol to use，传输层协议 the capability to fix a few transport-layer parameters(maximum buffer, maximum segment sizes) 参数，最大缓冲，最大段 the application is built on transport-layer services provided by that protocol 该应用将建立在该协议提供的传输层服务上 1.3 Transport Services Available to ApplicationsConclusion：传输层协议能为应用程序提供可靠数据传输、吞吐量、实时性、安全性四个维度的服务。应用层将消息存入当前进程的socket，传输层将该消息发送给目标接收进程的socket。应用层send messages through local socket，传输层send it to the socket of destination process应用层request，传输层offer/ensure；什么甲乙方关系4 dimensions传输协议能提供service of 4 dimensions: reliable data transfer（not necessary for loss-tolerant applications, loss/data integrity/order) throughput(specific requirements for bandwidth-sensitive applications，比如视频切换清晰度，应用层可以切换编码方式来降低对带宽的需求; contrary: elastic applications) timing 实时性，与吞吐量有关 security1.4 Transport Services Provided by the Internet传输层协议及其提供的服务有TCP/UDP两种。TCP是面向连接的、需要三次握手的、全双工的、可靠数据传输的、有拥堵控制的；但它与UDP都没有加密技术（安全性缺陷），因此出现了SSL（Secure socket layer），一种加入加密、端点鉴别、数据完整性技术的改良TCP协议，具体而言就是在原本的TCP通信中添加一层加密/解密socket层（这意味着它有独立的socket API），client先在SSL socket加密再传送给TCP socket，server先解密再传送给TCP socket。当今的网络能够支持时间敏感的应用，但无法支持任何对实时性与吞吐量的保证（guarantee）。这些提供的服务与上一小节中的四个维度是对应的。TCP connection-oriented, handshaking, full-duplex connection reliable data transfer congestion-control(throughput, timing) no encryption(security)Secure Sockets Layer(SSL)SSL是对TCP的改良(TCP-enhanced-with-SSL)，而非一种独立的网络传输协议，其服务包括encryption, data integrity, end-point authentication端点鉴别。 SSL服务代码需要被写在client和server两端的应用层 有独立的socket API。 过程：client-SSL socket(client encryption)-client TCP socket-server TCP socket-SSL socket(server decryption)-server TCP socket-serverUDP connectionless, no handshaking unreliable data transfer no congestion-control mechanism1.5 Application-Layer ProtocolsConclusion: 应用层协议是网络应用的组成之一，决定了不同终端系统上的进程如何通信，具体决定了交换的信息类型、语法、字段语义、发送响应的规则。这些协议有些是定义在RFCs中的共有协议，如HTTP，有些则是私有的，如Skype使用的协议。what a protocol determines 交换的信息类型，request &amp; response syntax，消息类型的语法，不同字段field的意思 when &amp; how 发送消息&amp; responsepublic/proprietary public：一些应用层协议在RFC(请求注解，Request For Comments)中指定，因此属于公共领域。例如HTTP RFC proprietary：其余的则是私有的，例如Skype$application\\ layer\\ protocol \\in application$，一个client-server应用程序可以包括：client，server，application layer protocol，standards of content formatweb-mail-DNS-directory service video streaming-P2P2. The Web and HTTPthe World Wide Web – first Internet application2.1 HTTP(Hyper Text Transfer Protocol)Conclusion: HTTP(Hyper Text Transfer Protocol)超文本传输协议是Web的应用层协议，其为stateless protocol，不记录client的信息。Web是client-server架构的应用程序，其采用TCP传输层协议，client与server之间利用TCP socket进行通信，server将Web page中的objects(HTML files/images etc.)传送给client作为响应。HTTP, the application-layer protocol of WebWeb page(document) contains: object, such as an HTML file, image, addressable by an URLstateless protocol 不记录client的信息architecture: client-server a client program/Web browsers(IE, ME, Firefox，浏览器是客户端，是Web的组成之一) a server program/Web servers（管理并导向Web objects，web server always on with a fixed IP address）transport protocol: TCPcommunication: client-TCP socket of client side-TCP socket of server side-server2.2 Non-Persistent and Persistent ConnectionsConclusion：在client与server之间有两种TCP连接方式，一是非持久连接，另一种是持久连接，按字面意思，非持久连接是指每次请求响应都需要建立新的TCP连接，而持久连接则沿用同一个连接进行通信。每次连接需要三次握手，耗费近2个RTT（一个小数据包从客户端到服务器再回到客户端所花费的时间），其具体过程为，client发送TCP段请求连接（initiate）、server确认并响应TCP段，client确认并发送request message，此后server才将其需要的信息及索引发送给client。使用非持久连接的缺点很显然，内存（空间，每个新建的TCP连接参数都会占用空间）和延迟（时间，每次都需要至少2个RTT）。举例，假如client需要请求的web page有10个objects，使用非持久连接一共需要建立11个TCP连接。Non-Persistent connections非持久连接意味着每次请求、响应都需要建立新的连接。步骤如下： client初始化一个TCP connection在[server IP:80](80是HTTP的默认端口号） client通过该TCP socket发送HTTP请求信息，包含页面路由信息 server接收请求信息并响应其需求，通过TCP socket发送该web page需要的objects（先发送其引用，即这些objects的路由） server告知TCP可以关闭连接，但直到TCP确认client完整接收响应信息才会关闭。 接下来每个objects都会重复1-3步这意味着，假如有10个objects，一共需要生成11个TCP连接（最初的是web page请求，后续的是objects请求）。round-trip time(RTT)RTT: 一个小数据包从客户端到服务器再回到客户端所花费的时间。3次握手： client发送TCP段 server确认并响应一个TCP段 client确认（并发送request message）shortcomings 内存：每次创建新TCP connections需要为TCP variables分配内存，对于同时处理多clients请求的Web server来说是巨大的负担 延迟：每次都需要nearly 2 RTTs，一次建立连接，一次request &amp; responsepersistent connections为了解决这些shortages，引出persistent connections，即部署在同一个Web Server上对多个Web Page的请求响应可以在同一个TCP connection中进行。HTTP/2能允许按优先级排列同一个TCP connection中的请求与响应。2.3 HTTP Message FormatConclusion：主要介绍了HTTP message的格式，分为request message和response message。RequestGET /somedir/page.html HTTP/1.1Host: www.someschool.eduConnection: closeUser-agent: Mozilla/5.0Accept-language: fr// Entity bodyrequest line:method GET, mostly表单提交会用到GET而非POST，如www.somesite.com/search?a&amp;b POST, 此时request message的Entity body不为空，会携带client输入的信息 HEAD, 与GET类似，但只有response message，没有requested object，一般用于debug PUT, 用于upload an object on a Web server，一般用于Web发布工具一起使用 DELETE，用于delete an object on a Web serverheader lines: Host, needed by the web proxy caches Connection, non-persistent/persistent User-agent, what browser client uses Accept-language, preference in languages etc.Response MessageHTTP/1.1 200 OKConnection: closeDate: Tue, 18 Aug 2015 15:44:04 GMTServer: Apache/2.2.3 (CentOS)Last-Modified: Tue, 18 Aug 2015 15:11:03 GMTContent-Length: 6821Content-Type: text/html(data data data data data ...)status line: protocol version field status code 200 OK 301 Moved Permanently: requested object被永久移动，新的URL会在Location(one of the headers)中指出 400 Bad Request: incomprehensible request 404 Not Found: web page不存在于当前server 505 HTTP Version Not Supported: the requested protocol is not supported corresponding status messageheader lines: Connection: close-&gt;发完就删 Date，这是server检索所需object并封装发送的时间 server，generated by what web server Last-Modified，object最后修改日期 Content-Length Content-Typeenity body: requested data2.4 User-Server Interaction: CookiesConclusion：用cookies来记录用户信息及其在browser上的行为，弥补HTTP无状态特点的不足，在这里client是browser，因此建立的是browser-cookie的键值关系。创建TCP连接时，server确认连接请求时给予当前client一个unique token作为cookie，server会将该cookie-client的对应关系记录在其数据库中，而client收到该cookie后会记录在本地浏览器缓存中，每次发送请求的时候会包含cookie信息。stateless与cookies的区别：stateless是指不记录client的状态，如果是已经response的完全一致的请求，还会再次response。cookies是记录client的历史数据信息，从该user-agent发出，在该web page进行了什么行为。2.5 Web CachingConclusion：简单来说就是在原服务器与client之间添加一个代理服务器，作为web缓存，客户端先向代理服务器发送请求，确认缓存中没有需求信息后，再由代理服务器向原服务器发送请求。好处是加速、减少流量，通常client-proxy server之间的带宽限制相比client-original server的更宽松，对于ISP企业网络，能够减少当前机构局域网对互联网的访问流量，减少整个互联网上的流量。用conditional GET可以保证cache和server之间的数据一致性。Web Cache-proxy server，在与client通信时是server，在与original server通信时是client，通常被ISP部署（用于大型机构网络，校园网或企业网）advantages time cost↓，cache与client之间的传输速度上限可能比cache与server之间的传输速度上限大 Web缓存可以大大减少机构到Internet的访问链路上的流量 减少整个互联网流量Content Distribution Networks(CDNs)CDN公司在整个互联网上安装许多地理分布的缓存，从而本地化大部分流量。Conditional GET作用：用于判断cache中的object是否up to date，保证数据一致性（consistency between cache and server）。运行原理：client向cache请求object，假如其不在cache中，cache向server请求，此时server会将object的last modified信息一起发送，cache保存这个信息。若在cache中，cache会先向server发送一个conditional GET作为一个up-to-date的确认，该报文包含If-modified-since属性，值与last modified相同，假如没有改变，server会response with304 Not Modified，不会再次传送object（entity body为空）。此时cache确认后，可以直接将缓存中的object响应给client。3. Electronic Mail3.1 Internet mail systemConclusion: 互联网邮件系统主要包含用户代理、邮件服务器与SMTP三个组成部分，用户代理对邮件进行操作，服务器储存、传送邮件，SMTP为服务器之间的TCP连接传输遵循的协议。 user agents, where the user operates mail servers, both client and server among other mail servers，存储数据、传送数据 SMTP(Simple Mail Transfer Protocol)，mail servers之间的TCP连接通信遵循SMTP服务3.2 Comparisons between HTTP &amp; SMTP   HTTP SMTP pull/push pull protocol，侧重download被upload在web server上的文件，TCP连接由client初始化（发出请求） push protocol，侧重推送文件，TCP连接由发送邮件的mail server初始化 format limitations 无 信息格式limited，7-bit ASCII format objects-messages 将单个requested object封装在一条message，一对一 将所有objects封装在一条message 3.3 Mail Message FormatsFrom: alice@crepes.frTo: bob@hamburger.eduSubject: Searching for the meaning of life.telnet serverName 2525 is the port number of SMTP server3.4 Mail Access ProtocolsConclusion: 收信人不能再采用SMTP来接收message，因为SMTP是push protocol而非pull protocol，因此要采用mail access protocols: POP3, IMAP, HTTP。POP3的运行具有3阶段，认证（用户名密码登陆）、业务（撤回、标记删除邮件、保存邮件数据）、更新（根据标记删除邮件，quit结束会话）。IMAP相比POP3，能够保存会话中的用户状态信息，具有文件夹分类功能，这也导致其实现比POP3要复杂得多。Post Office Protocol—Version 3 (POP3)3 phases: authorization: username-password transaction: retrieve/mark for deletion/obtain data update: issue quit command, end the session; delete the marked messagescommands: list,retr,dele,quitInternet Mail Access Protocol (IMAP)相比POP3，多了文件夹分类功能，这意味着IMAP会保存IMAP session中的用户状态信息，例如分类和文件夹名4. DNS(Domain Name System)已知：用hostname和IP地址可以定位一个主机需求：路由器需要定长的、分级的IP地址，需要建立hostname到IP地址的映射，即域名解析DNS4.1 Services provided by DNSConclusion: DNS包括在DNS servers层次结构中实现的分布式数据库，以及允许host查询分布式数据库的application-layer protocol。DNS是一个应用，其提供的服务为伪域名查询和负载平衡。definition 在DNS服务器层次结构中实现的分布式数据库 允许主机查询分布式数据库的应用层协议一般是UNIX机器，DNS protocol一般在UDP和端口53上运行，一般被其他应用层协议，如HTTP,SMTP应用。应用原理： browser client将hostname传送给DNS应用的client side DNS client将hostname传送给server，等待server回应该hostname对应的IP地址 browser client接收DNS应用传来的IP地址，初始化TCP连接services: host/mail server aliasing，主机/邮箱伪域名，DNS能找到一个alias hostname的canonical hostname及其IP Load Distribution，负载平衡，对于重复的不同终端上的web servers，其canonical hostname一致，但拥有不同的IP地址，DNS能获取并响应这组IP地址，但会对其顺序进行轮换，由于client一般会按顺序优先采用这个地址集中的IP地址，从而起到了分流的作用。4.2 How DNS WorksConclusion: 中心化设计的DNS具有单点故障（一损俱损）、流量代价、中心数据库距离远、维持成本高（经常需要更新hosts信息）的缺点，因此DNS服务器采用分布式和分级设计，具有三层分级，root，TLD，authoritative，每一层分别提供下一层的IP地址，最后还有local DNS，作为host到三级DNS服务器的proxy server，也能储存TLD server和短期内查询过的hostname的缓存。shortages of a centralized design of DNS: single point of failure 单点故障 traffic volume 流量代价 distant centralized database 遥远 maintenance 维持成本（更新hosts信息）因此要有distributed and hierarchical design3 classes of DNS servers Root DNS servers, 提供TLD servers的IP地址 Top-level domain(TLD) servers, com, org, net, edu, and gov，提供authoritative DNS servers的IP地址 Authoritative DNS servers, 在Internet上拥有可公开访问的主机(如Web服务器和邮件服务器)的每个组织都必须提供可公开访问的DNS记录，这些记录包含hostname到IP地址的映射。组织的授权DNS服务器保存这些DNS记录。 local DNS server, 严格来说不属于DNS服务器层级，但是DNS架构的中心；每个ISP都会有一个local DNS server，作为一个在请求host和DNS server hierarchy（以上三种）之间的proxy server从请求主机到本地DNS服务器的查询是递归的，其余查询是迭代的。cachelocal DNS server could cache IP of TLD servers same hostnames recently4.3 DNS Records and MessagesResource records(RRs)：cache中对hostname-IP mappings的记录(Name, Value, Type, TTL)参数 Type Name Value Example A hostname IP address $(relay1.bar.foo.com, 145.37.93.126, A)$ NS domain 能提供该域名族host的IP地址的authoritative DNS server的hostname $(foo.com,dns.foo.com, NS) $ CNAME alias hostname canonical hostname $(foo.com,relay1.bar.foo.com, CNAME)$ MX alias hostname canonical name of a mail server $(foo.com, mail.bar.foo.com, MX)$ 安全性针对DNS的攻击： DDoS（分布式拒绝服务）带宽泛洪攻击，向每个root DNS server发送大量packet，使得大多数合法DNS请求得不到回答。 截获来自主机的请求，并返回伪造的回答，替换服务器的缓存信息，能将Web用户重定向到病毒Web站点。4.4 DistributionP2P File Distribution从单一服务器向大量主机分发一个大文件。将不同小的片段发送给不同主机，让主机之间相互匹配传输缺失的部分；比client-server更高效。eg. BitTorrentDASHDASH(Dynamic Adaptive Streaming over HTTP) 经HTTP的动态适应性流，改变视频流编码，360p-1080pCDN (Content Distribution Network)CDN，内容分发网在为client播放流媒体时缓存该视频，当缓存满了则移除较少访问的视频。分布方式： enter-deep：在ISPs附近布置服务器，这样服务器的数量会比较大，Akamai采用这种方法 bring-home：在Internet Exchange Points（IXPs）布置服务器，这样服务器数量较少，Limelight等CDN公司采用这种方法运行原理：cluster selection strategies geographically closest 地理最近，弊端是每次都固定一个cluster，不考虑现实延迟和宽带上限，有时地理最近在网络上不是最近，且有些终端与定位较远的LDNSs绑定 real-time measurements 实时测量，让CDN中的clusters周期性地测量到达速度最快的LDNSs4.5 Netflix, YouTube, and kankanNetflix网飞的web pages分发由Akamai公司运营，视频内容分发主要依靠Amazon cloud &amp; private CDN。该网站是在亚马逊云中的亚马逊服务器运营的，亚马逊云实现了以下功能： Content ingestion，把原片上传到亚马逊云中的hosts Content processing，亚马逊云对视频进行各种编码并存储，以适应不同终端、流畅度、清晰度播放（DASH） Uploading versions to its CDN，将这些不同版本的视频上传到其private CDNpush caching:Netflix CDN uses push caching rather than pull caching，意思是在非高峰期定期将内容推送到服务器，而非高峰期需要的时候再pullYouTubeprivate CDN/pull caching/cluster-selection：找出RTT最短的cluster，但为了保持平衡一般会通过DNS定向到更远的cluster/现在已经有了适应性流媒，有一些视频使用DASH（书是2017出版，2018 stackoverflow说youtube还在使用dash）4.6 client-server application using UDP/TCP记点好玩的：RFC 2324: “超文本咖啡壶控制协议”（Hyper Text Coffee Pot Control Protocol，乍有其事的写了HTCPCP这样看起来很专业的术语缩写字）。以及如前面所提到纪念RFC的30周年庆的RFC文件。" }, { "title": "计算机体系结构文献综述", "url": "/posts/computerArchitecture/", "categories": "Courses", "tags": "Computer Architecture", "date": "2023-01-14 09:36:30 +0800", "snippet": " 体系结构在大规模机器学习的应用 Introduction A Software-defined Tensor Streaming Multiprocessor for Large-scale Machine Learning 确定性执行与向外扩展的保证 网络拓扑结构 同步机制 软件调度网络（ Software-scheduled networking，SSN） 网络流量模式 调度策略 流控制 前向纠错 评析与总结 AStitch: Enabling a New Multi-dimensional Optimization Space for Memory-Intensive ML Training and Inference on Modern SIMT Architectures 多重运算器缝合方案 基于GPU执行的SIMT特性的任务打包和拆分方法 任务打包 任务拆分 自动编译器优化设计 主导性识别和操作分组 自适应线程映射和时间表传播 最终确定 评析与总结 结语 体系结构在大规模机器学习的应用Introduction随着对数据处理的规模与效率的需求发展，为了在软硬件之间取得平衡，从更底层的逻辑架构提高数据处理效率，计算机体系结构在与大数据、并行计算紧密相关的机器学习领域中得到广泛应用。恰当地选择和设计体系结构可以提高机器学习模型的性能，并使其能够在给定的资源限制下最大化其表现。目前，机器学习与体系结构的交叉领域面临着许多挑战，研究人员提出了可能的解决方式： 大数据问题：随着数据量的增加，训练和运行机器学习模型所需的计算资源也在增加。分布式体系结构可以帮助解决这个问题，例如使用分布式训练来加速模型的训练时间。对于高维数据，则可以借助机器学习优化编译器AStitch，拓展了新的多维优化空间。 模型复杂度问题：随着模型复杂度的增加，训练和运行模型所需的资源也会增加。结构化体系结构可以帮助解决这个问题，例如使用模型压缩技术来减小模型的大小。 隐式偏差问题：机器学习模型可能会在训练数据上过拟合，从而导致对新数据的不佳预测。结构化体系结构可以帮助解决这个问题，例如使用正则化技术来限制模型的复杂度。本次文献综述将围绕A Software-defined Tensor Streaming Multiprocessor for Large-scale Machine Learning（用于大规模机器学习的软件定义张量流多处理器）, AStitch: Enabling a New Multi-dimensional Optimization Space for Memory-Intensive ML Training and Inference on Modern SIMT Architectures（AStitch:为内存密集型ML训练和现代SIMT架构推理提供新的多维优化空间）两篇论文进行分析总结。之所以在给定论文中选定这两篇论文进行综述，是由于其主题较为接近且相似。这两篇论文针对机器学习中数据处理与模型训练的效率瓶颈，分别提出了一个张量流多处理器软件调度网络，以及AStitch，一个针对内存密集型机器学习的优化编译器。A Software-defined Tensor Streaming Multiprocessor for Large-scale Machine Learning目前，大规模机器学习模型的训练需求是双重的。 在网络层面上，训练ML模型的网络需求通常需要数据并行性(弱缩放)，这与使用流水线模型并行性(即并行性)对同一模型进行推断不同，后者需要强缩放。 在计算层面上，大型模型的计算需求通常需要大量内存资源来存储模型参数、常数和梯度，以适应每个处理元素（Processor Element，PE）的可用内存，并且在处理元素之间实现负载平衡。因此，在传统的CPU或GPU多处理器中，内存和网络资源在处理元素之间动态共享，这是不确定性的来源。基于该现状，该论文面向大规模机器学习，针对当前大规模机器学习模型训练的两难之处与需求瓶颈，提出了一种新型的、商用的、向外扩展张量流多处理器(Tensor Streaming Multiprocessor, TSP)的软件调度系统结构。软件定义的多TSP系统利用软件调度的高基数的蜻蜓拓扑网络和ISA “runtime deskew” 指令支持保持同步的假象，支持锁步系统，将单个TSP的确定性扩展到整个多TSP网络。论文从多TSP网络的拓扑结构、软件静态调度策略、同步、路由、流控制和容错等方面较为完备地描述了该系统架构，解决了传统多处理器中的不确定性难题，并赋予了网络可扩展性。确定性执行与向外扩展的保证确定性执行提供了一种机制，使得使用给定输入数据集进行的计算始终在限定时间内产生一致的输出。利用确定性执行可以保证软件行为的可预测性和可重现性。向外扩展性在集群系统（多个节点组成的系统）中主要以水平扩展方式（指增加节点的方式）来进行，突破单机限制，通过将多个低性能的机器组成一个分布式集群来共同抵御高并发流量的冲击，比如向原有的web、邮件系统添加一个新机器。根据该TSP网络向外扩展（Scale Out）、确定执行（Deterministic）系统架构的需求定位，在硬件上，该系统将基于蜻蜓拓扑的系统层次组织，以及处理元素之间芯片到芯片(C2C)链路的基本属性，保障其可扩展性（scalability）。在软件上，TSP的编程模型是基于静态调度策略，为参数和指令文本提供220 MiBytes的本地存储。每个TSP都使用生产者-消费者流编程模型进行编程，该模型允许将功能单元链接在一起。TSP的功能单元被组织为320元SIMD(单指令多数据)指令执行到能片组（functional slices）包括一组20个瓷砖，每个瓷砖对数据执行16路SIMD计算。网络拓扑结构蜻蜓拓扑（Dragonfly Topology）是由John Kim等人于2008年提出的一种网络拓扑结构，被广泛地应用在高性能计算网结构上。Dragonfly的结构有三个层级：路由器(Router)、组(Group)、系统(system)，它的拓扑结构一般由以下这些参数描述： 参数 描述 参数 描述 N 网络中的终端数 p 每个路由器连接的终端数 a 每个组的路由数 k 路由器的接口数 k’ 每一组的有效接口数 h 每个路由用于与其他组连接的频道个数 g 系统内组个数 q 输入端口队列长度 qvc 单个输出虚拟通道（VC）的队列深度 H 跳数 Out~i~ 路由i号输出端口     图 1 Dragonfly网络拓扑图如图1所示，最底层的每个路由器与p个终端链接，并拥有a-1个与组内路由器通信的本地频道(Local Channel)，以及h个与其他组路由连接的全局频道(Global Channel)，由此可以计算出每个路由器的接口数k=p+h+a-1。在组(Group)的层面上，每组连接着ap个终端，并拥有ah条全局频道的连接(ah connections to global channels)，可以将一个组抽象为拥有k’=a(p+h)个端口的虚拟路由器(virtual router)，由于其拥有非常高的路由数因此在系统层面的网络上可以拥有很短的全局网络直径(global diameter)^[1]^。在Dragonfly拓扑中组内路由的连接方式可以根据实际需求进行调节。组内的路由方式可以是任意的，如在Dragonfly+拓扑中组内使用胖树的结构。基于Dragonfly拓扑，论文提出的网络结构具有以下特征： 低网络直径：观察到的总通信延迟和方差随着网络跳数的增加而增加。因此，减小网络直径可以减少网络延迟(跳数)以及降低网络成本。 打包结构拓扑：互连网络的拓扑结构主要由系统封装层次结构所施加的封装约束驱动。 使用直连网络：如Figure 4，将端点processing point与router相连，直接连接tsp来创建它们的通信结构；减少indirect中由于引入路由器与交换机及其动态调度与排队的不确定性因素。同步机制通过分布式系统的同步机制，硬件将确定性执行的保证从单个TSP扩展到多TSP系统。同时，该同步机制也是TSP系统向外扩展性的保证。同步TSP网络涉及硬件和软件机制的组合。根据Table 1依次介绍ISA架构是如何支持确定性执行与向外扩展的： HAC指令：每个TSP维护一个自由运行的内部硬件对齐计数器(HAC)。 SYNC和NOTIFY指令：提供了一个芯片范围的同步机制，它依赖于共享时钟和完全确定的控制传播路径。公共HAC引用可以通过以下方式组合在整个系统中提供共享时钟的假象，因此，这实际上是一种模拟同步机制。 DESKEW指令：允许我们将程序执行与本地的HAC对齐，确保分布式计算可以相对于HAC epoch边界开始，该边界是网络中TSP端点之间的共享参考时间。 SAC指令：TSP提供了一个额外的软件对齐计数器(SAC)来允许TSP在计算期间重新同步，以保持独立TSP端点之间的累积漂移在允许的公差范围内。软件调度网络（ Software-scheduled networking，SSN）TSP体系结构的一个基本特征是它的确定性数据路径。所有指令的执行延迟都是静态的(在编译时)，因此通过ISA(指令集体系结构)向编译器公开。多TSP网络正是通过软件调度网络（SSN）来保证负载平衡的确定性。网络流量模式要启用SSN，需要知道通信或网络流量模式。对于multi-TSP目标的大多数工作负载，在空间和时间上的通信模式都是在编译时通信本身先验已知的。因此，SSN可以利用ML模型的静态计算图和流量模式的先验知识，做出最优的路由或“调度”决策。SSN实现了跨处理元素划分模型，同时考虑模型并行性(即跨tsp分布不同层)和数据并行性，以利用跨集群的小批并行性。模型分解由编译器自动执行，以便在所需数量的TSP元素之间自动缩放工作负载。编译器将工作负载划分为更小的子任务，并将它们映射到负责执行它们的各个tsp。多tsp系统利用蜻蜓的路径多样性，在可用链路上传播所提供的流量，这就保障了负载平衡的确定性。调度策略与传统的调度策略相比，以路由算法为例，路由算法确定消息在网络中所经过的路径，通常在硬件中使用查找表来实现，通过检查数据包的目的地节点为每个传入数据包提供简单的输出端口映射。相比之下，多tsp系统通过分别在源节点和目标节点上编排发送和接收指令序列，显式地控制和调度网络中的逐跳路径。假定所有数据移动都可以静态推断，编译器将基于跨时间和空间的全局信息编排数据移动，以消除共享输出端口的冲突。流控制在该多TSP网络中，根据论文描述，研究人员在TSP的节点之间与之内建立C2C（芯片-芯片）连接，作为在网络上传输数据的纽带。具体来说，当一个张量在网络中一跳一跳地流动时，我们使用每个TSP上的本地SRAM存储来为张量的各个向量提供中间缓冲，这样，一个向量就是流量控制单元(flit)。在硬件上，在多TSP网络中，除了C2C逻辑和TSP核心时钟边界之间的接口处有浅缓冲区外，没有数据中心网络中常见的传统缓冲区。这样避免了传统体系结构中虚拟/物理通道与缓冲区造成的拥堵与重发等调度所需的时间。相比之下，流控制是从软件层面进行的，通过软件调度网络防止路由死锁，减小了额外虚拟（物理）通道的复杂性与时空间成本。前向纠错在容错率上，TSP体系结构使用的是前向纠错方法，为了保持确定性，就地纠正任何传输错误，标记任何需要运行时软件重放的关键错误。具体做法为用网络链接上的FEC和整个TSP的内存系统、数据路径和指令缓冲区的单次纠错和双次错误检测（SECDED）来防止这些关键错误。评析与总结论文最后通过软件栈、分布式矩阵乘法、常见机器学习模型的训练对系统的性能进行了评估，作为对系统可行性与优越性的证明。论文提出的TSP体系结构，是在前人研究的基础上，对网络结构的一次创新。在相关领域的历史上，为了利用蜻蜓拓扑的路径多样性，研究人员已经提出了不同的全局自适应路由算法。然而，先前的工作都集中在基于硬件的蜻蜓网络，该论文则提出了基于软件调度的蜻蜓网络，通过改进后的直连网络结构与静态调度策略，最终在软件层面实现了确定执行的蜻蜓网络。我还注意到，论文提出的TSP体系结构使用的是芯片-芯片接口。C2C 接口的设计目的就是在移动应用处理器和调制解调器之间提供高效率的无缝接口，大幅降低成本，缩减印刷电路板 (PCB) 面积。有了高带宽低时延 C2C 接口，我们就不再需要调制解调器 DRAM，从而可节省金钱成本与空间资源。更重要的是，这种分离式架构可将两大领域的优势完美地结合起来，为移动产业带来最理想的创新环境^[2]^。在当前论文工作中，我认为可能具有的缺憾之处在于网络的流控制。论文提到，为了减小了额外虚拟/物理通道的复杂性与排队调度的不确定性，该TSP体系结构缺乏硬件流量控制；因此，流控制只从软件层面进行，通过静态调度策略防止路由死锁，并保障确定性。这意味着，在数据规模过大时，调度算法可能造成额外的内存资源占用或传输信息的缺失或错时。然而，在谈论计算性能时一向是时间复杂度与空间复杂度的相互转换，研究人员也一定考虑了这点，才做出将硬件流量控制剔除的选择。故而我认为，也许可以在此处实现软硬件流量控制相结合，或进一步优化调度算法。综上所述，该TSP体系结构为大规模并行机器学习的训练提出了一个优秀的网络结构，围绕该领域的网络需求与计算需求，实现了数据并行性与模型并行性，借助一定的调度策略将模型分离映射到对应的内存资源中，实现了确定性的负载平衡。其特色之处为软硬件结合，针对两个层面进行了改进，其一是确定性，该体系结构使用的软件调度网络具有显式的流量模式软件控制及其整个网络数据包的总顺序。其二，从降低通信成本的方面，它实现了TSP端点之间的确定性通信，消除延迟方差。AStitch: Enabling a New Multi-dimensional Optimization Space for Memory-Intensive ML Training and Inference on Modern SIMT Architectures机器学习模型通常包括两种类型的操作： 计算密集型操作通常由重型计算内核组成 ，例如GEMM/GEMV和卷积计算； 内存密集型操作通常受内存带宽的限制，例如按元素（element-wise）和减少（reduce）操作。 按元素（element-wise）操作：在一个按元素排列的操作中，各元素以按元素 排列的方式被独立处理。按元素计算的运算可以进一步分为轻型按元素计算的运算和重型按元素计算的运算。前者执行较轻的计算，如加和子，而后者执行明显更昂贵的计算（如tanh， power，和log）。 减少（reduce）操作：一个减少操作将一个张量作为输入，并还原其一个或多个维度。如果在一个维度上减少元素在内存中是连续的，则称为行还原；否则称为列还原。 论文揭示，在最近的机器学习模型中，内存密集型计算是一个比重不断上升的性能关键因素。其面临着如下的两难境地与挑战： 大量冗余计算造成内核占用率高，将冗余计算融合（fusion），维度过高的计算是昂贵的。这种复杂的两级依赖关系（运算符级别、操作进程层面）与及时性需求(Just in Time，JIT)相结合，加剧了训练/推理的低效率。 现实世界生产工作负载中的不规则张量形状，导致了模型的低并行性。基于该背景，论文提出AStitch，一个机器学习优化编译器，为内存密集型ML计算打开了一个新的多维优化空间。针对以上两个挑战，AStitch分别从两个方面提出了改进方法： 多重运算器缝合方案实现了分层的数据重用，能够有效地将任何运算器“缝合”在一起，而没有大量的计算冗余。论文用“缝合”来区别提出的的高级融合技术与现有的ML编译器融合方法：其对当前融合范围的扩展是将当前工作所实现的许多小型和基本的融合“缝合”成更大和更广泛的融合。 提出了自适应线程映射技术，其基于一种在GPU执行的SIMT（单指令多线程）特性的任务打包和拆分方法，以适应性地处理各种张量形状，并产生适当的线程映射时间表，以最大限度地提高硬件利用率和平行度多重运算器缝合方案论文抽象出四种算子拼接方案，涵盖了从依赖性、内存层次和并行性共同考虑的所有依赖情况。如Table1所示，各方案为： 独立方案代表相互独立的运算符。 本地方案代表相邻的运算符，具有元素级的一对一依赖（即元素级的方式），中间数据被缓冲在每线程寄存器中。这里线程级的数据定位得到了保证。这两种方案被最先进的设计所采用。 区域方案表示一对多的元素级别的依赖性。 全局方案是为了应对任何复杂的依赖关系，中间数据被缓冲在全局内存中。由于这是一个面向并行的方案，所以没有定位要求（全局内存对所有线程都是可见的）。Astitch会根据操作者的特性来决定局部性和并行性之间的权衡（即局部性与全局性）。Astitch实现了跨内存层次（即寄存器、共享内存和全局内存）的两级数据（元素级、操作员级别）重用，以消除融合的困境。 在Astitch中，对于一对多的元素级依赖关系，生产者对每个数据项只处理一次，不会产生多余的计算。其结果可以保留在GPU共享/全局内存缓冲区，供其消费者在区域/全局缝合方案中重复使用。 对于一对多的操作者级依赖关系，Astitch只处理一次生产者，并将其结果存放在缓冲区，供其多个消费者重用。对于本地缝合方案，待重用的数据被维护在寄存器上，而对于区域/全局内存，数据被维护在共享/全局内存上。基于GPU执行的SIMT特性的任务打包和拆分方法论文提出基于GPU执行的SIMT特性的任务打包和拆分方法，以适应性地处理各种张量形状。值得注意的是，受到不规则张量形状影响的主要是减少（reduce）操作，这是内存密集型计算中最耗时的操作。任务打包任务打包包括两个方面：水平和垂直。 横向打包是将多个小块打包，每个小块处理一行的还原，打包成一个大的线程块。 垂直打包是将多个线程块的任务打包成一个，以减少块的数量。这有助于将多个线程块打包成一波，以满足全局屏障的要求。任务拆分任务拆分是将一个线程块内的任务拆分成几个线程块，以增加块的数量，以防因块的数量少而造成利用率低的问题。自动编译器优化设计论文将以上方法嵌入编译器中，设计了一个自动编译器，能在一定程度上缓解内存密集型计算面对的计算与及时性（JIT）需求困境，也即是AStitch。AStitch主要完成的工作是确认缝合方案。其步骤如下：主导性识别和操作分组Astitch只需要确定几个关键运算符的线程映射，然后将其传播给所有其他运算符。我们将这些关键运算符命名为主导运算符。AStitch首先确定几个成为主导运算符的候选者，并最终确定具有主导合并的最终运算符。自适应线程映射和时间表传播AStitch为每个主导操作生成并行代码，并在相应的组内传播线程映射计划。通过这种方式，我们得到了所有操作者的线程映射时间表。张量形状适应。AStitch根据张量形状和硬件资源，根据上述的任务打包与拆分方法，自动为主导操作应用任务打包和拆分。以行缩减为例，如果要缩减的行数小于每波允许的块数，且每行包含大量的数据项（即大于1024），则AStitch会对行进行分割以增加并行性。最终确定AStitch在最后一步确定了主导和次主导的行动。减少操作会优先考虑并行性，因为它们需要更多的计算。因此，减少（reduce）操作主导的组将执行通过的块定位检查，而不调整其线程映射。另一方面，由于其低成本的计算，按元素（element-wise）操作通常优先考虑位置性。因此，由它们主导的组执行主动的块检查，以实现更多的块级定位。评析与总结论文最后使用单个NVIDIA V100 GPU为设备，用AStitch实现了具有代表性的机器学习模型，如BERT等，并给出了详细评估结果。结果显示，AStitch的速度比最先进的编译器高2.73倍，作为AStitch优越性的证明。根据论文所说，AStitch已被部署到一个生产集群中，并在一周内为70,000个任务节省了约20,000个GPU小时。这表明了Astitch的稳健性。目前许多机器学习编译器的优化主要针对计算密集型操作，而对内存密集型计算的性能问 题关注有限。然而，根据该论文发现，近来在机器学习领域，内存密集型计算的性能比重正在上升，因此，论文的研究人员将提高内存密集型计算的性能作为基础动机。在相关工作中，存在一些编译器将融合优化应用于内存密集型的操作，但这些工作缺乏解决复杂的依赖性问题，并且存在融合不足的问题。 AStitch解决了这个问题，并在以前的工作基础上扩大了融合的范围，这也是其区别于相关的历史工作的特色之处。AStitch作为一个机器学习优化编译器，具有如下优势： 多维：它在考虑多维优化目标的同时，系统地抽象了四种算子缝合方案，用新颖的分层数据重用处理复杂的计算图依赖，并通过自适应线程映射有效地处理各种张量形状。 可移植性：虽然AStitch是一个独立的编译器引擎，但其基本思想可以普遍应用于其他ML框架和优化编译器，可以移植到任何版本的TensorFlow。 高性能：解决了内存密集型ML计算的两个主要性能问题：效率低下的融合和输入、不规则的张量形状。综上所述，论文提出分层数据重用技术来解决复杂的依赖关系，以扩大融合范围，减少非计算开销。论文提出了自适应线程映射技术来处理不规则张量形状的问题。结合这些技术，论文开发了一个名为AStitch的JIT编译器，将这些优化与高可用性相结合。可以说，AStitch填补了机器学习编译器中一个长期被忽视的空白。结语本次课程报告分析总结了两篇能够体现体系结构在大规模机器学习中应用的论文文献，A Software-defined Tensor Streaming Multiprocessor for Large-scale Machine Learning（用于大规模机器学习的软件定义张量流多处理器）关注机器学习模型的网络需求与计算需求，针对对并行性与确定性的保证，提出了TSP（Tensor Streaming Multiprocessor）系统结构。而AStitch: Enabling a New Multi-dimensional Optimization Space for Memory-Intensive ML Training and Inference on Modern SIMT Architectures（AStitch:为内存密集型ML训练和现代SIMT架构推理提供新的多维优化空间）则关注机器学习中的内存密集型计算性能，提出了一个自动编译器优化设计AStitch。经过这两篇文献的阅读，可以对机器学习目前的发展图景与瓶颈有一个较为清晰的认识脉络。最首要的是，机器学习具有大规模数据与高效的需求，一切挑战都以计算性能为基点。通过提高数据与模型的并行性，可以提高计算性能，也便于在支持并行的硬件设备（如基于GPU的CUDA）上运行，由此在这一分支上引出若干条支线： 软硬件的接口设计，包括支持并行式计算的网络拓扑结构，及运行在该网络上的软件调度策略；对此，论文提出了基于软件层面静态调度策略的蜻蜓网络拓扑。 对模型与数据的分割，包括以何种策略分割、分割批次的大小；对此，论文提出了基于GPU执行的SIMT特性的任务打包和拆分方法，以及相对应的自适应线程映射技术。 分布式系统设计，包括向外扩展性的实现、节点校正、节点之间的同步机制、节点之间如何联系传输；对此，论文提出了ISA架构的HAC指令与DESKEW指令等，模拟达到同步机制的效果，在容错率上，采用前向纠错来进行校正，在传输上，采用C2C结构作为节点之间的传输纽带，并采用软件层面的流控制来划定传输模式。此外，通过保证确定性执行，可以提高计算性能与稳定性，确定性执行能保证计算始终在限定时间内产生一致的输出。通过对单次迭代计算本身的优化，也可提高计算性能，对计算形式及其特性进行分类，又可分为计算密集型与内存密集型的计算，从而产生面向运算符的多重运算器缝合方案。在讨论计算性能时，时间与空间成本的对立与相互转换是一个永恒的命题，体系结构在机器学习领域的应用，则是试图在软硬件之间建立联系，从更底层的网络结构、编译器结构出发，从而在时间与空间成本中取得平衡，来使性能与收益最大化。参考文献:［1］ J. Kim, W. J. Dally, S. Scott and D. Abts, “Technology-Driven, Highly-Scalable Dragonfly Topology,” 2008 International Symposium on Computer Architecture, Beijing, China, 2008, pp. 77-88, doi: 10.1109/ISCA.2008.19.［2］ C2C接口——推动移动产业未来发展与创新.Brian Carlson http://www.chinaaet.com/article/150357［3］ Dennis Abts, Garrin Kimmell, Andrew C. Ling, John Kim, Matthew Boyd, Andrew Bitar, Sahil Parmar, Ibrahim Ahmed, Roberto DiCecco, David Han, et al.: A software-defined tensor streaming multiprocessor for large-scale machine learning. ISCA 2022: 567-580.［4］ Zhen Zheng, Xuanda Yang, Pengzhan Zhao, Guoping Long, Kai Zhu, Feiwen Zhu, Wenyi Zhao, Xiaoyong Liu, Jun Yang, Jidong Zhai, Shuaiwen Leon Song, Wei Lin: AStitch: enabling a new multidimensional optimization space for memory-intensive ML training and inference on modern SIMT architectures. ASPLOS 2022: 359- 373." }, { "title": "Machine Learning: Clustering", "url": "/posts/clustering/", "categories": "Courses", "tags": "Machine Learning, AI, K-Means, GMM, Clustering", "date": "2022-12-18 09:36:30 +0800", "snippet": " 机器学习：K-Means&amp;GMM学习笔记 1、K-Means 1）算法思路 2）初始化中心点 1. 随机选取k个中心点 2. 最大距离选取中心点 3）关键代码展示 2、EM算法与GMM模型 1）EM算法 2）GMM模型 3）算法思路 1. E Step 2. M Step 4）初始化模型参数 5）协方差矩阵格式 机器学习：K-Means&amp;GMM学习笔记1、K-MeansK-Means算法是最常用的聚类算法，主要思想是：在给定K值和K个初始类簇中心点的情况下，把每个点(即样本数据)分到离其最近的类簇中心点所代表的类簇中，所有点分配完毕之后，根据一个类簇内的所有点重新计算该类簇的中心点(取平均值)，然后再迭代的进行分配点和更新类簇中心点的步骤，直至类簇中心点的变化很小，或者达到指定的迭代次数。1）算法思路 初始化中心点； 在第j次迭代中，对于每个样本点，使用欧几里得距离计算样本间的距离，选取最近的中心点，归为该类； 更新中心点为每类的均值； 重复(2)(3)迭代更新，直至误差小到某个值或者到达一定的迭代步数，误差不变。 空间复杂度：o(N)，时间复杂度：o(I*K*N)，其中，N为样本点个数，K为中心点个数，I为迭代次数。在解决MNIST聚类问题时，由于标签为10个数字，K默认为10。2）初始化中心点1. 随机选取k个中心点2. 最大距离选取中心点随机初始化质心可能导致算法迭代很慢，K-means++是对K-mean随机初始化质心的一个优化，具体步骤如下： 随机选取一个点作为第一个聚类中心。 计算所有样本与第一个聚类中心的距离。 选择出上一步中距离最大的点作为第二个聚类中心。 迭代：计算所有点到与之最近的聚类中心的距离，选取最大距离的点作为新的聚类中心。 终止条件：直到选出了这k个中心。3）关键代码展示实现了Kmeans类。class KMEANS: def __init__(self, n_clusters=10, max_iter=20,device = torch.device(\"cuda:0\"),is_random=True): \"\"\"init Args: n_clusters (int, optional): the number of clusters. Defaults to 10. max_iter (int, optional): the maximum iterations. Defaults to 20. device (torch.device, optional): use cuda or gpu. Defaults to torch.device(\"cuda:0\"). is_random (bool, optional): randomly initialize the center points or not. Defaults to True. \"\"\" self.n_clusters = n_clusters self.labels = None # the labels of input data self.centers = None # the center points self.max_iter = max_iter self.device = device self.is_random = is_random def acc(self,y_true:np.array, y_pred:np.array): \"\"\" 参考: https://blog.csdn.net/qq_42887760/article/details/105720735 Calculate clustering accuracy. Require scikit-learn installed # Arguments y: true labels, numpy.array with shape `(n_samples,)` y_pred: predicted labels, numpy.array with shape `(n_samples,)` # Return accuracy, in [0,1] \"\"\" y_true = y_true.astype(np.int64) assert y_pred.size == y_true.size D = max(y_pred.max(), y_true.max()) + 1 w = np.zeros((D, D), dtype=np.int64) for i in range(y_pred.size): w[y_pred[i], y_true[i]] += 1 ind = linear_sum_assignment(w.max() - w) ind = np.array(ind).T return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size def initial(self,x): \"\"\"initialize the center points Args: x (torch.Tensor): the data of trainset, trainx, shape(60000,784) \"\"\" if self.is_random: # 随机选择初始中心点 init_row = torch.randint(0, x.shape[0], (self.n_clusters,)) self.centers = x[init_row].to(self.device) else: # 最大距离初始化 # 只随机选择一个初始中心点 first = torch.randint(0,x.shape[0],(1,)) cen = x[first] centers = torch.empty((0, x.shape[1])).to(self.device) centers = torch.cat([centers, cen], (0)) dists = torch.empty((x.shape[0],0)).long().to(self.device) # 记录每个样本与中心点的距离 for i in range(1, self.n_clusters): # 要选n个中心点 for cen in centers: # 对于每个样本点，计算其与每个中心点的距离 dist = torch.sum(torch.mul(x-cen,x-cen),(1)) dists = torch.cat([dists,dist.unsqueeze(1)],(1)) # 选出离最近中心点距离最大的样本点，作为下一个中心点 cen = x[torch.argmax(torch.min(dists,dim=1).values)] centers = torch.cat([centers, cen.unsqueeze(0)], (0)) self.centers = centers def fit(self, x, y, testx,testy): \"\"\"fit the model Args: x (Tensor): trainx y (Tensor): trainy, the labels of trainset testx (Tensor): testx testy (Tensor): testy, the labels of testset \"\"\" for i in range(self.max_iter): print(f\"Epoch {i+1}\") # 分类 self.labels = self.classify(x) # 更新中心点 self.update_center(x) # 计算trainset与testset的精度 accu_train.append(self.acc(np.array(y.cpu()), np.array(self.labels.cpu()))) print(f\"Accuracy of train data: {accu_train[-1]}\", end=', ') accu_test.append(self.acc(np.array(testy.cpu()), np.array(self.classify(testx).cpu()))) print(f\"Accuracy of test data: {accu_test[-1]}\") def classify(self, x): \"\"\"classify Args: x (tensor): train x, shape(60000,784) \"\"\" dists = torch.empty((x.shape[0],0)).long().to(self.device) # 对应每个样本与各中心点的距离 for cen in self.centers: dist = torch.sum(torch.mul(x-cen,x-cen),(1)) dists = torch.cat([dists,dist.unsqueeze(1)],(1)) return torch.argmin(dists,dim=1) def update_center(self, x): \"\"\"update the center points Args: x (tensor): train x, shape (60000,784) \"\"\" centers = torch.empty((0, x.shape[1])).to(self.device) for i in range(self.n_clusters): cluster_samples = x[self.labels == i] # 该族样本点 # 取该族所有样本点的均值，取代当前中心点 centers = torch.cat([centers, torch.mean(cluster_samples, (0)).unsqueeze(0)], (0)) self.centers = centers2、EM算法与GMM模型1）EM算法EM（Expectation Maximization）算法是一种常用的数据挖掘和机器学习方法，用于估计带隐变量的概率模型的参数。它的基本思想是通过迭代的方式不断优化模型的参数，以达到模型对数据的最佳拟合。EM 算法主要包括两个步骤：期望（Expectation）步骤和极大化（Maximization）步骤。在期望步骤中，通过计算数据点在隐变量的某种可能状态下的期望值来估计隐变量的分布。在极大化步骤中，通过极大化隐变量的期望值来优化模型的参数。EM 算法可以用来解决多种问题，例如高斯混合模型的参数估计、图像分割和语音识别等。它的优点在于可以用于处理带隐变量的模型，并且可以通过迭代的方式不断优化模型的参数。2）GMM模型GMM（高斯混合模型）是一种概率模型，用于对一个数据集进行拟合。它假设数据是由若干个高斯分布组成的混合体，并使用最大似然估计或最大后验概率估计来估计每个分布的参数。在数学上，一个由 $K$ 个高斯分布组成的 GMM 可以表示为：\\(p(x) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(x | \\mu_k, \\Sigma_k)\\)其中 $\\pi_k$ 表示第 $k$ 个高斯分布在混合中的权重，$\\mathcal{N}(x | \\mu_k, \\Sigma_k)$ 表示第 $k$ 个高斯分布的概率密度函数，$\\mu_k$ 和 $\\Sigma_k$ 分别表示第 $k$ 个高斯分布。3）算法思路EM算法是用来训练混合模型（例如GMM）的一种常见方法。具体来说，EM算法通过迭代地执行下面两个步骤来训练GMM模型： E步骤（期望步骤）：对于给定的模型参数，计算每个观测值属于每个混合成分的概率（预测标签）。 M步骤（极大似然步骤）：使用计算出的每个观测值属于每个混合成分的概率来更新模型参数（均值$\\mu$，协方差矩阵$\\Sigma$、权重$\\pi$），使得模型对观测数据的似然最大。 检查模型是否收敛，计算当前的模型参数与上一次迭代的模型参数之间的差异，如果差异小于某个阈值，则认为已经收敛。收敛则结束迭代，反之继续从E步骤开始迭代。1. E Step在E步骤中，我们需要计算每个观测值属于每个混合成分的概率。这通常使用给定的模型参数（例如每个混合成分的均值、协方差和混合系数）以及观测数据的高斯分布概率密度函数来实现。具体来说，对于每个观测值x，我们需要计算其属于每个混合成分的概率。这可以使用下面的公式计算：\\(p(z_i = j | x_i, θ) = π_j * N(x_i | μ_j, Σ_j)\\)其中，$z_i$表示观测值$x_i$属于的混合成分的编号，θ表示模型参数（包括每个混合成分的均值、协方差和混合系数），$π_j$表示混合成分j的混合系数，$N(x_i | μ_j, Σ_j)$表示观测值$x_i$的高斯分布概率密度函数，其中$μ_j$和$Σ_j$分别表示混合成分j的均值和协方差。2. M Step在M步骤中，我们使用极大似然估计来更新模型参数。具体来说，我们需要求解下面的优化问题：\\(maximize L(θ) = ∑_{i=1}^{n} log ∑_{j=1}^{m} π_j * N(x_i | μ_j, Σ_j)\\)其中，$n$表示观测数据的数量，$m$表示混合成分的数量，$θ$表示模型参数（包括每个混合成分的均值、协方差和混合系数），$π_j$表示混合成分j的混合系数，$N(x_i | μ_j, Σ_j)$表示观测值$x_i$的高斯分布概率密度函数，其中$μ_j$和$Σ_j$分别表示混合成分j的均值和协方差。我们可以使用拉格朗日乘数法来求解上述优化问题。具体来说，对于每个混合成分$j$，我们需要更新其均值$μ_j$、协方差$Σ_j$和混合系数$π_j$，使得似然函数$L(θ)$最大。 对于每个混合成分$j$，更新其均值$μ_j$：\\(μ_j = ∑_{i=1}^{n} p(z_i = j | x_i, θ) * x_i / ∑_{i=1}^{n} p(z_i = j | x_i, θ)\\)其中，$n$表示观测数据的数量，$p(z_i = j | x_i, θ)$表示观测值$x_i$属于混合成分$j$的概率，$x_i$表示第$i$个观测值。 对于每个混合成分$j$，更新其协方差$Σ_j$：\\(Σ_j = ∑_{i=1}^{n} p(z_i = j | x_i, θ) * (x_i - μ_j)(x_i - μ_j)^T / ∑_{i=1}^{n} p(z_i = j | x_i, θ)\\) 对于每个混合成分$j$，更新其混合系数$π_j$：\\(π_j = ∑_{i=1}^{n} p(z_i = j | x_i, θ) / n\\) 4）初始化模型参数在使用EM算法训练GMM模型时，需要初始化模型参数。常用的方法包括： 随机初始化：将模型参数随机初始化，并开始迭代训练。 K-means聚类：使用K-means聚类算法将数据分成若干类，然后将每一类的均值作为GMM的均值，协方差设为协方差，并将每一类的权重设为相同值。 统计量初始化：使用数据的均值和协方差矩阵作为GMM的均值和方差，并将每一类的权重设为相同值。5）协方差矩阵格式GMM模型允许每个混合成分使用不同的协方差矩阵。具体来说，GMM模型可以使用以下三种协方差矩阵格式： 全局协方差（full covariance）：每个混合成分使用相同的协方差矩阵。这种协方差矩阵格式在多维情况下可以很好地捕捉数据的复杂关系，但是它的计算代价比较大。 对角协方差（diagonal covariance）：每个混合成分使用一个对角矩阵作为协方差矩阵，即只有对角线上的元素不为零。这种协方差矩阵格式计算代价较小，但是可能不能很好地捕捉数据的复杂关系。 共轭协方差（spherical covariance）：每个混合成分使用一个单位矩阵乘以一个单独的方差值作为协方差矩阵。这种协方差矩阵格式计算代价最小，但是对于多维数据可能不能很好地捕捉其复杂关系。" }, { "title": "ModuleNotFoundError: No module named '_sqlite3'解决方法", "url": "/posts/sqlite3/", "categories": "log", "tags": "WSL2, Django", "date": "2022-11-30 00:11:30 +0800", "snippet": "WSL2 ubuntu18.04运行django的时候发现该module缺失1. 安装编译sqlite3 wget https://sqlite.org/2019/sqlite-autoconf-3290000.tar.gz tar zxvf sqlite-autoconf-3290000.tar.gz cd sqlite-autoconf-3290000 ./configure make &amp;&amp; make install验证安装成功：2. 重新编译python找到安装路径： $ python -V Python 3.10.1 $ find / -name \"Python-3.10.1\" /root/Python-3.10.1 /root/Python-3.10.1/build/temp.linux-x86_64-3.10/root/Python-3.10.1进入该路径，重新编译python： $cd /root/Python-3.10.1 $./configure --prefix=/usr/local/python3.10 $ make &amp;&amp; make install $which python //查看环境变量python路径 /usr/bin/python $rm -rf /usr/bin/python3.10 $ln -s /usr/local/python3.10/bin/python3.10 /usr/bin/验证能否导入该模块：成功解决：" }, { "title": "Machine Learning: Classification", "url": "/posts/classification/", "categories": "Courses", "tags": "Machine Learning, AI, CNN, MLP", "date": "2022-11-08 09:36:30 +0800", "snippet": " 基于CIFAR-10图像分类任务训练线性分类器、MLP和CNN模型 一、模型原理 1）Softmax分类器 2）MLP 3）CNN 4）网络参数 1、CUDA加速 2、损失函数 3、优化器 SGD（Stochastic Gradient Descent） SGD Momentum Adam 二、代码框架 1）数据读取及预处理 2）单次训练 3）评估正确率 4）main 三、实验分析 1）Softmax线性模型 2）MLP模型 3）CNN模型 1、对优化算法及其参数的实验 2、对网络结构的实验 4）对比三个模型在CIFAR-10图像分类任务上的性能 基于CIFAR-10图像分类任务训练线性分类器、MLP和CNN模型一、模型原理1）Softmax分类器Softmax分类器是一个单层线性神经网络，即只有一个输入层、一个输出层，再经过Softmax函数激活层，得到标签的预测概率。Softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持可导的性质。实现一个Softmax线性分类模型：class Softmax(nn.Module): def __init__(self, inNum, outNum) -&gt; None: super(Softmax, self).__init__() self.out = nn.Linear(inNum,outNum) def forward(self, x): y = F.relu(self.out(x.view(x.shape[0],-1))) y = F.log_softmax(y,dim=0) return y 2）MLP通过在网络中加入一个或多个隐藏层，可以克服线性模型的限制， 使其能处理更普遍的、非单调的函数关系类型。将许多全连接层堆叠在一起，每一层都输出到上面的层，直到生成最后的输出。 我们可以把前L−1层看作表示，把最后一层看作线性预测器。 这种架构通常称为多层感知机（multilayer perceptron），通常缩写为MLP。实现一个MLP模型：该神经网络结构由一个输入层、三个隐藏层及一个输出层构成，在层与层之间使用ReLU激活层，最后用Softmax计算标签的预测概率。class MLP(nn.Module): def __init__(self, inNum, outNum, hidden,hid2,hid3) -&gt; None: super(MLP, self).__init__() self.hid = nn.Linear(inNum,hidden) self.hid2 = nn.Linear(hidden,hid2) self.hid3 = nn.Linear(hid2,hid3) self.out = nn.Linear(hid3,outNum) def forward(self, x): x = F.relu(self.hid(x.view(x.shape[0],-1))) #input(3,32,32) output(1024) x = F.relu(self.hid2(x.view(x.shape[0],-1)))#output(256) x = F.relu(self.hid3(x.view(x.shape[0],-1)))#output(84) x = F.relu(self.out(x.view(x.shape[0],-1)))#output(10) x = F.log_softmax(x,dim=0) return x3）CNN卷积神经网络（convolutional neural network，CNN）是一类强大的、为处理图像数据而设计的神经网络。使用前述的模型时，将图像数据展平成一维向量而忽略了每个图像的空间结构信息，卷积神经网络则能弥补这个缺漏。LeNet（LeNet-5）由两个部分组成： 卷积编码器：由两个卷积层组成; 全连接层密集块：由三个全连接层组成。实现一个LeNet模型：class LeNet(nn.Module): def __init__(self): super(LeNet,self).__init__() self.conv1 = nn.Conv2d(3,16,5) self.pool1 = nn.MaxPool2d(2,2) self.conv2 = nn.Conv2d(16,32,5) self.pool2 = nn.MaxPool2d(2,2) self.fc1 = nn.Linear(32*5*5,120) self.fc2 = nn.Linear(120,84) self.fc3 = nn.Linear(84,10) def forward(self, x): x = F.relu(self.conv1(x))#input(3,32,32) output(16,28,28) x = self.pool1(x) #output(16，14，14) x = F.relu(self.conv2(x)) #output(32,10,10) x = self.pool2(x) #output(32,5,5) x = x.view(-1,32*5*5) #output(5*5*32) x = F.relu(self.fc1(x)) #output(120) x = F.relu(self.fc2(x)) #output(84) x = F.relu(self.fc3(x)) # output(10) x = F.log_softmax(x, dim=1) return x4）网络参数1、CUDA加速利用GPU进行计算、让CPU读取数据，可以大幅减少训练的耗时，为此需要将数据与网络迁移到GPU上进行大规模计算。import torchfrom torch import nn# 查看gpu信息cudaMsg = torch.cuda.is_available()gpuCount = torch.cuda.device_count()print(\"1.是否存在GPU:{}\".format(cudaMsg), \"如果存在有：{}个\".format(gpuCount))# 将数据/网络移到GPU上net = net.cuda()# 命令行$ nvidia-smi #可以查看当前GPU适配的CUDA版本及显卡占用率NVIDIA-SMI 522.25 Driver Version: 522.25 CUDA Version: 11.8$ nvcc -V #可以确认CUDA是否已安装成功nvcc: NVIDIA (R) Cuda compiler driverCopyright (c) 2005-2022 NVIDIA CorporationBuilt on Wed_Sep_21_10:41:10_Pacific_Daylight_Time_2022Cuda compilation tools, release 11.8, V11.8.89Build cuda_11.8.r11.8/compiler.31833905_0事实上对于该模型，BatchSize=500时，N卡占用率顶多70%，多数时间有大量空余，真正占用时间的是迭代数据时，CPU对数据集图像的读取及预处理（即transform操作），这是由于CPU对Tensor的处理很慢，而torchvision库没有将数据集迁移到GPU进行预处理计算的API，若要解决这个问题，只能使用DALI或其余库接口加速预处理与数据读取，或者将预处理后的数据集进行保存。2、损失函数由于是预测标签概率类型的网络，在此次实验中都采用交叉熵函数。import torch.nn as nn# 损失函数loss = nn.CrossEntropyLoss()3、优化器import torch# SGD / SGD Momentumoptimizer = torch.optim.SGD(net.parameters(),lr=0.03,momentum=0.9, weight_decay=1e-5) # Adamoptimizer = torch.optim.Adam(net.parameters(),weight_decay=1e-5) # 控制学习率指数衰减torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)# 控制学习率按固定步长衰减torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.65)SGD（Stochastic Gradient Descent）随机梯度下降算法即是在给定数据集中，每次随机选择一则数据，根据该数据的训练结果计算损失梯度，更新参数。SGD Momentum在随机梯度的学习算法中，每一步的步幅都是固定的，而在动量学习算法中，每一步走多远不仅依赖于本次的梯度的大小，还取决于过去的速度。速度v是累积各轮训练参数的梯度，速度越大，依赖以前的梯度越大。物理学中，用变量v表示速度，表明参数在参数空间移动的方向即速率，而代价函数的负梯度表示参数在参数空间移动的力，根据牛顿定律，动量等于质量乘以速度，而在动量学习算法中，我们假设质量的单位为1，因此速度v就可以直接当做动量了，我们同时引入超参数$\\beta$，其取值在$[0,1]$范围之间，用于调节先前梯度（力）的衰减效果。Adam自适应动量优化算法结合了RMSProp和动量学习法的优势。二、代码框架1）数据读取及预处理直接调用torchvision.datasets中的CIFAR10数据集，对图像进行随机翻转、随机灰度调正、转换为Tensor张量、正则化等预处理操作，返回torch.utils.data.DataLoader作为迭代器。import torchimport torchvisionimport torchvision.transforms as transformsbatch = 500 # batch_sizedef load_data(): # 读取数据，返回迭代器 mean = torch.tensor([0.4915, 0.4823, 0.4468]) std = torch.tensor([0.2470, 0.2435, 0.2616]) # 图像预处理操作 transform = transforms.Compose( [ transforms.RandomHorizontalFlip(), transforms.RandomGrayscale(), transforms.ToTensor(), transforms.Normalize(mean,std)]) transform1 = transforms.Compose( [ transforms.ToTensor(), transforms.Normalize(mean,std)]) trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch, shuffle=True)#,num_workers=1,pin_memory=True) testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=transform1) testloader = torch.utils.data.DataLoader(testset, batch_size=batch, shuffle=False)#,num_workers=1,pin_memory=True) return trainloader,testloader2）单次训练实现一个epoch内的训练函数： 向前传播得到预测标签 根据预测值计算损失值 根据损失值进行反向传播，得到各参数的梯度 根据梯度下降更新参数 返回损失值def SingleTrain(net,train_iter,loss,optim): \"\"\"train for one epoch Args: net (nn.module): training model train_iter (dataloader): iterator of training set loss (): loss function of the model optim (): optimizer of the model Returns: float: loss value \"\"\" net.train() # 开启训练模式 # 将计算累计loss值的变量定义在GPU上，无需在计算时在CPU与GPU之间移动，耗费时间 los = torch.zeros(1).cuda() for k,data in enumerate(train_iter,0): x,y = data # 将数据迁移到GPU x = x.cuda() y = y.cuda() # 清零梯度 optim.zero_grad() # 向前传播，输出预测标签 haty = net(x) # 计算损失值 l = loss(haty,y) # 反向传播，计算得到每个参数的梯度值 l.backward() # 梯度下降，由优化器更新参数 optim.step() # 累计损失值 los += (los * k + l.detach()) / (k + 1) #right += torch.eq(torch.max(haty, dim=1)[1], y).sum() return los.item()3）评估正确率计算用当前网络预测正确的样本个数。@torch.no_grad() # 使新增的tensor没有梯度，使带梯度的tensor能够进行原地运算def score(net,data_iter): net.eval() # 开启评估模式 # 将计算累计正确预测样例数的变量定义在GPU上，无需在计算时在CPU与GPU之间移动，耗费时间 right_sum = torch.zeros(1).cuda() for k,data in enumerate(data_iter): X,y = data X = X.cuda() y = y.cuda() # 计算预测标签一致的样例数 right_sum += torch.eq(torch.max(net(X), dim=1)[1], y).sum() return right_sum.item()4）maindef main(): input,output = 3072,10 hid,hid2,hid3 = 1024,256,84 trainlen,testlen = 50000,10000 train_iter, test_iter = load_data() #net = LeNet() #net = MLP(input,output,hid,hid2,hid3) #net = Softmax(input,output) net = CNN() net = net.cuda() #损失函数 loss = nn.CrossEntropyLoss() #优化函数 optimizer = torch.optim.SGD(net.parameters(),lr=0.03,momentum=0.9, weight_decay=1e-5) torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98) # torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.65) #optimizer = torch.optim.Adam(net.parameters(),weight_decay=1e-5) epoch = 40 start = time() for i in range(epoch): t = time() loss_visual.append(SingleTrain(net,train_iter,loss,optimizer)) # train.append(score(net,train_iter)/trainlen) test.append(score(net,test_iter)/testlen) print(f'epoch: {i}\\nloss:{loss_visual[-1]}\\ntest accuracy:{test[-1]}') print(f'Time for this epoch: {time()-t}s') print(f'Time: {time()-start}s\\n for {epoch} epoches.\\nAverage {(time()-start)/epoch} for each.') print(f'The optimal test accuracy:{max(test)}') plt.plot(range(len(loss_visual)),loss_visual) plt.xlabel('Epoch') plt.ylabel('Loss') plt.figure() #plt.plot(range(len(train)),train,color='blue', label='Train Accuracy') plt.plot(range(len(test)),test,color='purple', label='Test Accuracy') plt.legend() # 显示图例 plt.xlabel('Epoch') plt.ylabel('Accuracy') plt.ylim((0,1)) plt.show()if __name__ == '__main__': main()三、实验分析1）Softmax线性模型对更改优化算法、学习率lr及迭代次数epoch等参数进行实验。运行结果： 优化器及参数 损失值 训练集表现 测试集表现 Adamlr=0.002epoch=300 Adamlr=0.05epoch=30 SGDlr=0.03momentum=0.9weight_decay=1e-5epoch=40   从表中可以看出： 在训练效率上，使用不同优化算法的时间是相近的，Adam优化算法会较快。Adam采用自适应优化，所以它的优势是训练快，但是问题在于更容易陷入局部最优、鞍点。 在训练效果上： 使用Adam优化算法时，正确率不如使用SGD Momentum优化算法，损失函数正常收敛，学习率较大时损失函数出现微小的震荡。当学习率为0.002时，迭代了300次后，训练集正确率仅仅是从12%增加到约26%，测试集也是，说明欠拟合。当学习率为0.05时，迭代了30次后，训练集与测试集正确率都在17%到23%左右震荡，绘制正确率图像时没有将y轴范围设置在[0,1]，故直观看上去产生强烈震荡，实际上只是在低准确率处小幅震荡，同样欠拟合。 使用SGD Momentum优化算法时，可以看到损失函数在大幅下降后存在震荡情况，收敛并不平滑，这也许是由动量衰减因素引起的。此时，测试集正确率能达到39.72%，为该Softmax分类器的最优训练结果，但从测试机正确率图像可以看出，测试机正确率的上涨幅度极小，几乎是一条平稳的直线。 最终选用的最优优化算法及其参数为：SGD算法，学习率0.03，动量衰减因数0.9，权重衰减1e-5torch.optim.SGD(net.parameters(),lr=0.03,momentum=0.9, weight_decay=1e-5) 综上所述，Softmax线性分类器对CIFAR10数据集的分类效果较差，模型训练的拟合效果较差，这是由于，单层神经网络的线性分类器与多维的图像本就难以拟合，为了转换为二维的矩阵计算，将图像的数据进行平展，损失了图像的空间结构信息。2）MLP模型通过实验，在SGD算法，学习率0.03，动量衰减因数0.9，权重衰减1e-5条件下，对比了单隐藏层与三隐藏层网络结构的训练效果。运行结果： 网络结构 损失值 测试集表现 inputhidden 1(512)outputepoch=40 50.71% inputhidden 1(120)outputepoch=20 49.72% inputhidden 1(1024)hidden 2 (256)hidden 3(84)outepoch=60 58.41% inputhidden 1(4096)hidden 2 (1280)hidden 3(256)hidden4(64)outepoch=40 60.09% 从表中可以看出： 在训练效率上，网络结构对用时影响甚微，时间基本为18s/epoch。 在训练效果上： 隐藏层数量的影响：隐藏层越多，非线性的比重更大，模型的拟合效果越好，预测正确率越高。对于单隐藏层的网络结构，最高正确率在50%左右，从图像可以看出，损失函数收敛不平滑，测试集正确率仅从40%上升到50%，训练效果较差，欠拟合。对多隐藏层的网络结构，正确率能接近60%左右，三隐藏层的最高正确率为58%，四隐藏层迭代次数更少而正确率更高，为60%。然而，此时损失函数皆未完全收敛，测试集正确率却已收敛不再增加，说明模型欠拟合。 隐藏层神经元数量的影响：在适当范围内，隐藏层神经元数量越多，拟合效果越好。理论上，隐藏层中使用太少的神经元将导致欠拟合(underfitting)。相反，使用过多的神经元同样会导致一些问题。首先，隐藏层中的神经元过多可能会导致过拟合(overfitting)。当神经网络具有过多的节点（过多的信息处理能力）时，训练集中包含的有限信息量不足以训练隐藏层中的所有神经元，因此就会导致过拟合。即使训练数据包含的信息量足够，隐藏层中过多的神经元会增加训练时间，从而难以达到预期的效果。 3）CNN模型以LeNet网络结构为基础，通过实验，分别对比了优化算法及其参数、网络结构对训练效果的影响。1、对优化算法及其参数的实验在LeNet网络结构下，探究不同优化算法及其参数对训练性能的影响。运行结果： 优化器及参数 损失值 测试集表现 SGDlr=0.03momentum=0.9weight_decay=1e-5epoch=60 73.82% Adamlr=0.001weight_decay=1e-5epoch=50 62.26% SGDlr=0.03weight_decay=1e-5epoch=50 61.37% 从表中可以看出，在训练效果上： 使用SGD Momentum优化算法时，经过60次迭代，损失函数正常收敛；模型拟合效果最好，测试集正确率最高，能达到73.82%，训练集正确率能达到90%。 使用Adam优化算法时，模型拟合效果较差，正确率仅为62%，也许是因为默认的学习率太低。 使用SGD优化算法时，缺乏动量衰减因子后，损失函数收敛速度明显变慢，接近最优点的速度变慢，震荡情况增加，模型拟合效果较差，测试集正确率仅为61%。2、对网络结构的实验在SGD算法，学习率0.03，动量衰减因数0.9，权重衰减1e-5条件下，对不同的网络结构进行实验。运行结果： 网络结构 测试集表现及用时 LeNet:inputConv(out_channels=6,conv_size=5)MaxPool($2\\times2$)Conv(out_channels=16,conv_size=5)MaxPool($2\\times2$)linear(120)linear(84)out(10) 73.82% 在LeNet基础上添加一个卷积层、一个全连接层:inputConv(out_channels=16,kern_size=5)Conv(out_channels=32,kern_size=5)MaxPool($2\\times2$)Conv(out_channels=64,conv_size=5)MaxPool($2\\times2$)linear(256)linear(120)linear(84)out(10) 74.5% inputConv(out_channels=32,kern_size=3,pad=1)MaxPool($2\\times2$)Conv(out_channels=64,kern_size=3,pad=1)MaxPool($2\\times2$)linear(1024)linear(512)out(10) 77.89% 在LeNet基础上添加两个卷积层、一个全连接层:inputConv(out_channels=6,kern_size=2,pad=2)Conv(out_channels=16,kern_size=2,pad=2)MaxPool($2\\times2$)Conv(out_channels=64,conv_size=2,pad=2)MaxPool($2\\times2$)Conv(out_channels=128,conv_size=3,pad=2)linear(2069)linear(496)linear(84)out(10) 78.59% 从表中可以看出： 在训练效率上，相同条件下，卷积层数越少，模型收敛速度越快，且单次epoch用时越少，但网络结构含有2~4个卷积层时一般在18s/epoch左右浮动，实际变动不明显。对于只有两个卷积层、三个全连接层的CNN网络结构，只需要15 epoch就能收敛到最优的正确率。若有三个卷积层、四个全连接层，则需要30 epoch来达到收敛。若有四个卷积层，四个全连接层，用该网络结构训练了两次，epoch皆为25，此时损失函数还未收敛，仍然呈下降趋势，说明收敛速度较慢。 在训练效果上：适当增加卷积层、减小卷积核大小，添加零填充层能够增强模型的拟合效果，提高正确率。在适当的范围内，卷积核越小，意味着图像的特征采样越细，故可以提高准确率。添加零填充层，可以防止对边缘像素信息的遗失。若有四个卷积层，四个全连接层，用该网络结构训练了四次，迭代到25次时，一次达到了78%的正确率且已收敛，两次则只达到56%且已收敛，说明卷积层过多，可能出现过拟合的情况。此外，用SGD Momentum优化算法，也可能导致对局部最优点的搜索具有随机性。卷积层过多时，会导致神经元失效，尝试了七层卷积层，正确率一直为0.1，正好10个分类，神经元已经陷入瞎猜。4）对比三个模型在CIFAR-10图像分类任务上的性能显然的，CNN模型在图像分类任务上训练效果最佳，最高测试集正确率能达到77%，远超另外两个模型，这是由于Softmax线性模型与MLP模型都基于线性计算，需要将图像的数据进行平展，损失了图像的空间结构信息，而CNN模型可以通过卷积运算，保留读取这些结构信息。次之的是MLP模型，通过增加隐藏层数量，可以得到非线性模型的训练效果，最高测试集正确率能达到60%。拟合效果最差的则是Softmax模型，单层神经网络的线性分类器与多维的图像本就难以拟合，并非意外结果。在训练效率上，事实上，无论是基于优化算法的实验，还是网络结构的实验，或是模型的实验，都较难观察出训练速度的差别，基本在18s/epoch左右。我在本实验中采用了GPU加速，但对于该模型，BatchSize=500时，极少数情况下，N卡占用率才能达到70%，多数时间有大量空余，真正占用时间的是迭代数据时，CPU对数据集图像的读取及预处理（即transform操作），而非与优化算法、网络机构、模型有关的计算，这是由于用dataloader读取数据时，每epoch要对数据做一次预处理、转换为张量的操作，再迭代取数据，CPU对Tensor的处理很慢，而torchvision库没有将数据集迁移到GPU进行预处理计算的API。因此，未能观察出训练速度的差别。若要解决这个问题，只能使用DALI或其余库接口加速预处理与数据读取，或者将预处理后的数据集进行保存。" }, { "title": "SVM学习笔记", "url": "/posts/svm/", "categories": "Courses", "tags": "Machine Learning, SVM, AI", "date": "2022-11-06 09:36:30 +0800", "snippet": " SVM学习笔记 1）SVM模型 原理推导 实践 2）Hinge Loss 3）Cross-Entropy Loss 二分类 多分类 4）库函数 5）手动实现 Hinge Loss线性分类模型 模型 算法流程 代码 Cross-Entropy Loss线性分类模型 模型 算法流程 代码 SVM学习笔记SVM(Support Vector Machine)，即支持向量机，是在分类与回归分析中分析数据的监督式学习模型与相关的学习算法。SVM模型是一个非概率二元线性分类器，若从空间角度理解，SVM模型将样本表示为空间中的点，并试图找出一个最优的超平面来分隔样本，基于样本落在间隔的哪一侧，来预测所属的类别。对于支持向量机来说，数据点若是$p$维向量，我们用$p−1$维的超平面来分开这些点。但是可能有许多超平面可以把数据分类。最佳超平面的一个合理选择就是以最大间隔把两个类分开的超平面。因此，SVM选择能够使离超平面最近的数据点的到超平面距离最大的超平面。1）SVM模型原理推导由于SVM模型是一个非概率二元线性分类器，可以设有一个最优的参数组合$(w^,b^)$，使得分界最优，则该超平面$Η$可以用下式表示：\\[\\{\\vec{x}|\\vec{w^*}^T\\vec{x}+b^*=0\\}\\]易知$\\vec{w^*}⊥H$。如下图，对该Hyperplane，为使其分类效果达到最优，我们需要使其两侧的样本点到它的距离$margin$最大。对于SVM模型来说，真正重要的是$Margin$内的样本点，即靠近超平面的点，这些样本点便是支持向量（Support Vector）。支持向量对模型起着决定性的作用，这也是“支持向量机”名称的由来。为求支持向量到超平面的距离，过样本点作与超平面平行的平面，可以表示为：​\\[\\{\\vec{w}^T\\vec{x}+b=h(\\vec{x})\\}\\]如图，假设$\\vec{x}$在$H$的右侧，则有：\\[\\left\\{ \\begin{array}{**lr**} \\vec{x}=\\vec{m_1}+\\vec{m_2} \\\\ \\vec{w^T}\\vec{m_1}+\\vec{b}=0 \\\\ \\vec{w^T}\\vec{x}+b=h(\\vec{x}) \\end{array} \\right.\\therefore \\vec{w^T}\\vec{m_2}=h(\\vec{x})\\]\\[\\because \\vec{m_2}⊥H,\\vec{w}⊥H\\therefore \\vec{m_2}//\\vec{w}\\]\\[\\left\\{ \\begin{array}{**lr**} \\vec{w^T}\\vec{m_2}=h(\\vec{x}) \\\\ \\vec{m_2}=r\\frac{\\vec{w}}{||\\vec{w}||} \\end{array} \\right.\\therefore r\\frac{\\vec{w^T}\\vec{w}}{||\\vec{w}||}=h(\\vec{x})\\]\\[\\therefore r=\\frac{h(\\vec{x})}{||\\vec{w}||}=||\\vec{m_2}||\\]假设$\\vec{x}$在$H$的左侧，则有：\\[r=-\\frac{h(\\vec{x})}{||\\vec{w}||}=||\\vec{m_2}||\\]故写成：\\[d_{H-h(\\vec{x})} = yr = \\frac{y(\\vec{w^T}\\vec{x}+b)}{||\\vec{w}||},y∈\\{-1，1\\}\\]计算最大间隔及其对应的最优参数：\\[Margin = d_{{H-h(\\vec{x})}_{min}}=min_l\\frac{y^l(\\vec{w^T}\\vec{x}+b)}{||\\vec{w}||}\\]\\[(\\vec{w^*},\\vec{b^*})=argmax_{\\vec{w},b} \\frac{1}{||\\vec{w}||} min_l[{y^l(\\vec{w^T}\\vec{x}+b)}]\\]该问题可以根据拉格朗日对偶性，转化为二次凸优化问题：\\[min_{\\vec{w},b}\\frac{1}{2}||\\vec{w}||^2 s.t. y^l(\\vec{w^T}\\vec{x}+b)≥1,\\forall l\\]得到最优超平面对应的参数值$(w^,b^)$，最终的预测值为：\\[\\hat{y(\\vec{x})}=sign(\\vec{w^*}^T\\vec{x}+b^*)\\]实践在实践中，支持向量机学习方法有一些由简至繁的模型: 线性可分SVM当训练数据线性可分时，通过硬间隔(hard margin)最大化可以学习得到一个线性分类器，即硬间隔SVM，即上述方法。 线性SVM当训练数据不能线性可分但是可以近似线性可分时，通过软间隔(soft margin)最大化也可以学习到一个线性分类器，即软间隔SVM。相比硬间隔，软间隔放宽了硬间隔最大化的条件，允许少量样本不满足约束$s.t. y^l(\\vec{w^T}\\vec{x}+b)≥1,\\forall l$。为使这些样本点尽可能少，需要在优化目标函数中新增一个对这些点的惩罚项。常用的损失函数是hinge loss(合页损失)，$Loss_{hinge}(z)=max(0,1-z)$。要使软间隔最大化，优化目标将形如：\\[min_{W,b,\\xi}\\frac{1}{2}||W||^2+C\\sum^n_{i=1}{\\xi_i} s.t.y^i(\\vec{X_i}^T\\vec{W}+b)≥1-\\xi_i,\\forall i, \\xi_i≥0\\] 非线性SVM当训练数据线性不可分时，通过使用核技巧(kernel trick)和软间隔最大化，可以学习到一个非线性SVM。通过核函数，支持向量机可以将特征向量映射到更高维的空间中，使得原本线性不可分的数据在映射之后的空间中变得线性可分。假设原始向量为x，映射之后的向量为z，这个映射为：\\[\\boldsymbol{z} = \\varphi(\\boldsymbol{x})\\]在实现时不需要直接对特征向量做这个映射，而是用核函数对两个特征向量的内积进行变换，这样做等价于先对向量进行映射然后再做内积：\\[K(x_i,x_j)=K(x_i^Tx_j)=\\phi(x_i)^T\\phi(x_j)\\]在这里K为kernel核函数。常用的非线性核函数有多项式核，高斯核（也叫径向基函数核，RBF）。下表列出了各种核函数的计算公式：若想训练一个多分类模型，也可将多个SVM模型进行组合，得到多个划分超平面。2）Hinge Loss在机器学习中，hinge loss作为一个损失函数(loss function)，通常被用于最大间隔算法(maximum-margin)，而最大间隔算法又是SVM(支持向量机support vector machines)用到的重要算法。事实上，SVM的学习算法有两种解释，除了间隔最大化与拉格朗日对偶之外，便是Hinge Loss。Hinge loss专用于二分类问题，标签值$y_i=±1$，预测值$\\hat{y_i}=\\boldsymbol{x_i}\\boldsymbol{w}+b∈R$，$\\boldsymbol{x_i}\\in R^{1\\times number\\ of\\ feature}$,$\\boldsymbol{w}\\in R^{number\\ of\\ feature\\times 1}$对单个样本点，其损失函数为：\\[HingeLoss_\\hat{y_i}=max(0,1-y_i\\hat{y_i})\\]从该式可以看出，该损失函数忽视正确分类且距离预测值分界点较远的样本点，只关注错误分类或正确分类但距离预测值分界点较近的样本点，这正是SVM中支持向量的概念。其体现了SVM中支持向量的稀疏性。该函数在零点处不可导，但可以在编程时用梯度下降法实现。在$1-y_i\\hat{y_i}&gt;0$时对其求导：\\[\\frac{\\partial L_i}{\\partial \\vec{w}}=-y_i\\vec{x_i}^T\\]使用梯度下降法优化参数，$\\alpha$为学习率：\\[\\boldsymbol{w_{i+1}}=\\boldsymbol{w_i}-\\alpha \\sum_{i=l}^N\\frac{\\partial L_l}{\\partial \\boldsymbol{w_i}}\\]若加上L2正则化项（软间隔），总损失函数为：\\[HingeLoss = \\sum_{i=1}^N HingeLoss_{\\hat{y_i}} + \\lambda||\\boldsymbol{w}||^2\\]3）Cross-Entropy Loss二分类在二分类的情况下，模型最后需要预测的结果只有两种情况，对于每个类别我们的预测得到的概率为 p 和 1−p ，此时表达式为（log 的底数是 e）：\\[L=\\frac{1}{N}∑_iL_i=\\frac{1}{N}\\sum_i^N−[y_i⋅log(p_i)+(1−y_i)⋅log(1−p_i)]\\]其中：-$ y_i$ —— 表示样本 $i$ 的$label$，正类为 $1$ ，负类为 $0$- $p_i$ —— 表示样本 $i$ 预测为正类的概率在交叉熵模型中，通过$z_i=\\vec{x_i}\\vec{w}+b$得到线性的预测值，再用$\\sigma(z)$转换为概率值$p_i$，在二分类情况下，一般使用sigmoid函数：\\[p_i=\\sigma(z_i)=sigmoid(z_i)=\\frac{1}{1+e^{-z_i}}\\]\\[\\frac{\\partial p_i}{\\partial z_i}=\\sigma'(z_i) =\\sigma(z_i)(1-\\sigma(z_i))=p_i(1-p_i)\\]对损失函数求导：\\[\\frac{\\partial L_i}{\\partial \\boldsymbol w}=\\frac{\\partial L_i}{\\partial p_i}\\frac{\\partial p_i}{\\partial z_i}\\frac{\\partial z_i}{\\partial \\boldsymbol w}\\]\\[\\frac{\\partial L_i}{\\partial \\boldsymbol w}=(-\\frac{y_i}{p_i}+\\frac{1-y_i}{1-p_i})p_i(1-p_i)\\boldsymbol x_i^T\\]\\[\\frac{\\partial L_i}{\\partial \\boldsymbol w}=(p_i-y_i)\\boldsymbol x_i^T\\]使用梯度下降法优化参数，$\\alpha$为学习率：\\(\\boldsymbol{w_{i+1}}=\\boldsymbol{w_i}-\\alpha \\sum_{l=1}^N\\frac{\\partial L_l}{\\partial \\boldsymbol{w_i}}\\)若加上L2正则化项（软间隔），总损失函数为：{% raw %}\\(CrossEntropy Loss = \\frac{1}{N}∑_i^T−[y_i⋅log(p_i)+(1−y_i)⋅log(1−p_i)] + \\lambda||\\boldsymbol{w}||^2\\)多分类多分类的情况实际上就是对二分类的扩展：\\[L=\\frac{1}{N}∑_iL_i=-\\frac{1}{N}∑_i∑_{c=1}^My_{ic}log⁡(p_{ic})\\]其中：- $M$ ——类别的数量- $y_{ic}$ ——符号函数（ 0 或 1 ），如果样本 $i$ 的真实类别等于 $c$ 取 $1$ ，否则取 $0$- $p_{ic}$ ——观测样本 $i$ 属于类别 $c$ 的预测概率4）库函数from sklearn import svmmodel=svm.SVC(*, C: *float* = 1, kernel: *str* = \"rbf\", degree: *int* = 3, gamma: *str* = \"scale\"...)在SVC函数中，注意到几个参数： C：C是惩罚系数，即对误差的宽容度。c越高，说明越不能容忍出现误差,容易过拟合。C越小，容易欠拟合。C过大或过小，泛化能力变差。C的默认取值为1.0。 C=0.1 C=1000 kernel：linear（线性核）、poly（多项式核）、rbf（径向基核函数）、sigmoid（Sigmoid核）、precomputed（如果使用precomputed模式,也就是不传入函数,而直接传入计算后的核,那么参与这个核计算的数据集要包含训练集和测试集），默认为rbf（高斯核）。 degree：如果选择多项式核，则需要进一步设置多项式的次数这个参数——degree，默认为3。 gamma：gamma是选择RBF函数作为kernel（高斯核函数）后，该函数自带的一个参数。隐含地决定了数据映射到新的特征空间后的分布，gamma越大，支持向量越少，gamma值越小，支持向量越多。支持向量的个数影响训练与预测的速度。 5）手动实现Hinge Loss线性分类模型模型预测值：$f(X;w,b)=sign(Xw+b)$$w’=w⊕b$$X’=X⊕[1,..,1]$则模型可化为: $f(X’;w’)=sign(X’w’)$算法流程 读取、分析、导入数据 随机初始化对应维度的权重$\\vec{w}$与偏置$b$ 用train set训练模型，利用梯度下降算法，以Hinge Loss为损失函数不断更新迭代$\\vec{w}$​参数\\[\\frac{\\partial L_i}{\\partial \\vec{w}}=-y_i\\vec{x_i}^T,1-y_i\\hat{y_i}&gt;0\\]\\[\\boldsymbol{w_{i+1}}=\\boldsymbol{w_i}-\\alpha \\sum_{i=l}^N\\frac{\\partial L_l}{\\partial \\boldsymbol{w_i}}\\] 直到Loss值收敛，得到构造最优分类超平面的最优的权重$\\vec{w}$ 记录每次迭代的平均损失值，绘制Loss值趋势图 用test set验证模型的准确性 代码实现一个Hinge Loss线性分类器。class LinearClassifier(): def __init__(self, feat): self.w = np.random.rand(feat+1,1) # initialize the weight; shape like (feat,1) self.feat = feat def train(self, X_, y, alpha=1e-5, max_iter=100, reg=False, lamb=0.5): \"\"\"train the model Args: X (numpy.ndarray): input of trainset, shape like (sample,feat). y (numpy.ndarray): label of trainset, shape like (sample,1). alpha (float,optional): learning rate. Defaults to 1e-3. max_iter (int, optional): epoch. Defaults to 100. reg (bool,optional): whether to L2-regularize. Defaults to True. lamb (float, optional): parameter of L2-regularization. Defaults to 0.5. \"\"\" X = np.column_stack((X_,np.ones((X_.shape[0],1)))) sample = X.shape[0] for epoch in range(max_iter): avg = 0 # Gradient Descend for i in range(sample): x = X[i].reshape(1,self.feat+1) loss = max(0,1 - (y[i] * np.dot(x,self.w)[0][0])) if loss != 0: self.w -= alpha * (-y[i]*x.T) if reg: self.w *= 1-alpha*lamb*2 loss += lamb*np.linalg.norm(self.w,2) avg+=loss los.append(avg/sample) def predict(self,X_): \"\"\"predict label of X_ by trained weights Args: X_ (np.ndarray): input of dataset, shape like (sample,feat). Returns: np.ndarray: predicted label of X_, shape like(sample,1). \"\"\" X = np.column_stack((X_,np.ones((X_.shape[0],1)))) Y = np.dot(X,self.w) Y[Y&gt;0] = 1 Y[Y&lt;=0] = -1 return Y def Ascore(self, y, pre): # Accuracy 准确率：分类器正确分类的样本数与总样本数之比 sample = y.shape[0] error = np.linalg.norm(y-pre.ravel(),1)/2 return 1-error/sampleCross-Entropy Loss线性分类模型模型预测值：$f(X;w,b)=sign(sigmoid(Xw+b)-0.5)$$w’=w⊕b$$X’=X⊕[1,..,1]$则模型可化为: $f(X’;w’)=sign(sigmoid(X’w’+b)-0.5)$算法流程 读取、分析、导入数据 随机初始化对应维度的权重$\\vec{w}$与偏置$b$ 用train set训练模型，利用梯度下降算法，以为Cross-Entropy Loss损失函数不断更新迭代$\\vec{w}$参数\\[\\frac{\\partial L_i}{\\partial \\boldsymbol w}=(p_i-y_i)\\boldsymbol x_i^T\\]\\[\\boldsymbol{w_{i+1}}=\\boldsymbol{w_i}-\\alpha \\sum_{i=l}^N\\frac{\\partial L_l}{\\partial \\boldsymbol{w_i}}\\] 直到Loss值收敛，得到构造最优分类超平面的最优的权重$\\vec{w}$ 记录每次迭代的平均损失值，绘制Loss值趋势图 用test set验证模型的准确性 代码实现一个Cross-Entropy Loss线性分类器。class LinearClassifier(): def __init__(self, feat): self.w = np.random.rand(feat+1,1) # initialize the weight; shape like (feat,1) self.feat = feat def sigmoid(self,z): warnings.filterwarnings('ignore') return 1 / (1 + np.exp(-z)) def train(self, X_, y, alpha=1e-5, max_iter=100, reg=False, lamb=0.2): \"\"\"train the model Args: X (numpy.ndarray): input of trainset, shape like (sample,feat). y (numpy.ndarray): label of trainset, shape like (sample,1). alpha (float,optional): learning rate. Defaults to 1e-3. max_iter (int, optional): epoch. Defaults to 100. reg (bool,optional): whether to L2-regularize. Defaults to True. lamb (float, optional): parameter of L2-regularization. Defaults to 0.5. \"\"\" X = np.column_stack((X_,np.ones((X_.shape[0],1)))) sample = X.shape[0] for epoch in range(max_iter): avg = 0 # gradient descend for i in range(sample): x = X[i].reshape(1,self.feat+1) z = np.dot(x,self.w)[0][0] p = self.sigmoid(z)+1e-8 #防止p=0造成loss中log函数错误 loss = -y[i]*log(p)-(1-y[i])*log(1-p) self.w -= alpha * (p-y[i])*x.T if reg: self.w *= 1-alpha*lamb*2 loss += lamb*np.linalg.norm(self.w,2) avg+=loss los.append(avg/sample) def predict(self,X_): \"\"\"predict label of X_ by trained weights Args: X_ (np.ndarray): input of dataset, shape like (sample,feat). Returns: np.ndarray: predicted label of X_, shape like(sample,1). \"\"\" X = np.column_stack((X_,np.ones((X_.shape[0],1)))) Y = self.sigmoid(np.dot(X,self.w)) Y[Y&gt;0.5] = 1 Y[Y&lt;=0.5] = 0 return Y def Ascore(self, y, pre): #Accuracy 准确率：分类器正确分类的样本数与总样本数之比 sample = y.shape[0] error = np.linalg.norm(y-pre.ravel(),1) return 1-error/sample" }, { "title": "OS Outline", "url": "/posts/os/", "categories": "Courses", "tags": "OS", "date": "2022-06-28 09:36:30 +0800", "snippet": " OS期末复习 1. Introduction OS定义 现代计算机硬件组成 进程（processor） 存储 IO 中断 中断的分类 2. 操作系统结构 操作系统服务 用户和程序员接口 系统组件 简单结构 微内核 模块 3. 进程 进程概念 进程 状态 进程表（Process Table） 进程控制块（Prosess Control Block，PCB） 进程调度（Process Scheduling） 调度队列 调度程序 上下文切换（Context Switch） 进程运行 进程创建 进程终止 进程通信（InterProcess Communication，IPC） 共享内存 消息传递 命名 同步 缓存 例子 POSIX共享内存 Mach Windows 客户机/服务器通信 套接字（socket） 远程过程调用（RPC） 管道（pipe） 普通管道 命名管道 4. 线程 并行与并发 多线程 多对一 一对一 多对多 线程库 隐式多线程（implicit multithreading） 线程池 OpenMp 大中央调度（Grand Central Dispatch，GCD） 多线程问题 信号处理 线程撤销（thread cancellation） 线程本地存储（Thread-Local Storage，TLS） 调度程序激活 例子 Windows线程 Linux线程 OS期末复习1. IntroductionOS定义一个操作系统是管理计算机硬件的程序，为应用程序提供基础，充当计算机用户和硬件的中介。现代计算机硬件组成进程（processor）计算机运行： 运行初始程序或引导程序（bootstrap program，一般位于ROM只读内存），定位操作系统内核并加载到内核 系统程序加到内存便成为系统进程/系统后台程序，生命周期与内核一致。 事件发生通过硬件或软件的中断来通知。硬件通过系统总线bus发送信号到CPU，软件通过调用system call与trap触发中断。存储 内存（main memory） CPU只能从内存中加载指令，因此执行程序位于内存。 内存一般称为RAM，通常为动态随机访问内存DRAM，采用半导体技术实现。也存在ROM内存，只能将静态程序（如引导程序）存在ROM只读内存中。 内存具有易失性（volatile），掉电失去所有内容。 外存（secondary storage）：磁盘、硬盘、固态硬盘、光盘 高速缓存器（Cache）：Cache Coherency (缓存一致性)IO IO中断驱动：接到I/O请求后，I/O设备传送数据，CPU继续执行用户进程，传送完成后，CPU进行I/O中断处理，处理完成后，再回到用户进程。 直接内存访问（Direct Memory Access，DMA）：外部设备不通过CPU而直接与系统内存交换数据的接口技术。 对于一个高速I/O设备，以及批量交换数据的情况，只能采用DMA方式，才能解决效率和速度问题。 DMA在外设与内存间直接进行数据交换，而不通过CPU，这样数据传送的速度就取决于存储器和外设的工作速度。 通常系统的总线是由CPU管理的。在DMA方式时，就希望CPU把这些总线让出来，即CPU连到这些总线上的线处于第三态–高阻状态，而由DMA控制器接管，控制传送的字节数，判断DMA是否结束，以及发出DMA结束信号。 中断中断指计算机CPU获知某些事，暂停正在执行的程序，转而去执行处理该事件的程序，当这段程序执行完毕后再继续执行之前的程序。整个过程称为中断处理，简称中断，而引起这一过程的事件称为中断事件。中断是计算机实现并发执行的关键，也是操作系统工作的根本。中断的分类 内外部 可屏蔽和不可屏蔽 同异步中断按事件来源分类，可以分为外部中断（external）和内部中断（internal）。中断事件来自于CPU外部的被称为外部中断，来自于CPU内部的则为内部中断。进一步细分，外部中断还可分为可屏蔽中断（maskable interrupt）和不可屏蔽中断（non-maskable interrupt）两种，而内部中断按事件是否正常来划分可分为软中断和异常两种。外部中断的中断事件来源于CPU外部，必然是某个硬件产生的，所以外部中断又被称为硬件中断（hardware interrupt）。外部设备的中断信号是通过两根信号线通知CPU的，一根是INTR，另一根是NMI。CPU从INTR收到的中断信号都是不影响系统运行的，CPU可以选择屏蔽（通过设置中断屏蔽寄存器中的IF位），而从NMI中收到的中断信号则是影响中断处理器运行的严重错误，不可屏蔽。内部中断来自于处理器内部，其中软中断是由软件主动发起的中断，常被用于系统调用（system call）；而异常则是指令执行期间CPU内部产生的错误引起的。异常也和不可屏蔽中断一样不受eflags寄存器的IF位影响，区别在于不可屏蔽中断发生的事件会导致处理器无法运行（如断电、电源故障等），而异常则是影响系统正常运行的中断（如除0、越界访问等）。中断还可以分为同步中断（被称为异常）和异步中断（被称为中断）。同步中断是在指令执行时由CPU主动产生的，受到CPU控制，其执行点是可控的。异步中断是CPU被动接收到的，由外设发出的电信号引起，其发生时间不可预测。2. 操作系统结构操作系统服务用户功能 用户界面（User Interface，UI）：有命令行界面（Command-Line Interface，CLI）、批处理界面（batch interface），图形用户界面（Graphical User Interface，GUI）。 程序执行：系统加载程序到内存并运行。 I/O操作 文件系统操作 通信：通信存在于同一台计算机的两个进程，或通过网络连接的不同计算机进程间。实现方式为共享内存（shared memory）和消息交换（message passing）。 错误检测提高系统效率的功能 资源分配：CPU周期、内存、文件存储、I/O设备 记账：记录用户使用资源的类型和数量 保护与安全：多用户/网络信息安全；独立进程并发执行时，不干预其他进程或操作系统本身；保护外部I/O设备不受非法访问。用户和程序员接口系统调用接口系统调用（system call）提供操作系统服务接口。开发者一般根据应用编程接口（Application Programming Interface，API）来设计程序。这样做的好处是提高程序的可移植性。系统调用接口截取API函数的调用，并调用操作系统中的所需系统调用。每个系统调用都有一个相关数字，系统调用接口根据这些数字来建立一个索引列表。向操作系统传递参数的方法： 通过寄存器传递参数 将参数存在内存的块或表中，将其地址通过寄存器传递，Linux和Solaris就采用这种方法 通过程序放在或压入（push）到堆栈（stack），并通过操作系统弹出（pop）系统调用类型 进程控制（process control） 举例：MS-DOS操作系统是单任务系统，不会创建新进程；FreeBSD（源于Berkeley UNIX）是多任务系统，会启动新进程。 UNIX函数：fork(), exit(), wait() 文件管理（file manipulation） UNIX函数：open(), read(), write() 设备管理（device manipulation） 信息维护（information maintenance） 通信（communication） 保护（protection） 系统组件简单结构MS-DOS系统利用最小空间提供最多功能，没有被划分为模块，没有很好地区分功能的接口和层次。UNIX系统由内核和系统程序两个独立部分组成，采用单片结构，系统调用接口之下和物理硬件之上的所有部分可以被视为内核，内核通过系统调用提供操作系统功能。微内核Mach操作系统采用微内核技术对内核进行模块化，从内核中删除所有非必须的部件，将其当作系统级与用户级的程序来实现，因此内核较小。微内核的主要功能是，为客户端程序和运行在用户空间中的各种服务提供通信，采用消息传递模式。微内核的优点之一是便于扩展操作系统，可移植性高。模块采用可加载的内核模块（loadable kernel module）：内核提供核心服务，而其它服务可在内核运行时动态实现，动态链接服务优于直接添加新功能到内核。这种方法与微内核的区别在于：模块无需调用消息传递来进行通信。举例：Solaris，linux3. 进程进程概念进程进程是执行的程序，其包括： 程序代码，或称为文本段（text section）、代码段（code section） 当前活动 程序计数器（program counter，PC） 处理器寄存器内容 进程堆栈（stack），包括临时数据（函数参数、返回地址、局部变量） 数据段（data section），包括全局变量 堆（heap），进程运行时动态分配的内存程序是被动实体，当一个可执行文件被加载到内存时，这个程序就成为进程。进程本身可以作为一个环境，执行其他代码。例如：python test.py可以运行该python代码。状态 new：进程正在创建。 running：进程正在执行。 waiting：进程等待发生某个事件（I/O完成或收到信号）。 ready：进程等待分配处理器 terminated：进程已经完成执行。一次只有一个进程可在一个处理器上运行；但是许多进程可处于就绪或等待状态。进程表（Process Table）进程表是一个内核数据结构，包含用于进程管理、内存管理、文件管理三种数据记录的空间，内核总是有权限访问这三片空间。进程控制块（Prosess Control Block，PCB）操作系统使用进程控制块表示每个进程。操作系统根据PCB管理和控制进程，这是进程存在的标志。进程调度（Process Scheduling）分时系统的目标：在进程间快速切换CPU，以便用户在程序运行时能与其交互。调度队列 作业队列（job queue）：系统中所有的进程 就绪队列（ready queue）：驻留在内存中的、就绪等待运行的进程 设备队列（device queue）：等待特定I/O设备的进程列表调度程序无法立即执行的进程会被保存到大容量存储设备（如磁盘）的缓冲池。 长期调度程序/作业调度程序：从缓冲池中选择进程，加载到内存（加入就绪队列）。长期调度进程控制多道程序程度（内存中的进程数量），从I/O密集型进程和CPU密集型进程中选择。 短期调度程序/CPU调度程序：从准备执行的进程（就绪队列）中选择进程，分配CPU 中期调度程序（交换）：将进程从内存中移出，从而降低多道程序程度。此后，进程可被重新调入内存，并从中断处继续执行。长期与短期调度程序主要区别是执行频率。上下文切换（Context Switch）中断发生时，CPU从执行当前任务改变到执行内核程序（从用户到内核），系统需要保存当前运行进程的上下文，以便恢复进程。进程上下文用PCB表示。内核将旧进程状态保存在其PCB中，然后加载经调度而要执行的新进程的上下文。进程运行相关函数：#include &lt;sys/types.h&gt;#include &lt;unistd.h&gt;//得到进程的PIDpid_t getpid(void);//得到进程的PPIDpid_t getppid(void);//创建进程pid_t fork(void);//对父进程返回0，对子进程返回一个比大于0的pid，失败则返回小于0的数pid_t vfork(void);//替换进程int execv(const char *path, char *constargv[]); //终止进程exit(1);//回收子进程pid_t wait(int);进程创建父进程创建新的子进程，每个新进程可以再创建其他进程，从而形成进程树。多数操作系统采用唯一进程标识符（process identifier，PID）来作为进程索引，访问内核中进程的各种属性。在Linux系统中（相比进程，linux偏爱task这个术语），进程init的PID总是1，作为所有用户进程的根进程或父进程。init进程是linux内核启动后第一个执行的进程，作为引导程序，启动守护进程并且运行必要的程序。对于UNIX和Linux系统，可以通过ps命令得到进程列表。当一个进程创建子进程： 子进程可以从两处获取资源（CPU时间、内存、文件、I/O设备等）： 操作系统 父进程（除了逻辑资源，子进程从父进程获取参数初始化） 父子进程的执行顺序： 父进程与子进程并发执行 父进程等待，直到某个或全部子进程执行完 父子进程的地址空间： 子进程是父进程的复制品 子进程拷贝父进程的地址空间，与父进程具有同样的程序和数据副本，虚拟映射地址一致，物理地址不一致 子进程共享父进程的地址空间，虚拟映射地址一致，物理地址一致 子进程加载另一个新程序（exec()可进行进程替换） 进程终止级联终止（Cascading Termination）：有些系统不允许子进程在父进程已经终止的情况下存在。如果一个进程终止，那么它的所有子进程也应终止，这种现象成为级联终止，通常由操作系统启动。父进程一旦调用了wait，就立即阻塞自己，由wait自动分析是否当前进程的某个子进程已经退出，如果让它找到了这样一个已经变成僵尸的子进程，wait就会收集这个子进程的信息，并把它彻底销毁后返回；如果没有找到这样一个子进程，wait就会一直阻塞在这里，直到有一个出现为止。假如父进程没有调用wait()就终止，子进程会成为孤儿进程，其PCB包含了进程的退出状态，一直被保留在进程表中，无法释放。Linux和UNIX会将init进程作为孤儿进程的父进程。进程init定期调用wait()，收集任何孤儿进程的退出状态，释放PID和进程表条目。进程通信（InterProcess Communication，IPC）操作系统内并发执行进程可以是独立或协作的。协作进程需要有一种IPC机制。进程间通信有两种基本模型： 共享内存。共享内存系统仅需要在建立共享内存区域时需要系统调用，一旦建立共享内存，所有访问无需借助内核。但对于多核系统而言，共享内存具有高速缓存一致性的问题。 消息传递。消息传递的实现经常采用系统调用，需要消耗更多时间使内核介入。但对于多核系统而言，消息传递的性能更高。共享内存生产者-消费者缓冲区分为无界缓冲区和有界缓冲区。消息传递命名 直接通信：需要通信的每个进程必须明确指示通信的接收者或发送者。 对称寻址：发送和接受一对一。 非对称寻址：发送者指定接收者。 缺点：生成进程定义的有限模块化。 间接通信：通过邮箱或端口作为中介来发送和接受消息，其可归属于操作系统或进程。 最基本的通信原语有两条,它们是send原语和receive原语。send(M,N)中，M为信箱名同步阻塞和非阻塞同步和异步缓存通信进程交换的消息总是驻留在临时队列中。根据容量分类： 零容量 有限容量 无限容量例子POSIX共享内存Mach内核通过内核邮箱与任务通信，将事件发生的通知发送到通知邮箱。消息结构：固定大小的头部（消息长度、发送端口、接收端口）和可变大小的数据。Windows客户机/服务器通信客户机/服务器系统除了利用共享内存和消息传递进行通信，还有以下三种策略：套接字（socket）每个套接字由一个IP地址和一个端口号组成。 客户进程发出连接请求，其主机为其分配一个端口。这个端口是大于1024的某个数字。 服务器调用accpet()监听端口。 客户端创建一个套接字，连接到服务器监听的端口。套接字属于分布式进程之间的一种低级形式的通信，其中一个原因是，只允许在通信线程之间交换无结构的字节流。远程过程调用（RPC）中介子函数转发管道（pipe）普通管道普通管道是单向无名管道。 对于UNIX系统，子进程自动继承由父进程创建的管道。 对于Windows系统，程序员需要指定子进程继承的属性。 对于Windows和UNIX系统，采用普通管道通信的进程需要有父子关系，这说明这些管道只可用于同一机器的进程间通信。 对于Windows和UNIX系统，一旦进程已经完成通信并终止，普通管道就不存在了。命名管道命名管道可以是双向管道，其进程间的父子关系不必须，但必须存在同一机器上；当通信进程完成后，命名管道继续存在。对于UNIX系统，命名管道为FIFO，一旦创建即表现为文件系统的典型文件，可以通过系统调用读写文件。FIFO一经创建会一直存在，直到被显式删除文件。FIFO只允许半双工传输。对于Windows系统，允许全双工通信，且通信进程可以位于同一机器或不同机器。 单工：简单的说就是一方只能发信息，另一方则只能收信息，通信是单向的。 半双工：比单工先进一点，就是双方都能发信息，但同一时间则只能一方发信息。 全双工：比半双工再先进一点，就是双方不仅都能发信息，而且能够同时发送。管道经常用于将一个命令的输出作为另一个命令的输入，例如：ls | more（UNIX命令行）、dir | more（Windows命令行）4. 线程每个线程是CPU使用的一个基本单元，其包括线程ID、程序计数器PC，寄存器组和堆栈，与同一进程的其他线程共享代码段、数据段和文件等其他操作系统资源。 代码 数据 文件 线程1的寄存器组 线程2的寄存器组 线程3的寄存器组 线程1的堆栈 线程2的堆栈 线程3的堆栈 线程1的ID、PC及活动内容 线程2的ID、PC及活动内容 线程3的ID、PC及活动内容 传统或重量级进程（Heavy Weight Process，HWP）只有单个控制线程。例如，进程创建是重量级进程，而线程创建是轻量级进程。进程与线程的关系：线程的优点： 响应快 线程默认共享他们所属进程的内存和资源 创建和切换线程更加经济 具有可伸缩性，线程可在多处理器核上并行运行并行与并发处理核只能同一时间执行单个线程。多核（multicore）系统/多处理器（multiprocessor）系统能够并行运行，因为系统可以为每个核分配一个单独线程。而单处理器系统只能并发运行。 并行：同时执行多个任务 数据并行：将数据分布于多个计算核上，并在每个核上执行相同操作 任务并行：将线程而不是数据分配到多个计算核，每个线程并行执行独特的操作。 并发：支持多个任务，允许所有任务都取得进展；线程能够交错执行。CPU调度器通过快速切换进程，以便每个进程取得进展，造成并行的假象。Amdahl定律：\\(加速比 ≤ \\frac{1}{S+\\frac{1-S}{N}}\\)，其中，S为应用程序中串行执行的占比，1-S自然为并行执行的占比，该系统具有N个处理器核。多线程用户线程位于内核之上，管理无需内核支持；而内核线程由操作系统直接支持和管理。多对一多对一模型映射多个用户级线程到一个内核线程。优点：线程管理由用户空间的线程库完成，效率较高。缺点：一个线程阻塞，整个进程阻塞；该线程无法并行运行在多处理核系统上。一对一一对一模型映射每个用户线程到一个内核线程。优点：提供并发性；允许多个线程并行运行在多处理器系统上。缺点：创建一个用户线程就要创建一个相应的内核线程，开销影响性能，实现需要限制系统支持的线程数量。多对多多对多模型多路复用多个用户级线程到同样数量或更少的内核线程。（内核线程数≤用户级线程数）优点：可以创建任意多的用户线程，且内核线程能在多处理器系统上并发执行。该模型的亚种是双层模型（tow-level model），允许绑定某个用户线程到一个内核线程。线程库线程库（thread library）为程序员提供创建和管理线程的API。实现方法： 在用户空间内提供一个没有内核支持的库，所有代码和数据结构都位于用户空间，调用库内的函数只是用户空间的本地调用而非系统调用 实现由操作系统直接支持的内核级的一个库，所有代码和数据结构都位于内核空间，调用库内API函数会导致系统调用目前主要使用的线程库： POSIX Pthreads，可提供用户级或内核级的库；全局声明数据可以为同一进程的所有线程共享 Windows，可提供内核级的库；全局声明数据可以为同一进程的所有线程共享 Java，采用宿主系统的线程库来实现；线程对共享数据的访问需要显式安排多线程创建的常用策略 异步线程：父线程创建子线程后即恢复自身运行，与子线程并发执行，相互独立，很少数据共享 同步线程：父线程在回复执行前等待所有子线程的终止，通常涉及大量数据共享，例如由父进程组合输出子线程计算的结果隐式多线程（implicit multithreading）将多线程的创建管理交给编译器和运行时的库来完成，这种策略称为隐式线程。线程池线程池的主要思想：在进程开始时创建一定数量的线程，并加到池中等待工作。当服务器收到请求，其唤醒池内的可用线程传递请求，一旦线程完成服务，回到池中等待工作。如果池中没有可用线程，那么服务器会等待。线程池的优点： 高效。用现有程序服务请求比等待创建线程更快。 限制了任何时候可用线程的数量，保障了容量安全 允许采用不同策略运行任务OpenMp大中央调度（Grand Central Dispatch，GCD）多线程问题信号处理UNIX信号用于通知进程某个特定事件已发生。其模式为：特定事件的发生产生了信号-&gt;信号被传递给某个进程-&gt;进程一旦接收到信号就应处理信号的接收是同步还是异步，取决于事件信号的来源和原因。 当一个信号由运行程序以外的事件产生，该进程就异步接收这一信号。 同步信号发送到由于导致该信号的同一进程。信号处理程序有： 缺省的信号处理程序 用户定义的信号处理程序例如ptrhread_kill()线程撤销（thread cancellation）线程撤销是在线程完成之前终止线程。 异步撤销：一个线程立即终止目标线程（他杀）；可能会不会释放必要的系统资源。 延迟撤销：目标线程不断检查它是否应终止，这允许目标线程有机会有序终止自己（教唆自杀）线程本地存储（Thread-Local Storage，TLS）每个线程独有的本地数据叫线程本地存储。其与局部变量的区别是，局部变量只在单个函数调用时才可见，而TLS数据在多个函数调用时都可见。TLS类似于静态static数据。调度程序激活调度器激活（scheduler activation）是用户线程库与内核之间的一种通信方案，内核提供一组虚拟处理器（LWP）给应用程序，而应用程序调度用户线程到任何可用的虚拟处理器。内核将有关特定时间通知应用程序，该步骤称为回调（upcall），由线程库通过回调处理程序（upcall handler）来处理。例子Windows线程线程一般包括： 线程ID，TID 寄存器组 用户堆栈，内核堆栈 私有存储区域，用于各种运行时库和动态链接库（DLL）后三种部件通常称为线程上下文（context）。Linux线程批处理系统、分时系统、实时操作系统的特点和比较_马小超i的博客-CSDN博客_分时系统,实时系统,批处理系统操作系统考点之PV操作、信号量_guangod的博客-CSDN博客_操作系统信号量pv操作操作系统进程状态模型__吟游诗人的博客-CSDN博客_进程的七状态模型操作系统之多道程序设计_莫之的博客-CSDN博客_多道程序设计最高响应比优先算法（HRRF）及例题详解_EMT00923的博客-CSDN博客_响应比高者优先调度算法例题进程典型七大调度算法_emcpper的博客-CSDN博客_常见的进程调度算法有哪些" }, { "title": "picgo报错Error: connect ECONNREFUSED 127.0.0.1:443的解决方式", "url": "/posts/picgo/", "categories": "log", "tags": "picgo", "date": "2022-06-04 21:39:30 +0800", "snippet": "picgo报错Error: connect ECONNREFUSED 127.0.0.1:443的解决方式默认图床是github，考虑了几种情况： 代理的问题：搜索得到的方案，在浏览器设置搜索代理，打开系统代理设置，切换手动代理的状态，没用 图床的问题：检查了更新；在github仓库上传了图片，图床相册没有更新，直接在图床程序内上传图片也失败；考虑过监听端口占用的问题，似乎无关；检查了github图床的配置，路径没有问题 网络的问题：看了picgo.log日志文件，报错为RequestError: Error: connect ECONNREFUSED 127.0.0.1:443 2022-06-04 13:00:25 [PicGo ERROR] ------Error Stack Begin------ RequestError: Error: connect ECONNREFUSED 127.0.0.1:443 at new RequestError (C:\\Program Files\\PicGo\\resources\\app.asar\\node_modules\\request-promise-core\\lib\\errors.js:14:15) at Request.plumbing.callback (C:\\Program Files\\PicGo\\resources\\app.asar\\node_modules\\request-promise-core\\lib\\plumbing.js:87:29) at Request.RP$callback [as _callback] (C:\\Program Files\\PicGo\\resources\\app.asar\\node_modules\\request-promise-core\\lib\\plumbing.js:46:31) at self.callback (C:\\Program Files\\PicGo\\resources\\app.asar\\node_modules\\request\\request.js:185:22) at Request.emit (events.js:200:13) at Request.onRequestError (C:\\Program Files\\PicGo\\resources\\app.asar\\node_modules\\request\\request.js:877:8) at ClientRequest.emit (events.js:200:13) at TLSSocket.socketErrorListener (_http_client.js:402:9) at TLSSocket.emit (events.js:200:13) at emitErrorNT (internal/streams/destroy.js:91:8) -------Error Stack End------- 在PicGo的github仓库issue搜索该报错，有一模一样的报错，据开发者说是运营商DNS解析https://api.github.com 域名有问题，picgo没法帮你解决网络问题。处理方式：换dns域名解析服务器、换host具体解决方式： 查找 api.github.com 的正确域名（不要直接 ping，建议通过 DNS 查询工具进行查询） 将其添加到对应的 hosts 文件中 在C:\\Windows\\System32\\drivers\\etc路径找到hosts文件，在末尾加入： 140.82.112.6 api.github.com 140.82.114.5 api.github.com //github域名会定期更新，如果不行需要自行查询 刷新本地 dns 缓存：cmd执行命令行ipconfig /flushdns 成功解决。" }, { "title": "Solving TSP Problems Using VNS, SA, GA", "url": "/posts/TSP/", "categories": "Courses", "tags": "VNS, SA, GA, Searching, AI", "date": "2022-05-05 09:36:30 +0800", "snippet": " 用变邻域搜索、模拟退火、遗传算法解决TSP问题 1. 算法原理 1、获取新邻域的算子设计 1）将路径四个区间随机排序 2）反转一段区间 3）交换两个城市 4）随机挑选两个城市插入序列头部 2、变邻域搜索算法（VNS） 3、模拟退火（SA） 4、遗传算法（GA） 2. 关键代码展示 1、读取输入，存储地图 2、计算路径长度 3、VND 4、VNS 5、SA 6、PMX 7、GA 3. 创新点与优化 1、缩短迭代中计算距离的时间 2、控制搜索方向向最优化迭代 用变邻域搜索、模拟退火、遗传算法解决TSP问题完整代码见该项目仓库1. 算法原理模拟退火算法与遗传算法皆为在局部搜索基础上的优化算法。局部搜索（Local Search）是指寻找近似最优解、不断优化局部最优解的启发式算法。其基本思路为，算法从一个或若干个初始解出发，在当前状态的邻域中搜索出若干个候选解，并以某种策略在候选解中确定新的当前解；重复执行上述搜索过程，直至满足算法终止条件，结束搜索过程并输出近似最优结果。1、获取新邻域的算子设计无论是变邻域算法与模拟退火算法的扰动操作、变邻域操作，还是遗传算法的变异操作，本质上都是产生新邻域的随机解，都可采用以下算子。1）将路径四个区间随机排序#扰动产生新的随机解，扰动方式为分成四个区间随机排序def shaking(path): global size ini = visited[path] cnt = 0 while True: pos1,pos2,pos3 = sorted(random.sample(range(0,size),3)) path_ = path[pos1:pos2] + path[:pos1] + path[pos3:] + path[pos2:pos3] if path_ not in visited: cost = getCost(path_) visited.update({path_:cost}) else: cost = visited[path_] cnt+=1 if ini &gt;= cost: break elif cnt &gt; 100: path_ = path cost = ini break return path_2）反转一段区间#反转一段区间，获取新邻域def getNei_rev(path): global size min = visited[path] cnt = 0 while True: i,j = sorted(random.sample(range(1,size-1),2)) path_ = path[:i] + path[i:j+1][::-1] + path[j+1:] if path_ not in visited: cost = getCost(path_) visited.update({path_:cost}) else: cost = visited[path_] cnt+=1 if cost &lt; min: min = cost break elif cnt &gt; 1000: path_ = path break return path_,min3）交换两个城市#交换两个城市，获取新邻域def getNei_exc(path): global size min = visited[path] cnt = 0 while True: i,j = sorted(random.sample(range(1,size-1),2)) path_ = path[:i] + path[j:j+1] + path[i+1:j] + path[i:i+1] + path[j+1:] if path_ not in visited: cost = getCost(path_) visited.update({path_:cost}) else: cost = visited[path_] cnt+=1 if cost &lt; min: min = cost break elif cnt &gt; 1000: path_ = path break return path_,min4）随机挑选两个城市插入序列头部#随机挑选两个城市插入序列头部，获取新邻域def getNei_ins(path): global size min = visited[path] cnt = 0 while True: i,j = sorted(random.sample(range(1,size-1),2)) path_ = path[i:i+1] + path[j:j+1] + path[:i] + path[i+1:j] + path[j+1:] if path_ not in visited: cost = getCost(path_) visited.update({path_:cost}) else: cost = visited[path_] cnt+=1 if cost &lt; min: min = cost break elif cnt &gt; 1000: path_ = path break return path_,min2、变邻域搜索算法（VNS）变邻域搜索算法（Variable Neighborhood Search）是一种改进的局部搜索算法。此处的邻域，是指当前状态的临近状态，通过扰动、变邻域等函数操作，在邻域中产生新的随机解，选择其中的局部优解替代当前解，反复迭代，以此逼近最优解。VNS的算法思路为： 初始化，选择一个可行的初始解； 扰动当前解，获得一个新的解； 使用变邻域（Variable Neighborhood Descent, VND）策略的局部搜索： 用当前解作为初始解 对当前解做变邻域操作，假如得到的解比当前解更优，将变邻域后得到的解作为下一次迭代的当前解 重复第二步，直到迭代次数满足终止条件，返回局部最优解，退出迭代 假如在VND操作中获得的局部最优解较当前解更优，则令其替代当前解，将迭代计数置为0；反之，迭代计数加一。 返回第二步，重复直到迭代次数满足终止条件，返回近似最优解，退出迭代。其伪代码为：VNS：VND策略：3、模拟退火（SA）基于上述的变邻域搜索算法，加入模拟退火策略，即为模拟退火算法。模拟退火算法的原理类似固体的物理退火过程，在进行随机生成解的过程中，接受劣解的概率逐渐下降趋近0，由随机搜索（高温）转变为局部搜索（降温），最终算法找到最优解（达到物理基态）。模拟退火算法的本质是通过温度来控制算法接受劣解的概率。退火系数在0.99，\\(\\frac{1}{lg(k+1)}\\)（经典退火），\\(\\frac{1}{k+1}\\)（快速退火）三者之间选择，其中最后一个系数搜索效率较快。接受劣解的概率公式为：\\(e^{\\frac{f(x)-f(x')}{tk}}\\)SA的算法思路为： 初始化，选择一个可行的初始解，以路径总长度为适应值，长度越短，解越优，越适应环境； 在当前解的邻域中随机选择一个解，若该解优于当前解，替换该解为当前解；反之，计算接受概率，若产生的介于（0，1）之间的随即小数小于该概率，则接受该劣解，否则以当前解继续下一次迭代。 重复第二步，直到搜索在当前温度下达到收敛，进行降温冷却操作，返回第二步； 重复二、三步直到温度降温至满足终止条件，返回近似最优解，退出迭代。其伪代码为：4、遗传算法（GA）遗传算法的基本原理是通过作用于染色体上的基因寻找好的染色体来求解问题，它需要对算法所产生的每个染色体进行评价，并基于适应度值来选择染色体，使适应性好的染色体有更多的繁殖机会，在遗传算法中，通过随机方式产生若干个所求解问题的数字编码，即染色体，形成初始种群；通过适应度函数给每个个体一个数值评价，淘汰低适应度的个体，选择高适应度的个体参加遗传操作，经过遗传操作后的个体集合形成下一代新的种群，对这个新的种群进行下一轮的进化。遗传算法主要分为四个部分：交叉（crossover）、变异（mutation）、评估（fitness）、选择（selection）。交叉算子有：部分映射（Partial-Mapped Crossover）、顺序交叉（OX crossover）、基于位置的交叉（Position-based Crossover ）、基于顺序的交叉（Order-Based Crossover ）、循环交叉（Cycle Crossover）。在本实验中采取部分映射法（PMX）。变异算子有：反转变异（Invertion）、插入变异（Insertion）、替代变异（displacement）、交换变异（swap）、启发式变异（heuristic）。在本实验中测试反转变异与交换变异，结果表明反转变异的效率要优于交换变异。GA的算法思路为： 初始化，选择一个可行的初始解种群，对该种群的适应度进行评估； 对当前解种群进行交叉、变异、评估操作； 在当前解与经过交叉变异得到的子种群中，根据适应度评估值进行选择，得到下一代种群，重复第二步； 重复二、三步直到迭代次数满足终止条件，返回近似最优解，退出迭代。其伪代码为：2. 关键代码展示1、读取输入，存储地图读取城市的横纵坐标后，求出两两间的二维欧几里得距离，用一个全局变量DIST二维数组进行记录，使计算路径长度时无需反复计算两城市间的二维欧几里得距离。#读取城市的x，y坐标def load(txt): f = open(txt) map=[] flag = 0 for line in f: line = line.strip() if line == \"NODE_COORD_SECTION\": flag = 1 continue if line == \"EOF\": break if flag: a = line.split() map.append((float(a[1]),float(a[2]))) return tuple(map)#获取两个城市间的二维欧几里得距离def getDist(): global map,size dist = np.zeros((size,size)) for i in range(0,size): for j in range(0,size): dist[i][j] = ((map[i][0]-map[j][0])**2 + (map[i][1]-map[j][1])**2)**0.5 return dist2、计算路径长度#根据路径获取该路径总代价def getCost(path): cost = 0 former = path[0] for city in path: cost += DIST[former][city] former = city cost += DIST[path[0]][path[-1]] return cost3、VND在局部搜索中使用VND策略进行搜索。#在Local Search中使用VND方法进行搜索 def VND(path): l = 0 min = visited[path] while l &lt; 3: if l == 0: path_,cost = getNei_rev(path) elif l == 1: path_,cost = getNei_exc(path) elif l == 2: path_,cost = getNei_ins(path) if cost &lt; min: path = path_ min = cost l = 0 else: l+=1 return path,min 4、VNS#进行变邻域局部搜素def VNS(path,kmax): k = 0 temp = path min = solutions[0] global count while k &lt; kmax: #扰动后进行变邻域操作 path_nei,cost = VND(shaking(temp)) print(cost) solutions.append(cost) count+=1 if cost &lt; min: temp = path_nei #记录迭代过的最优的解 min = cost k = 0 else: k+=1 return temp,min5、SA#模拟退火算法def SA(path,kmax,t0,t_end): temp = path min = solutions[0] result = [temp,min] #记录迭代过的最优的解 global count t = t0 #初始温度 while t &gt; t_end: for k in range(1,kmax): path_nei,cost = VND(temp) #进行变邻域操作 #print(cost) solutions.append(cost) count+=1 #判断是否接受该解 if cost &lt; min or random.random() &lt; np.exp(-((cost-min)/t*k)): temp = path_nei min = cost if cost &lt; result[1]: result = [path_nei,cost] #t/=math.log10(1+k) t/=k+1 #降温操作 return result[0],result[1]6、PMX#Partial-Mapped crossoverdef PMX(i,j): global size s,t = sorted(random.sample(range(1,size),2)) next_i = list(i[:s] + j[s:t] + i[t:]) next_j = list(j[:s] + i[s:t] + j[t:]) #建立映射表 mapped_i = {next_i[k]:next_j[k] for k in range(s,t)} mapped_j = {next_j[k]:next_i[k] for k in range(s,t)} #判断是否满足解的条件（每个城市皆访问一次） while len(set(next_i)) != len(next_i): for k in range(size): if k &lt; t and k &gt;= s: continue while next_i[k] in j[s:t]: next_i[k] = mapped_i[next_i[k]] while len(set(next_j)) != len(next_j): for k in range(size): if k &lt; t and k &gt;= s: continue while next_j[k] in i[s:t]: next_j[k] = mapped_j[next_j[k]] next_i = tuple(next_i) next_j = tuple(next_j) if next_i not in visited: visited.update({next_i:getCost(next_i)}) if next_j not in visited: visited.update({next_j:getCost(next_j)}) return next_i,next_j7、GA#遗传算法def GA(paths,kmax): global M,solutions temp = paths for k in range(kmax): count = 0 flag = 0 children = [] #存储此代交叉、变异产生的子种群 #加入当前种群中的最优解，使得下一代种群的最优解一定不会劣于当前种群最优解 children.append(temp[0]) for l in range(M): while True: i,j = random.sample(range(M),2) count+=1 if temp[i] != temp[j]: break if count &gt; 1000000: flag = 1 break if flag == 0: a,b = PMX(temp[i],temp[j]) #使用PMX交叉操作 children.append(a) children.append(b) for l in range(M): i = random.randrange(M) children.append(reverse(temp[i])) #使用反转一段区间作为变异操作 temp = sorted(children[:], key=lambda x:visited[x])[:M] #选取子代中最优的前M个解 solutions.append(visited[temp[0]]) #记录此次迭代产生的下一代的最优解 print(k,visited[temp[0]]) return temp[0]3. 创新点与优化1、缩短迭代中计算距离的时间1）用一个全局变量DIST二维数组记录所有城市两两间的距离，使计算路径长度时无需反复计算两城市间的二维欧几里得距离。2）用一个全局变量visited字典记录已经搜索过的路径，以元组储存路径作为键值，储存其路径长度。在搜索过程中，可能多次搜索到同一条路径，可以在visited字典中直接获取路径，无需反复计算。2、控制搜索方向向最优化迭代1）在扰动、变邻域、变异操作中，通过反复随机生成路径，选择候选解中优于当前解的路径；若多次随机后当前路径仍然为最优，则返回此次已搜索路径中的最优解。通过控制最大随机生成路径的次数，可以有效控制搜索方向向最优化迭代，效率提高。2）在遗传算法中，每一次迭代，将当代种群的最优路径复制到子代种群，在经过交叉、变异操作后，使用精英（elitist）策略，再在子代种群中选择最优的前M（种群大小）个解，以此保证下一代种群的最优解一定不会劣于当前种群最优解，搜索方向向最优化迭代。" }, { "title": "Solving 15-Puzzle with A* and IDA*", "url": "/posts/astar/", "categories": "Courses", "tags": "A*, IDA*, Searching, AI", "date": "2022-03-17 09:36:30 +0800", "snippet": " 用A*和IDA*解决15-Puzzle问题 1. 算法原理 1、启发式函数的设计 2、算法描述 1）A* 2）IDA* 2. 关键代码展示 1、曼哈顿函数 2、判断棋盘是否有解 3、棋盘类 4、A* 5、IDA* DFS： IDA*： 3. 创新点与优化 1、为启发式函数添加附加值（创新） 1）曼哈顿函数的返回值精度 2）改变曼哈顿函数的系数 3）将曼哈顿距离作为仅次于估价得分的第二权重 4）添加初始-目标向量和当前-目标向量的向量叉积作为附加值 2、优化空间开销 3、环检测与剪枝操作 1）对无解棋盘进行剪枝 2）用字典维持环检测，存储棋盘与最小估价得分 用A*和IDA*解决15-Puzzle问题完整代码见该项目仓库1. 算法原理与无信息搜索相对的是启发式搜索，它利用启发信息构造估价函数，来减少搜索范围，优化搜索算法。常见的无信息搜索有BFS和DFS算法，分别基于BFS和DFS，启发式搜索有A* 和 IDA*算法。估价函数：f(x)=h(x)+g(x)其中，g(x)是从初始结点到结点x已付出的实际代价；h(x)是从结点x到目标结点的最优路径估计代价。通过更改两者的系数占比，或添加附加值，能对搜索结点做进一步筛选，以改变搜索效率。1、启发式函数的设计A* 和 IDA*算法的估价函数都是：f(x)=h(x)+g(x)对于h(x)，有以下几种情况：设h*(x)为从x移动到目标的实际代价，则 h(n)=0 只有g(n)起作用，A*演变成Dijkstra算法，保证找到最短路径 h(n)&lt;=h*(n) *保证找到最短路径，且h(n)越接近h(n)，算法的搜索范围越精准，运行越快 ** h(n)&gt;h*(n) A*不能保证找到一条最短路径，但运行更快 h(n)»g(n) 只有h(n)起作用，A*演变成BFS算法 在15-Puzzle问题中，在正方形网格内，空白格只能向4邻域移动，故采用曼哈顿函数作为启发式函数。将每一步棋盘状态储存为结点，则此处的h(x)为当前结点棋盘状态每个数字达到目标位置所需要的横、纵轴平移量之和。注意，此时不能将空白格（0）距离目标位置的距离计入，否则将导致h(n)&gt;h*(n)，失去可采纳性，无法得到最短路径。2、算法描述1）A*A*是在BFS的基础上，添加启发信息的启发式搜索算法。首先，定义作为被搜索结点的抽象类。创建一个棋盘类，其主要成员有：棋盘数组（为压缩存储空间，用一维数组按行优先顺序存储数字）、从初始状态到当前状态的路径（为方便估价函数的计算，还可能含有当前路径长度、目标棋盘数组）、得到邻居结点的函数，计算曼哈顿距离和估价得分的函数。与BFS算法类似，设置一个优先队列作为边界队列，记录访问结点的相邻结点，以估价函数的返回值为权重，权重越小、估计路径越短，优先级越高。设置一个set类型的并查集或者字典的关闭集合，来记录已关闭的结点。将队列中的结点依次出队，出队时将该结点的相邻按照权重大小的优先级入队，再将该结点加入关闭集合。由于A*算法是在广度优先搜索的基础上，每次择优先级最高的结点进行扩展，因此第一次扩展某点时一定是最短路径。A*的算法思路为：1、开始时，起点入队。2、取队头并出队，将队头的相邻结点中不在关闭集合内的结点依次入优先级队列，将队头标记为已关闭，加入关闭集合。3、重复第二步直到搜索到终点时结束循环，得到最优解和路径；或队列为空，代表无路径。由于边界队列是优先级队列，并且保证不将已关闭的结点再次加入边界队列，因此当新的、更优的相邻结点遇到棋盘状态相同的旧结点已经在边界队列中时，不需要搜索到它并更新其g(x)值，只需直接入队，它会先于旧结点出队、被关闭。2）IDA*IDA*是在DFS的基础上，添加启发信息的启发式搜索算法。在DFS中，假如我们不给予一个最大深度阈值，算法可能无止境地向着一个方向做无效的搜索。而IDA* 便是基于这点，在算法迭代的每一步深度优先搜索中，当某一步所有可访问结点对应的最小可估价函数值大于某个给定的阈值时，将会剪枝。IDA* 的算法思路如下：1、开始时，计算起点所有邻居结点的估价函数，选取估价函数最小的结点作为下一个访问结点。2、重复步骤一，在递归过程中，若当前结点的估价函数大于阈值bound，则返回当前结点的估价得分。3、若当前结点是目标结点，则得到最优解，返回路径。2. 关键代码展示1、曼哈顿函数A* 与 IDA*都使用曼哈顿函数作为启发式函数，其算法大致如下，返回值根据具体计算需要有不同情况。#计算曼哈顿距离def manhatten(map): dx = 0 dy = 0 for i in range(16): if map[i] != 0: #跳过空白格 j = map[i]-1 #数字的目标下标比数字本身小1 dy += abs(i-j)/4 dx += abs((i % 4) - (j % 4)) return dx,dy #return dx+dy2、判断棋盘是否有解N是奇数时，当且仅当当前棋盘的逆序对是偶数的时候有解。N是偶数时，当且仅当当前棋盘的逆序对数加上空格所在的行（从0开始算）是奇数的时候有解。在15-Puzzle问题中，N=4#通过逆序性判断有无解def solvable(map): cnt = 0 flag = 0 for i in range(16): if map[i] == 0: flag = int(i/4) #向下取整即为行数 continue for j in range(i,16): if map[j] == 0: continue if map[i] &gt; map[j]: #说明是逆序数对 cnt+=1 return (cnt + flag) % 2 != 0 3、棋盘类#设置棋盘结点class board(): def __init__(self,map_=(),path_='',cnt_= 0): #用元组存储棋盘数组，字符串存储路径 self.map = map_ self.path = path_ self.cnt = cnt_ self.dx,self.dy = manhatten(map_) #调用曼哈顿函数 self.score = (self.dx+self.dy)*1.0143+self.cnt #得到邻居节点 def getNeighbour(self): for i in range(16): if self.map[i] == 0: break neighbour = [] #空白格的移动方向 move = { 'left': -1, 'down': 4, 'up': -4, 'right': 1 } #查找四个方向的相邻结点 for direc in move: if i + move[direc] &lt; 16 and i + move[direc] &gt;= 0: if i + move[direc] &gt; i: x,y = i,i+move[direc] else: x,y = i+move[direc],i #用元组切片拼接来实现两个数字的位置交换 map_ = self.map[:x]+self.map[y:y+1]+self.map[x+1:y]+self.map[x:x+1]+self.map[y+1:] path_ = self.path #在路径中添加当前被替换的数字 path_ += str(map_[i]) + ' ' if solvable(map_): #假如有解才记录该相邻结点，剪枝 neighbour.append(board(map_,path_,self.cnt+1)) #在返回邻居列表时就对其权重进行排序，首要是估价得分，其次是h(x)的大小，为了避免调用曼哈顿函数耗时，采用当前实际路径长度的相反数，效果相同 return sorted(neighbour,key=lambda x:[x.score,-x.cnt])4、A*def Astar(map): frontier = queue.PriorityQueue() closed = dict() #用字典作为关闭集合，存储对应结点的估价得分 #判断有无解 if solvable(map) is False: return \"No Solution\" else: #起点入队 frontier.put((0,0,0,board(map))) j = 0 #为比较算法效率，设置一个计数值，计算搜索过的结点 while not frontier.empty(): top = frontier.get() #取队头 node = top[3] #棋盘结点 nsco = top[0] #该结点的估价得分 print(j) #假如当前结点在关闭集合中，并且估价得分大于集合中旧结点的估价得分，则进行剪枝 if node.map in closed and nsco &gt; closed[node.map]: continue #反之，在字典中添加或更新该结点与当前估价得分的键值对 closed.update({node.map: nsco}) \t#若当前结点即为目标结点，返回路径 if node.map == goal: return node.path + \"\\nA optimal solution \" + str(node.cnt) + \" moves\\n\" +\"Searched for \" + str(j) + \" times!\" nei = node.getNeighbour() #取邻居结点 for n in nei: j+=1 if n.map not in closed: #假如当前邻居节点不在关闭集合中，入队 frontier.put((n.score,-n.cnt,j,n)) #优先队列以元组内先后次序为优先次序比较，将计数值j作为第三项权重，防止有前两项权重都相同，无法比较的情况 return \"Error!\" #既非无解，又直到边界队列为空都未搜索到解，返回出错5、IDA*IDA* 算法由两部分构成，一是DFS递归函数，另一部分是IDA*在DFS基础上的启发式搜索函数。DFS：#深度遍历搜索count = 0 #为比较算法效率，设置一个计数值，计算搜索过的结点def dfs(path,g,bound,visited): #判断有无解 node = list(path.keys())[-1] #路径是有序字典，取倒数第一个结点进行扩展 f = g + manhatten(node) #计算估价函数 if f &gt; bound: #假如估价得分大于当前设定的最大深度，退出 return f if node == goal: #找到目标解，返回 return -1 min = 9999 global count nei = getNeighbour(node) #取依照估价得分排序的邻居结点，列表中存储的是（邻居结点棋盘状态元组，当前替换的值） for n in nei: #假如对于全局都未访问过当前棋盘状态，或旧结点的实际路径大于当前实际路径 if n[0] not in visited or visited[n[0]] &gt;= g: visited.update({n[0]:g}) #则在已访问字典中添加或更新当前棋盘状态和对应的实际路径的键值对，反之剪枝 #假如当前路径未访问过当前棋盘状态 if n[0] not in path: path.update({n[0]:n[1]}) #更新路径 t = dfs(path,g+1,bound,visited) #继续深度遍历 count += 1 print(count,t,sep=' ') if t == -1: return -1 if t &lt; min: min = t path.pop(n[0]) #删除当前相邻结点，准备遍历下一相邻结点 return min #返回相邻结点中超过当前bound值的估价得分中的最小值IDA*：def IDAstar(map): bound = manhatten(map) #以曼哈顿函数的值作为初始的bound值 path = collections.OrderedDict({map:0}) #用有序字典存储路径 visited = {} #用于记录全局的已搜索结点及其最小实际代价 while(True): t = dfs(path, 0, bound, visited) if t == -1: #找到最优解，返回 return (path, bound) if t &gt; 70: #超过最大深度，退出 return ([], bound) bound += 2 #每次递归，实际代价g+1，曼哈顿距离h也随之改变±1，因此0&lt;=Δf(x)&lt;=2，最大深度至多增加23. 创新点与优化1、为启发式函数添加附加值（创新）在搜索过程中，当边界队列中有估价得分相同的结点时，我们无法判断它们的优先级，冗余的、无效的搜索会导致性能的下降。并且，在大小4X4、相邻方向固定上下左右的棋盘中，估价得分相同的情况相当普遍，对搜索效率有较大影响。为解决该问题，我们尝试为启发式函数添加附加值。1）曼哈顿函数的返回值精度首先，按照曼哈顿函数的定义，其结点之间横纵轴移动应以1为步长，即其曼哈顿距离应是整型数据。在PYTHON中，若没有对变量做int()强制类型转换，则默认为浮点数。经过测试，我发现返回浮点数能减少估价得分相等的概率，相比用整型数据累加，搜索效率要更高。2）改变曼哈顿函数的系数如果我们减少曼哈顿函数的系数，那么当搜索范围朝着目标移动的时候f将逐渐增加，而这意味着算法倾向于扩展靠近起点的结点，而不是靠近目标的结点。因此，增加曼哈顿函数的系数，能使算法倾向于扩展靠近目标的结点。给该函数添加一个选择因子p来增加搜索效率，使h(x)*=(1.0+p).其中，$p&lt;\\frac{最小步长}{期望的最长路径长度}$在15-Puzzle问题的求解中，根据测例，将最优解的最大可能路径设定为70步，令$p = \\frac{1}{70}≈0.0143$.添加这个附加值的结果是，搜索的结点更少了。3）将曼哈顿距离作为仅次于估价得分的第二权重当估价得分f相等时，曼哈顿距离h越大，实际代价g越小，因此，要使曼哈顿距离尽可能小，避免重复调用曼哈顿函数耗时，采用当前实际代价g的相反数-g作为第二权重，作为优先级的比较信息之一。4）添加初始-目标向量和当前-目标向量的向量叉积作为附加值该附加值的添加使得搜索方向倾向于尽量沿起点到目标点的直线连线，提高搜索效率。dx1 = current.x - goal.xdy1 = current.y - goal.ydx2 = start.x - goal.xdy2 = start.y - goal.ycross = abs(dx1*dy2 - dx2*dy1)heuristic += cross*k #系数由具体情况决定此处设k = 0.0143，搜索的结点数有所减少。2、优化空间开销由于A*需要维护边界队列和关闭集合，其空间复杂度较高，棋盘情况较复杂时容易出现内存不足、程序跑满磁盘、不得不强行退出的状况。因此，对存储空间进行优化是相当有必要的。通过用一维元组存储棋盘数组，以及用字符串存储路径，能在一定程度上提高存储效率。相较用列表存储棋盘数组，用元组的优点有两点：一是占据空间更小，二是元组可以直接作为哈希值进行比较与匹配，在判断与目标棋盘数组是否相等时，不必将列表数组转换为字符串或通过循环比较是否达到目标状态，同时，更方便作为字典的索引，维护关闭集合。最开始，为求路径清晰，我在路径列表中保存了每个结点的棋盘状态，而这造成了巨大的内存开销。改为用字符串存储路径，每次添加当前替换的数字后，存储效率大大提高。3、环检测与剪枝操作1）对无解棋盘进行剪枝在求邻居结点时，判断邻居结点是否有解，无解则剪枝，有解则被添加入邻居列表。经过实验检测，这能减少搜索结点。2）用字典维持环检测，存储棋盘与最小估价得分用字典按键索引，更便于更新最小估价得分或判断是否扩展过当前结点。在A*算法中，用字典closed维持关闭集合，以棋盘元组和最小估价得分作为键值对。从边界队列中取队头准备进行扩展时，若当前棋盘在关闭集合中，且当前估价得分大于集合中旧结点的估价得分，则进行剪枝；反之，在字典中添加或更新该结点与当前估价得分的键值对，并进行扩展、获取邻居结点。在遍历邻居结点入队时，同样检测该邻居结点的棋盘是否在关闭集合中，是则剪枝；反之，入边界队列。在IDA*算法中，每结点皆用有序字典path记录当前路径，以当前棋盘元组和替换数字为键值对；用一个无序字典visited记录全局状态的结点访问状态，以棋盘元组和最小实际代价g（达到该状态的最小深度）作为键值对。在遍历邻居结点时，若该结点不在visited字典中，或它在visited字典中的实际路径g’≥当前实际代价g，则在visited字典中添加或更新当前结点，反之剪枝。在搜索到目标状态时，h=0，这意味着f=g，而g必然是最小值，这能保证得到最优解。在添加邻居结点至路径中时，若已在当前路径中则剪枝，这保证不会出现结点间的死循环。" } ]
