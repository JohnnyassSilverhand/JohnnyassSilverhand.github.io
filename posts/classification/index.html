<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="prefer-datetime-locale" content="en"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Machine Learning: Classification" /><meta property="og:locale" content="en" /><meta name="description" content="基于CIFAR-10图像分类任务训练线性分类器、MLP和CNN模型 基于CIFAR-10图像分类任务训练线性分类器、MLP和CNN模型" /><meta property="og:description" content="基于CIFAR-10图像分类任务训练线性分类器、MLP和CNN模型 基于CIFAR-10图像分类任务训练线性分类器、MLP和CNN模型" /><link rel="canonical" href="https://witchpuff.github.io/posts/classification/" /><meta property="og:url" content="https://witchpuff.github.io/posts/classification/" /><meta property="og:site_name" content="CyberWitch" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-11-08T09:36:30+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Machine Learning: Classification" /><meta name="twitter:site" content="@twitter_username" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-03-29T22:27:46+08:00","datePublished":"2022-11-08T09:36:30+08:00","description":"基于CIFAR-10图像分类任务训练线性分类器、MLP和CNN模型 基于CIFAR-10图像分类任务训练线性分类器、MLP和CNN模型","headline":"Machine Learning: Classification","mainEntityOfPage":{"@type":"WebPage","@id":"https://witchpuff.github.io/posts/classification/"},"url":"https://witchpuff.github.io/posts/classification/"}</script><title>Machine Learning: Classification | CyberWitch</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="CyberWitch"><meta name="application-name" content="CyberWitch"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img class="avatar" alt="WitchPuff" width="100" height="100" data-proofer-ignore="true" src="https://avatars2.githubusercontent.com/WitchPuff?v=3&s=100" srcset="https://avatars2.githubusercontent.com/WitchPuff?v=3&s=100 1x, https://avatars2.githubusercontent.com/WitchPuff?v=3&s=200 2x, https://avatars2.githubusercontent.com/WitchPuff?v=3&s=300 3x, https://avatars2.githubusercontent.com/WitchPuff?v=3&s=400 4x" /> </a></div><div class="site-title mt-3"> <a href="/">CyberWitch</a></div><div class="site-subtitle font-italic">A Room of One's Own</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/WitchPuff" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['ttqsjzsq','163.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Machine Learning: Classification</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>Machine Learning: Classification</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1667871390" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Nov 8, 2022 </em> </span> <span> Updated <em class="" data-ts="1680100066" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Mar 29, 2023 </em> </span><div class="d-flex justify-content-between"> <span> By <em> WitchPuff </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="4875 words"> <em>27 min</em> read</span></div></div></div><div class="post-content"><ul id="markdown-toc"><li><a href="#基于cifar-10图像分类任务训练线性分类器mlp和cnn模型" id="markdown-toc-基于cifar-10图像分类任务训练线性分类器mlp和cnn模型">基于CIFAR-10图像分类任务训练线性分类器、MLP和CNN模型</a><ul><li><a href="#一模型原理" id="markdown-toc-一模型原理">一、模型原理</a><ul><li><a href="#1softmax分类器" id="markdown-toc-1softmax分类器">1）Softmax分类器</a><li><a href="#2mlp" id="markdown-toc-2mlp">2）MLP</a><li><a href="#3cnn" id="markdown-toc-3cnn">3）CNN</a><li><a href="#4网络参数" id="markdown-toc-4网络参数">4）网络参数</a><ul><li><a href="#1cuda加速" id="markdown-toc-1cuda加速">1、CUDA加速</a><li><a href="#2损失函数" id="markdown-toc-2损失函数">2、损失函数</a><li><a href="#3优化器" id="markdown-toc-3优化器">3、优化器</a><ul><li><a href="#sgdstochastic-gradient-descent" id="markdown-toc-sgdstochastic-gradient-descent">SGD（Stochastic Gradient Descent）</a><li><a href="#sgd-momentum" id="markdown-toc-sgd-momentum">SGD Momentum</a><li><a href="#adam" id="markdown-toc-adam">Adam</a></ul></ul></ul><li><a href="#二代码框架" id="markdown-toc-二代码框架">二、代码框架</a><ul><li><a href="#1数据读取及预处理" id="markdown-toc-1数据读取及预处理">1）数据读取及预处理</a><li><a href="#2单次训练" id="markdown-toc-2单次训练">2）单次训练</a><li><a href="#3评估正确率" id="markdown-toc-3评估正确率">3）评估正确率</a><li><a href="#4main" id="markdown-toc-4main">4）main</a></ul><li><a href="#三实验分析" id="markdown-toc-三实验分析">三、实验分析</a><ul><li><a href="#1softmax线性模型" id="markdown-toc-1softmax线性模型">1）Softmax线性模型</a><li><a href="#2mlp模型" id="markdown-toc-2mlp模型">2）MLP模型</a><li><a href="#3cnn模型" id="markdown-toc-3cnn模型">3）CNN模型</a><ul><li><a href="#1对优化算法及其参数的实验" id="markdown-toc-1对优化算法及其参数的实验">1、对优化算法及其参数的实验</a><li><a href="#2对网络结构的实验" id="markdown-toc-2对网络结构的实验">2、对网络结构的实验</a></ul><li><a href="#4对比三个模型在cifar-10图像分类任务上的性能" id="markdown-toc-4对比三个模型在cifar-10图像分类任务上的性能">4）对比三个模型在CIFAR-10图像分类任务上的性能</a></ul></ul></ul><h1 id="基于cifar-10图像分类任务训练线性分类器mlp和cnn模型">基于CIFAR-10图像分类任务训练线性分类器、MLP和CNN模型</h1><h2 id="一模型原理"><span class="mr-2">一、模型原理</span><a href="#一模型原理" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="1softmax分类器"><span class="mr-2">1）Softmax分类器</span><a href="#1softmax分类器" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Softmax分类器是一个单层线性神经网络，即只有一个输入层、一个输出层，再经过Softmax函数激活层，得到标签的预测概率。Softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持可导的性质。</p><p>实现一个Softmax线性分类模型：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">Softmax</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inNum</span><span class="p">,</span> <span class="n">outNum</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Softmax</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">inNum</span><span class="p">,</span><span class="n">outNum</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">out</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y</span> 
</pre></table></code></div></div><h3 id="2mlp"><span class="mr-2">2）MLP</span><a href="#2mlp" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>通过在网络中加入一个或多个隐藏层，可以克服线性模型的限制， 使其能处理更普遍的、非单调的函数关系类型。将许多全连接层堆叠在一起，每一层都输出到上面的层，直到生成最后的输出。 我们可以把前L−1层看作表示，把最后一层看作线性预测器。 这种架构通常称为多层感知机（multilayer perceptron），通常缩写为<em>MLP</em>。</p><p><img data-src="https://raw.githubusercontent.com/WitchPuff/typora_images/main/img/202303291842106.svg%252Bxml" alt="../_images/mlp.svg" data-proofer-ignore></p><p>实现一个MLP模型：</p><p>该神经网络结构由一个输入层、三个隐藏层及一个输出层构成，在层与层之间使用ReLU激活层，最后用Softmax计算标签的预测概率。</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span> 
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inNum</span><span class="p">,</span> <span class="n">outNum</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span><span class="n">hid2</span><span class="p">,</span><span class="n">hid3</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">hid</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">inNum</span><span class="p">,</span><span class="n">hidden</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">hid2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span><span class="n">hid2</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">hid3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hid2</span><span class="p">,</span><span class="n">hid3</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hid3</span><span class="p">,</span><span class="n">outNum</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">hid</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span> <span class="c1">#input(3,32,32) output(1024)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">hid2</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span><span class="c1">#output(256)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">hid3</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span><span class="c1">#output(84)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">out</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span><span class="c1">#output(10)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></table></code></div></div><h3 id="3cnn"><span class="mr-2">3）CNN</span><a href="#3cnn" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><em>卷积神经网络</em>（convolutional neural network，CNN）是一类强大的、为处理图像数据而设计的神经网络。使用前述的模型时，将图像数据展平成一维向量而忽略了每个图像的空间结构信息，卷积神经网络则能弥补这个缺漏。</p><p>LeNet（LeNet-5）由两个部分组成：</p><ul><li>卷积编码器：由两个卷积层组成;<li>全连接层密集块：由三个全连接层组成。</ul><p><img data-src="https://raw.githubusercontent.com/WitchPuff/typora_images/main/img/202303291842123.svg%252Bxml" alt="../_images/lenet.svg" data-proofer-ignore></p><p><img data-src="https://raw.githubusercontent.com/WitchPuff/typora_images/main/img/202211072147111.png" alt="image-20221107214725042" style="zoom: 67%;" data-proofer-ignore></p><p>实现一个LeNet模型：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">LeNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LeNet</span><span class="p">,</span><span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">16</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">pool1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">pool2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="o">*</span><span class="mi">5</span><span class="o">*</span><span class="mi">5</span><span class="p">,</span><span class="mi">120</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span><span class="mi">84</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
        

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="c1">#input(3,32,32) output(16,28,28)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pool1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#output(16，14，14)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="c1">#output(32,10,10)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pool2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#output(32,5,5)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">32</span><span class="o">*</span><span class="mi">5</span><span class="o">*</span><span class="mi">5</span><span class="p">)</span> <span class="c1">#output(5*5*32)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="c1">#output(120)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="c1">#output(84)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c1"># output(10)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></table></code></div></div><h3 id="4网络参数"><span class="mr-2">4）网络参数</span><a href="#4网络参数" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><h4 id="1cuda加速"><span class="mr-2">1、CUDA加速</span><a href="#1cuda加速" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>利用GPU进行计算、让CPU读取数据，可以大幅减少训练的耗时，为此需要将数据与网络迁移到GPU上进行大规模计算。</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="c1"># 查看gpu信息
</span><span class="n">cudaMsg</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span>
<span class="n">gpuCount</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">device_count</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s">"1.是否存在GPU:{}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">cudaMsg</span><span class="p">),</span> <span class="s">"如果存在有：{}个"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">gpuCount</span><span class="p">))</span>

<span class="c1"># 将数据/网络移到GPU上
</span><span class="n">net</span> <span class="o">=</span> <span class="n">net</span><span class="p">.</span><span class="n">cuda</span><span class="p">()</span>

<span class="c1"># 命令行
</span><span class="err">$</span> <span class="n">nvidia</span><span class="o">-</span><span class="n">smi</span> <span class="c1">#可以查看当前GPU适配的CUDA版本及显卡占用率
</span><span class="n">NVIDIA</span><span class="o">-</span><span class="n">SMI</span> <span class="mf">522.25</span>       <span class="n">Driver</span> <span class="n">Version</span><span class="p">:</span> <span class="mf">522.25</span>       <span class="n">CUDA</span> <span class="n">Version</span><span class="p">:</span> <span class="mf">11.8</span>


<span class="err">$</span> <span class="n">nvcc</span> <span class="o">-</span><span class="n">V</span> <span class="c1">#可以确认CUDA是否已安装成功
</span>
<span class="n">nvcc</span><span class="p">:</span> <span class="n">NVIDIA</span> <span class="p">(</span><span class="n">R</span><span class="p">)</span> <span class="n">Cuda</span> <span class="n">compiler</span> <span class="n">driver</span>
<span class="n">Copyright</span> <span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="mi">2005</span><span class="o">-</span><span class="mi">2022</span> <span class="n">NVIDIA</span> <span class="n">Corporation</span>
<span class="n">Built</span> <span class="n">on</span> <span class="n">Wed_Sep_21_10</span><span class="p">:</span><span class="mi">41</span><span class="p">:</span><span class="mi">10</span><span class="n">_Pacific_Daylight_Time_2022</span>
<span class="n">Cuda</span> <span class="n">compilation</span> <span class="n">tools</span><span class="p">,</span> <span class="n">release</span> <span class="mf">11.8</span><span class="p">,</span> <span class="n">V11</span><span class="p">.</span><span class="mf">8.89</span>
<span class="n">Build</span> <span class="n">cuda_11</span><span class="p">.</span><span class="mf">8.</span><span class="n">r11</span><span class="p">.</span><span class="mi">8</span><span class="o">/</span><span class="n">compiler</span><span class="p">.</span><span class="mi">31833905_0</span>
</pre></table></code></div></div><p>事实上对于该模型，BatchSize=500时，N卡占用率顶多70%，多数时间有大量空余，真正占用时间的是迭代数据时，CPU对数据集图像的读取及预处理（即transform操作），这是由于CPU对Tensor的处理很慢，而torchvision库没有将数据集迁移到GPU进行预处理计算的API，若要解决这个问题，只能使用DALI或其余库接口加速预处理与数据读取，或者将预处理后的数据集进行保存。</p><h4 id="2损失函数"><span class="mr-2">2、损失函数</span><a href="#2损失函数" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>由于是预测标签概率类型的网络，在此次实验中都采用交叉熵函数。</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="c1"># 损失函数
</span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
</pre></table></code></div></div><h4 id="3优化器"><span class="mr-2">3、优化器</span><a href="#3优化器" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># SGD / SGD Momentum
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span><span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span> 
<span class="c1"># Adam
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span><span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span> 

<span class="c1"># 控制学习率指数衰减
</span><span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">lr_scheduler</span><span class="p">.</span><span class="n">ExponentialLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.98</span><span class="p">)</span>
<span class="c1"># 控制学习率按固定步长衰减
</span><span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">lr_scheduler</span><span class="p">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.65</span><span class="p">)</span>
</pre></table></code></div></div><h5 id="sgdstochastic-gradient-descent"><span class="mr-2">SGD（Stochastic Gradient Descent）</span><a href="#sgdstochastic-gradient-descent" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>随机梯度下降算法即是在给定数据集中，每次随机选择一则数据，根据该数据的训练结果计算损失梯度，更新参数。</p><p><img data-src="https://raw.githubusercontent.com/WitchPuff/typora_images/main/img/202303291842937.png" alt="image-20221107234841276" style="zoom:80%;" data-proofer-ignore></p><h5 id="sgd-momentum"><span class="mr-2">SGD Momentum</span><a href="#sgd-momentum" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>在随机梯度的学习算法中，每一步的步幅都是固定的，而在动量学习算法中，每一步走多远不仅依赖于本次的梯度的大小，还取决于过去的速度。速度v是累积各轮训练参数的梯度，速度越大，依赖以前的梯度越大。物理学中，用变量v表示速度，表明参数在参数空间移动的方向即速率，而代价函数的负梯度表示参数在参数空间移动的力，根据牛顿定律，动量等于质量乘以速度，而在动量学习算法中，我们假设质量的单位为1，因此速度v就可以直接当做动量了，我们同时引入超参数$\beta$，其取值在$[0,1]$范围之间，用于调节先前梯度（力）的衰减效果。 <img data-src="https://raw.githubusercontent.com/WitchPuff/typora_images/main/img/202303291842260.png" alt="image-20221108000030054" style="zoom:80%;" data-proofer-ignore></p><h5 id="adam"><span class="mr-2">Adam</span><a href="#adam" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>自适应动量优化算法结合了RMSProp和动量学习法的优势。</p><p><img data-src="https://raw.githubusercontent.com/WitchPuff/typora_images/main/img/202303291842297.png" alt="image-20221108000917989" style="zoom: 80%;" data-proofer-ignore></p><h2 id="二代码框架"><span class="mr-2">二、代码框架</span><a href="#二代码框架" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="1数据读取及预处理"><span class="mr-2">1）数据读取及预处理</span><a href="#1数据读取及预处理" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>直接调用torchvision.datasets中的CIFAR10数据集，对图像进行随机翻转、随机灰度调正、转换为Tensor张量、正则化等预处理操作，返回torch.utils.data.DataLoader作为迭代器。</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="n">transforms</span>


<span class="n">batch</span> <span class="o">=</span> <span class="mi">500</span> <span class="c1"># batch_size
</span>
<span class="k">def</span> <span class="nf">load_data</span><span class="p">():</span> <span class="c1"># 读取数据，返回迭代器
</span>    <span class="n">mean</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.4915</span><span class="p">,</span> <span class="mf">0.4823</span><span class="p">,</span> <span class="mf">0.4468</span><span class="p">])</span>   
    <span class="n">std</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.2470</span><span class="p">,</span> <span class="mf">0.2435</span><span class="p">,</span> <span class="mf">0.2616</span><span class="p">])</span>
    <span class="c1"># 图像预处理操作
</span>    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">transforms</span><span class="p">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(),</span>
            <span class="n">transforms</span><span class="p">.</span><span class="n">RandomGrayscale</span><span class="p">(),</span>
            <span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">(),</span>
            <span class="n">transforms</span><span class="p">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span><span class="n">std</span><span class="p">)])</span>

    <span class="n">transform1</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">(),</span>
            <span class="n">transforms</span><span class="p">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span><span class="n">std</span><span class="p">)])</span>

    
    <span class="n">trainset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                                <span class="n">download</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
    <span class="n">trainloader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">trainset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch</span><span class="p">,</span>
                                                <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="c1">#,num_workers=1,pin_memory=True)
</span>
    <span class="n">testset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                                                <span class="n">download</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform1</span><span class="p">)</span>
    <span class="n">testloader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">testset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch</span><span class="p">,</span>
                                                <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="c1">#,num_workers=1,pin_memory=True)
</span>    
    <span class="k">return</span> <span class="n">trainloader</span><span class="p">,</span><span class="n">testloader</span>
</pre></table></code></div></div><h3 id="2单次训练"><span class="mr-2">2）单次训练</span><a href="#2单次训练" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>实现一个epoch内的训练函数：</p><ol><li>向前传播得到预测标签<li>根据预测值计算损失值<li>根据损失值进行反向传播，得到各参数的梯度<li>根据梯度下降更新参数<li>返回损失值</ol><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">SingleTrain</span><span class="p">(</span><span class="n">net</span><span class="p">,</span><span class="n">train_iter</span><span class="p">,</span><span class="n">loss</span><span class="p">,</span><span class="n">optim</span><span class="p">):</span>
    <span class="s">"""train for one epoch

    Args:
        net (nn.module): training model
        train_iter (dataloader): iterator of training set
        loss (): loss function of the model
        optim (): optimizer of the model

    Returns:
        float: loss value
    """</span>
        <span class="n">net</span><span class="p">.</span><span class="n">train</span><span class="p">()</span> <span class="c1"># 开启训练模式
</span>        <span class="c1"># 将计算累计loss值的变量定义在GPU上，无需在计算时在CPU与GPU之间移动，耗费时间
</span>        <span class="n">los</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="n">cuda</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_iter</span><span class="p">,</span><span class="mi">0</span><span class="p">):</span>
            <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">data</span>
            <span class="c1"># 将数据迁移到GPU
</span>            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">cuda</span><span class="p">()</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">cuda</span><span class="p">()</span>
            <span class="c1"># 清零梯度
</span>            <span class="n">optim</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="c1"># 向前传播，输出预测标签
</span>            <span class="n">haty</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="c1"># 计算损失值
</span>            <span class="n">l</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">haty</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
            <span class="c1"># 反向传播，计算得到每个参数的梯度值
</span>            <span class="n">l</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="c1"># 梯度下降，由优化器更新参数
</span>            <span class="n">optim</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
            <span class="c1"># 累计损失值
</span>            <span class="n">los</span> <span class="o">+=</span> <span class="p">(</span><span class="n">los</span> <span class="o">*</span> <span class="n">k</span> <span class="o">+</span> <span class="n">l</span><span class="p">.</span><span class="n">detach</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="c1">#right += torch.eq(torch.max(haty, dim=1)[1], y).sum()
</span>        <span class="k">return</span> <span class="n">los</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>

</pre></table></code></div></div><h3 id="3评估正确率"><span class="mr-2">3）评估正确率</span><a href="#3评估正确率" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>计算用当前网络预测正确的样本个数。</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre><td class="rouge-code"><pre><span class="o">@</span><span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">()</span> <span class="c1"># 使新增的tensor没有梯度，使带梯度的tensor能够进行原地运算
</span><span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="n">net</span><span class="p">,</span><span class="n">data_iter</span><span class="p">):</span>
    <span class="n">net</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span> <span class="c1"># 开启评估模式
</span>    <span class="c1"># 将计算累计正确预测样例数的变量定义在GPU上，无需在计算时在CPU与GPU之间移动，耗费时间
</span>    <span class="n">right_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="n">cuda</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data_iter</span><span class="p">):</span>
        <span class="n">X</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">data</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">cuda</span><span class="p">()</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">cuda</span><span class="p">()</span>
        <span class="c1"># 计算预测标签一致的样例数
</span>        <span class="n">right_sum</span> <span class="o">+=</span> <span class="n">torch</span><span class="p">.</span><span class="n">eq</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">right_sum</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>
</pre></table></code></div></div><h3 id="4main"><span class="mr-2">4）main</span><a href="#4main" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="nb">input</span><span class="p">,</span><span class="n">output</span> <span class="o">=</span> <span class="mi">3072</span><span class="p">,</span><span class="mi">10</span>
    <span class="n">hid</span><span class="p">,</span><span class="n">hid2</span><span class="p">,</span><span class="n">hid3</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span><span class="mi">256</span><span class="p">,</span><span class="mi">84</span>
    <span class="n">trainlen</span><span class="p">,</span><span class="n">testlen</span> <span class="o">=</span> <span class="mi">50000</span><span class="p">,</span><span class="mi">10000</span>
    <span class="n">train_iter</span><span class="p">,</span> <span class="n">test_iter</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">()</span>
    <span class="c1">#net = LeNet()
</span>    <span class="c1">#net = MLP(input,output,hid,hid2,hid3)
</span>    <span class="c1">#net = Softmax(input,output)
</span>    <span class="n">net</span> <span class="o">=</span> <span class="n">CNN</span><span class="p">()</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">net</span><span class="p">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="c1">#损失函数
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
    <span class="c1">#优化函数
</span>    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span><span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span> 
    <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">lr_scheduler</span><span class="p">.</span><span class="n">ExponentialLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.98</span><span class="p">)</span>
    <span class="c1"># torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.65)
</span>    <span class="c1">#optimizer = torch.optim.Adam(net.parameters(),weight_decay=1e-5) 
</span>    <span class="n">epoch</span> <span class="o">=</span> <span class="mi">40</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
        <span class="n">loss_visual</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">SingleTrain</span><span class="p">(</span><span class="n">net</span><span class="p">,</span><span class="n">train_iter</span><span class="p">,</span><span class="n">loss</span><span class="p">,</span><span class="n">optimizer</span><span class="p">))</span>
        <span class="c1"># train.append(score(net,train_iter)/trainlen)
</span>        <span class="n">test</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">(</span><span class="n">net</span><span class="p">,</span><span class="n">test_iter</span><span class="p">)</span><span class="o">/</span><span class="n">testlen</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'epoch: </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="se">\n</span><span class="s">loss:</span><span class="si">{</span><span class="n">loss_visual</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="s">test accuracy:</span><span class="si">{</span><span class="n">test</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Time for this epoch: </span><span class="si">{</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">t</span><span class="si">}</span><span class="s">s'</span><span class="p">)</span>
    
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Time: </span><span class="si">{</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="si">}</span><span class="s">s</span><span class="se">\n</span><span class="s"> for </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s"> epoches.</span><span class="se">\n</span><span class="s">Average </span><span class="si">{</span><span class="p">(</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="p">)</span><span class="o">/</span><span class="n">epoch</span><span class="si">}</span><span class="s"> for each.'</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'The optimal test accuracy:</span><span class="si">{</span><span class="nb">max</span><span class="p">(</span><span class="n">test</span><span class="p">)</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">loss_visual</span><span class="p">)),</span><span class="n">loss_visual</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Epoch'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Loss'</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="c1">#plt.plot(range(len(train)),train,color='blue', label='Train Accuracy')
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">test</span><span class="p">)),</span><span class="n">test</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s">'purple'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Test Accuracy'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span> <span class="c1"># 显示图例
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Epoch'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Accuracy'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
    
    
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</pre></table></code></div></div><h2 id="三实验分析"><span class="mr-2">三、实验分析</span><a href="#三实验分析" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="1softmax线性模型"><span class="mr-2">1）Softmax线性模型</span><a href="#1softmax线性模型" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>对更改优化算法、学习率lr及迭代次数epoch等参数进行实验。</p><p>运行结果：</p><div class="table-wrapper"><table><thead><tr><th>优化器及参数<th>损失值<th>训练集表现<th>测试集表现<tbody><tr><td>Adam<br />lr=0.002<br />epoch=300<td><img data-src="https://raw.githubusercontent.com/WitchPuff/typora_images/main/img/202303291843126.png" alt="1" style="zoom: 67%;" data-proofer-ignore><td><img data-src="https://raw.githubusercontent.com/WitchPuff/typora_images/main/img/202303291843109.png" alt="2" style="zoom: 67%;" data-proofer-ignore><td><img data-src="https://raw.githubusercontent.com/WitchPuff/typora_images/main/img/202211080019367.png" alt="3" style="zoom: 67%;" data-proofer-ignore><tr><td>Adam<br />lr=0.05<br />epoch=30<td><img data-src="https://raw.githubusercontent.com/WitchPuff/typora_images/main/img/202303291843473.png" alt="1 " style="zoom: 67%;" data-proofer-ignore><td><img data-src="https://raw.githubusercontent.com/WitchPuff/typora_images/main/img/202303291843877.png" alt="2" style="zoom: 67%;" data-proofer-ignore><td><img data-src="https://raw.githubusercontent.com/WitchPuff/typora_images/main/img/202211080020554.png" alt="3" style="zoom: 67%;" data-proofer-ignore><tr><td>SGD<br />lr=0.03<br />momentum=0.9<br />weight_decay=1e-5<br />epoch=40<td><img data-src="https://raw.githubusercontent.com/WitchPuff/typora_images/main/img/202211080023288.png" alt="1" style="zoom: 67%;" data-proofer-ignore><td> <td><img data-src="https://raw.githubusercontent.com/WitchPuff/typora_images/main/img/202303291843273.png" alt="2" style="zoom: 67%;" data-proofer-ignore></table></div><p>从表中可以看出：</p><ol><li>在训练效率上，使用不同优化算法的时间是相近的，Adam优化算法会较快。Adam采用自适应优化，所以它的优势是训练快，但是问题在于更容易陷入局部最优、鞍点。<li>在训练效果上：<ol><li>使用Adam优化算法时，正确率不如使用SGD Momentum优化算法，损失函数正常收敛，学习率较大时损失函数出现微小的震荡。当学习率为0.002时，迭代了300次后，训练集正确率仅仅是从12%增加到约26%，测试集也是，说明欠拟合。当学习率为0.05时，迭代了30次后，训练集与测试集正确率都在17%到23%左右震荡，绘制正确率图像时没有将y轴范围设置在[0,1]，故直观看上去产生强烈震荡，实际上只是在低准确率处小幅震荡，同样欠拟合。<li>使用SGD Momentum优化算法时，可以看到损失函数在大幅下降后存在震荡情况，收敛并不平滑，这也许是由动量衰减因素引起的。此时，测试集正确率能达到39.72%，为该Softmax分类器的最优训练结果，但从测试机正确率图像可以看出，测试机正确率的上涨幅度极小，几乎是一条平稳的直线。</ol></ol><p>最终选用的最优优化算法及其参数为：SGD算法，学习率0.03，动量衰减因数0.9，权重衰减1e-5</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span><span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span> 
</pre></table></code></div></div><p>综上所述，Softmax线性分类器对CIFAR10数据集的分类效果较差，模型训练的拟合效果较差，这是由于，单层神经网络的线性分类器与多维的图像本就难以拟合，为了转换为二维的矩阵计算，将图像的数据进行平展，损失了图像的空间结构信息。</p><h3 id="2mlp模型"><span class="mr-2">2）MLP模型</span><a href="#2mlp模型" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>通过实验，在SGD算法，学习率0.03，动量衰减因数0.9，权重衰减1e-5条件下，对比了单隐藏层与三隐藏层网络结构的训练效果。</p><p>运行结果：</p><div class="table-wrapper"><table><thead><tr><th>网络结构<th>损失值<th>测试集表现<tbody><tr><td>input<br />hidden 1(512)<br />output<br />epoch=40<td><img data-src="https://raw.githubusercontent.com/WitchPuff/typora_images/main/img/202303291843704.png" alt="image-20221108010542122" style="zoom:67%;" data-proofer-ignore><td>50.71%<img data-src="https://raw.githubusercontent.com/WitchPuff/typora_images/main/img/202303291844874.png" alt="image-20221108010559641" style="zoom:67%;" data-proofer-ignore><tr><td>input<br />hidden 1(120)<br />output<br />epoch=20<td><img data-src="https://raw.githubusercontent.com/WitchPuff/typora_images/main/img/202303291844481.png" alt="image-20221108011349585" style="zoom:67%;" data-proofer-ignore><td>49.72%<img data-src="https://raw.githubusercontent.com/WitchPuff/typora_images/main/img/202303291844502.png" alt="image-20221108011341001" style="zoom:67%;" data-proofer-ignore><tr><td>input<br />hidden 1(1024)<br />hidden 2 (256)<br />hidden 3(84)<br />out<br />epoch=60<td><img data-src="https://raw.githubusercontent.com/WitchPuff/typora_images/main/img/202303291844146.png" alt="SGD MOM" style="zoom:67%;" data-proofer-ignore><td>58.41%<img data-src="https://raw.githubusercontent.com/WitchPuff/typora_images/main/img/202303291844943.png" alt="SGDMOM" style="zoom:67%;" data-proofer-ignore><tr><td>input<br />hidden 1(4096)<br />hidden 2 (1280)<br />hidden 3(256)<br />hidden4(64)<br />out<br />epoch=40<td><img data-src="https://raw.githubusercontent.com/WitchPuff/typora_images/main/img/202303291844118.png" alt="image-20221108013620177" data-proofer-ignore><td>60.09%<img data-src="https://raw.githubusercontent.com/WitchPuff/typora_images/main/img/202303291844835.png" alt="image-20221108013554670" data-proofer-ignore></table></div><p>从表中可以看出：</p><ol><li>在训练效率上，网络结构对用时影响甚微，时间基本为18s/epoch。<li>在训练效果上：<ol><li>隐藏层数量的影响：隐藏层越多，非线性的比重更大，模型的拟合效果越好，预测正确率越高。对于单隐藏层的网络结构，最高正确率在50%左右，从图像可以看出，损失函数收敛不平滑，测试集正确率仅从40%上升到50%，训练效果较差，欠拟合。对多隐藏层的网络结构，正确率能接近60%左右，三隐藏层的最高正确率为58%，四隐藏层迭代次数更少而正确率更高，为60%。然而，此时损失函数皆未完全收敛，测试集正确率却已收敛不再增加，说明模型欠拟合。<li>隐藏层神经元数量的影响：在适当范围内，隐藏层神经元数量越多，拟合效果越好。理论上，隐藏层中使用太少的神经元将导致欠拟合(underfitting)。相反，使用过多的神经元同样会导致一些问题。首先，隐藏层中的神经元过多可能会导致过拟合(overfitting)。当神经网络具有过多的节点（过多的信息处理能力）时，训练集中包含的有限信息量不足以训练隐藏层中的所有神经元，因此就会导致过拟合。即使训练数据包含的信息量足够，隐藏层中过多的神经元会增加训练时间，从而难以达到预期的效果。</ol></ol><h3 id="3cnn模型"><span class="mr-2">3）CNN模型</span><a href="#3cnn模型" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>以LeNet网络结构为基础，通过实验，分别对比了优化算法及其参数、网络结构对训练效果的影响。</p><h4 id="1对优化算法及其参数的实验"><span class="mr-2">1、对优化算法及其参数的实验</span><a href="#1对优化算法及其参数的实验" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>在LeNet网络结构下，探究不同优化算法及其参数对训练性能的影响。</p><p>运行结果：</p><div class="table-wrapper"><table><thead><tr><th>优化器及参数<th>损失值<th>测试集表现<tbody><tr><td>SGD<br />lr=0.03<br />momentum=0.9<br />weight_decay=1e-5<br />epoch=60<td><img data-src="https://raw.githubusercontent.com/WitchPuff/typora_images/main/img/202211080105559.png" alt="1" style="zoom:67%;" data-proofer-ignore><td>73.82%<img data-src="https://raw.githubusercontent.com/WitchPuff/typora_images/main/img/202303291844720.png" alt="acc" style="zoom:67%;" data-proofer-ignore><tr><td>Adam<br />lr=0.001<br />weight_decay=1e-5<br />epoch=50<td><img data-src="https://raw.githubusercontent.com/WitchPuff/typora_images/main/img/202211080120735.png" alt="ADAM" style="zoom:67%;" data-proofer-ignore><td>62.26%<img data-src="https://raw.githubusercontent.com/WitchPuff/typora_images/main/img/202303291844822.png" alt="ADAM_ACC" style="zoom:67%;" data-proofer-ignore><tr><td>SGD<br />lr=0.03<br />weight_decay=1e-5<br />epoch=50<td><img data-src="https://raw.githubusercontent.com/WitchPuff/typora_images/main/img/202303291844379.png" alt="sgdno" style="zoom:67%;" data-proofer-ignore><td>61.37%<img data-src="https://raw.githubusercontent.com/WitchPuff/typora_images/main/img/202303291844561.png" alt="sgd no mom" style="zoom:67%;" data-proofer-ignore></table></div><p>从表中可以看出，在训练效果上：</p><ol><li>使用SGD Momentum优化算法时，经过60次迭代，损失函数正常收敛；模型拟合效果最好，测试集正确率最高，能达到73.82%，训练集正确率能达到90%。<li>使用Adam优化算法时，模型拟合效果较差，正确率仅为62%，也许是因为默认的学习率太低。<li>使用SGD优化算法时，缺乏动量衰减因子后，损失函数收敛速度明显变慢，接近最优点的速度变慢，震荡情况增加，模型拟合效果较差，测试集正确率仅为61%。</ol><h4 id="2对网络结构的实验"><span class="mr-2">2、对网络结构的实验</span><a href="#2对网络结构的实验" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>在SGD算法，学习率0.03，动量衰减因数0.9，权重衰减1e-5条件下，对不同的网络结构进行实验。</p><p>运行结果：</p><div class="table-wrapper"><table><thead><tr><th>网络结构<th>测试集表现及用时<tbody><tr><td><strong>LeNet:</strong><br />input<br />Conv(out_channels=6,conv_size=5)<br />MaxPool($2\times2$)<br />Conv(out_channels=16,conv_size=5)<br />MaxPool($2\times2$)<br />linear(120)<br />linear(84)<br />out(10)<td>73.82%<img data-src="https://raw.githubusercontent.com/WitchPuff/typora_images/main/img/202303291844720.png" alt="acc" style="zoom:67%;" data-proofer-ignore><tr><td><strong>在LeNet基础上添加一个卷积层、一个全连接层:</strong><br />input<br />Conv(out_channels=16,kern_size=5)<br />Conv(out_channels=32,kern_size=5)<br />MaxPool($2\times2$)<br />Conv(out_channels=64,conv_size=5)<br />MaxPool($2\times2$)<br />linear(256)<br />linear(120)<br />linear(84)<br />out(10)<td>74.5%<img data-src="https://raw.githubusercontent.com/WitchPuff/typora_images/main/img/202303291844808.png" alt="SGD MOM 改了卷积层" data-proofer-ignore><tr><td>input<br />Conv(out_channels=32,kern_size=3,pad=1)<br />MaxPool($2\times2$)<br />Conv(out_channels=64,kern_size=3,pad=1)<br />MaxPool($2\times2$)<br />linear(1024)<br />linear(512)<br />out(10)<td>77.89%<img data-src="https://raw.githubusercontent.com/WitchPuff/typora_images/main/img/202303291844654.png" alt="acc" style="zoom:67%;" data-proofer-ignore><tr><td><strong>在LeNet基础上添加两个卷积层、一个全连接层:</strong><br />input<br />Conv(out_channels=6,kern_size=2,pad=2)<br />Conv(out_channels=16,kern_size=2,pad=2)<br />MaxPool($2\times2$)<br />Conv(out_channels=64,conv_size=2,pad=2)<br />MaxPool($2\times2$)<br />Conv(out_channels=128,conv_size=3,pad=2)<br />linear(2069)<br />linear(496)<br />linear(84)<br />out(10)<td>78.59%<img data-src="https://raw.githubusercontent.com/WitchPuff/typora_images/main/img/202303291844464.png" alt="image-20221108154608416" style="zoom:67%;" data-proofer-ignore></table></div><p>从表中可以看出：</p><ol><li>在训练效率上，相同条件下，卷积层数越少，模型收敛速度越快，且单次epoch用时越少，但网络结构含有2~4个卷积层时一般在18s/epoch左右浮动，实际变动不明显。对于只有两个卷积层、三个全连接层的CNN网络结构，只需要15 epoch就能收敛到最优的正确率。若有三个卷积层、四个全连接层，则需要30 epoch来达到收敛。若有四个卷积层，四个全连接层，用该网络结构训练了两次，epoch皆为25，此时损失函数还未收敛，仍然呈下降趋势，说明收敛速度较慢。<li>在训练效果上：适当增加卷积层、减小卷积核大小，添加零填充层能够增强模型的拟合效果，提高正确率。在适当的范围内，卷积核越小，意味着图像的特征采样越细，故可以提高准确率。添加零填充层，可以防止对边缘像素信息的遗失。若有四个卷积层，四个全连接层，用该网络结构训练了四次，迭代到25次时，一次达到了78%的正确率且已收敛，两次则只达到56%且已收敛，说明卷积层过多，可能出现过拟合的情况。此外，用SGD Momentum优化算法，也可能导致对局部最优点的搜索具有随机性。卷积层过多时，会导致神经元失效，尝试了七层卷积层，正确率一直为0.1，正好10个分类，神经元已经陷入瞎猜。</ol><h3 id="4对比三个模型在cifar-10图像分类任务上的性能"><span class="mr-2">4）对比三个模型在CIFAR-10图像分类任务上的性能</span><a href="#4对比三个模型在cifar-10图像分类任务上的性能" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>显然的，CNN模型在图像分类任务上训练效果最佳，最高测试集正确率能达到77%，远超另外两个模型，这是由于Softmax线性模型与MLP模型都基于线性计算，需要将图像的数据进行平展，损失了图像的空间结构信息，而CNN模型可以通过卷积运算，保留读取这些结构信息。次之的是MLP模型，通过增加隐藏层数量，可以得到非线性模型的训练效果，最高测试集正确率能达到60%。拟合效果最差的则是Softmax模型，单层神经网络的线性分类器与多维的图像本就难以拟合，并非意外结果。</p><p>在训练效率上，事实上，无论是基于优化算法的实验，还是网络结构的实验，或是模型的实验，都较难观察出训练速度的差别，基本在18s/epoch左右。我在本实验中采用了GPU加速，但对于该模型，BatchSize=500时，极少数情况下，N卡占用率才能达到70%，多数时间有大量空余，真正占用时间的是迭代数据时，CPU对数据集图像的读取及预处理（即transform操作），而非与优化算法、网络机构、模型有关的计算，这是由于用dataloader读取数据时，每epoch要对数据做一次预处理、转换为张量的操作，再迭代取数据，CPU对Tensor的处理很慢，而torchvision库没有将数据集迁移到GPU进行预处理计算的API。因此，未能观察出训练速度的差别。若要解决这个问题，只能使用DALI或其余库接口加速预处理与数据读取，或者将预处理后的数据集进行保存。</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/courses/'>Courses</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/machine-learning/" class="post-tag no-text-decoration" >Machine Learning</a> <a href="/tags/ai/" class="post-tag no-text-decoration" >AI</a> <a href="/tags/cnn/" class="post-tag no-text-decoration" >CNN</a> <a href="/tags/mlp/" class="post-tag no-text-decoration" >MLP</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Machine+Learning%3A+Classification+-+CyberWitch&url=https%3A%2F%2Fwitchpuff.github.io%2Fposts%2Fclassification%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Machine+Learning%3A+Classification+-+CyberWitch&u=https%3A%2F%2Fwitchpuff.github.io%2Fposts%2Fclassification%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fwitchpuff.github.io%2Fposts%2Fclassification%2F&text=Machine+Learning%3A+Classification+-+CyberWitch" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <a href="http://service.weibo.com/share/share.php?title=Machine+Learning%3A+Classification+-+CyberWitch&url=https%3A%2F%2Fwitchpuff.github.io%2Fposts%2Fclassification%2F" data-toggle="tooltip" data-placement="top" title="Weibo" target="_blank" rel="noopener" aria-label="Weibo"> <i class="fa-fw fab fa-weibo"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/transport/">Computer Network: Transport Layer(2)</a><li><a href="/posts/classification/">Machine Learning: Classification</a><li><a href="/posts/sqlite3/">ModuleNotFoundError: No module named '_sqlite3'解决方法</a><li><a href="/posts/clustering/">Machine Learning: Clustering</a><li><a href="/posts/astar/">Solving 15-Puzzle with A* and IDA*</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/computer-network/">Computer Network</a> <a class="post-tag" href="/tags/machine-learning/">Machine Learning</a> <a class="post-tag" href="/tags/bart/">BART</a> <a class="post-tag" href="/tags/music-generation/">Music Generation</a> <a class="post-tag" href="/tags/searching/">Searching</a> <a class="post-tag" href="/tags/a/">A*</a> <a class="post-tag" href="/tags/clustering/">Clustering</a> <a class="post-tag" href="/tags/cnn/">CNN</a> <a class="post-tag" href="/tags/computer-architecture/">Computer Architecture</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/clustering/"><div class="card-body"> <em class="small" data-ts="1671327390" data-df="ll" > Dec 18, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Machine Learning: Clustering</h3><div class="text-muted small"><p> 机器学习：K-Means&amp;amp;GMM学习笔记 1、K-Means 1）算法思路 2）初始化中心点 1. 随机选取k个中心点 2. 最大距离选取中心点 ...</p></div></div></a></div><div class="card"> <a href="/posts/svm/"><div class="card-body"> <em class="small" data-ts="1667698590" data-df="ll" > Nov 6, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>SVM学习笔记</h3><div class="text-muted small"><p> SVM学习笔记 1）SVM模型 原理推导 实践 2）Hinge Loss 3）Cross-Entropy Loss 二分类 多分类 4）库函数 ...</p></div></div></a></div><div class="card"> <a href="/posts/bart/"><div class="card-body"> <em class="small" data-ts="1678754610" data-df="ll" > Mar 14, 2023 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Paper Summary of BART</h3><div class="text-muted small"><p> Paper Summary of BART 本次阅读了论文 BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension 语言模型预训练+下游任务fine-tune 用任意噪声函数破坏文本：随机打乱句子顺序；将文本替...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/svm/" class="btn btn-outline-primary" prompt="Older"><p>SVM学习笔记</p></a> <a href="/posts/sqlite3/" class="btn btn-outline-primary" prompt="Newer"><p>ModuleNotFoundError: No module named '_sqlite3'解决方法</p></a></div><script type="text/javascript"> $(function () { const origin = "https://giscus.app"; const iframe = "iframe.giscus-frame"; const lightTheme = "light"; const darkTheme = "dark_dimmed"; let initTheme = lightTheme; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches)) { initTheme = darkTheme; } let giscusAttributes = { "src": "https://giscus.app/client.js", "data-repo": "WitchPuff/WitchPuff.github.io", "data-repo-id": "R_kgDOHwS97Q", "data-category": "Announcements", "data-category-id": "DIC_kwDOHwS97c4CQnTG", "data-mapping": "pathname", "data-reactions-enabled": "1", "data-emit-metadata": "0", "data-theme": initTheme, "data-input-position": "bottom", "data-lang": "en", "crossorigin": "anonymous", "async": "" }; let giscusScript = document.createElement("script"); Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value)); document.getElementById("tail-wrapper").appendChild(giscusScript); addEventListener("message", (event) => { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { /* global theme mode changed */ const mode = event.data.message; const theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme); const message = { setConfig: { theme: theme } }; const giscus = document.querySelector(iframe).contentWindow; giscus.postMessage({ giscus: message }, origin); } }); }); </script></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2023 <a href="">WitchPuff</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/computer-network/">Computer Network</a> <a class="post-tag" href="/tags/machine-learning/">Machine Learning</a> <a class="post-tag" href="/tags/bart/">BART</a> <a class="post-tag" href="/tags/music-generation/">Music Generation</a> <a class="post-tag" href="/tags/searching/">Searching</a> <a class="post-tag" href="/tags/a/">A*</a> <a class="post-tag" href="/tags/clustering/">Clustering</a> <a class="post-tag" href="/tags/cnn/">CNN</a> <a class="post-tag" href="/tags/computer-architecture/">Computer Architecture</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
